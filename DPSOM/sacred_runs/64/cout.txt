INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "64"
using LBP
num_features: 358
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-07-11 18:48:12.793908: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_3-3_2024-07-11_35310********* 

  0%|          | 0/810 [00:00<?, ?it/s]Number of batches: 6


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 561.3461303710938
Running validation...
Epoch 1, Step 0: Train Loss = 558.4290771484375, Test Loss = 558.5822143554688
  0%|          | 1/810 [00:00<04:44,  2.84it/s, epoch=0, test_loss=559, train_loss=558]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 558.678955078125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 556.5214233398438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 555.0053100585938
  0%|          | 4/810 [00:00<01:15, 10.67it/s, epoch=0, test_loss=559, train_loss=555]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 553.5386352539062
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 552.3779296875
Starting epoch 2/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 551.4091186523438
Running validation...
Epoch 2, Step 0: Train Loss = 550.4956665039062, Test Loss = 550.5645141601562
  1%|          | 7/810 [00:00<00:49, 16.24it/s, epoch=1, test_loss=551, train_loss=550]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 550.46337890625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 549.6864624023438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.926513671875
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.1290283203125
  1%|▏         | 11/810 [00:00<00:35, 22.30it/s, epoch=1, test_loss=551, train_loss=548]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 547.4733276367188
Starting epoch 3/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 546.5017700195312
Running validation...
Epoch 3, Step 0: Train Loss = 545.7259521484375, Test Loss = 545.76025390625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 545.6795654296875
  2%|▏         | 14/810 [00:00<00:32, 24.35it/s, epoch=2, test_loss=546, train_loss=546]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 544.7738647460938
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 543.7174682617188
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 542.6614990234375
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 541.669921875
  2%|▏         | 18/810 [00:00<00:28, 27.80it/s, epoch=2, test_loss=546, train_loss=542]Starting epoch 4/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 540.1671752929688
Running validation...
Epoch 4, Step 0: Train Loss = 538.8717651367188, Test Loss = 538.7446899414062
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 538.6103515625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 537.0289306640625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 535.2658081054688
  3%|▎         | 22/810 [00:01<00:27, 28.78it/s, epoch=3, test_loss=539, train_loss=535]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 532.8041381835938
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 531.2528076171875
Starting epoch 5/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 528.1815185546875
Running validation...
Epoch 5, Step 0: Train Loss = 525.2648315429688, Test Loss = 525.3646240234375
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 525.488525390625
  3%|▎         | 26/810 [00:01<00:26, 29.34it/s, epoch=4, test_loss=525, train_loss=525]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 521.9527587890625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 518.0077514648438
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 512.5491333007812
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 509.2414245605469
  4%|▎         | 30/810 [00:01<00:25, 31.12it/s, epoch=4, test_loss=525, train_loss=509]Starting epoch 6/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 502.99212646484375
Running validation...
Epoch 6, Step 0: Train Loss = 496.3337707519531, Test Loss = 495.99041748046875
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 496.7115173339844
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 486.5872802734375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 476.2886657714844
  4%|▍         | 34/810 [00:01<00:25, 30.88it/s, epoch=5, test_loss=496, train_loss=476]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 465.6485900878906
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 452.970947265625
Starting epoch 7/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 442.7645263671875
Running validation...
Epoch 7, Step 0: Train Loss = 424.69085693359375, Test Loss = 427.332275390625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 430.77789306640625
  5%|▍         | 38/810 [00:01<00:24, 30.88it/s, epoch=6, test_loss=427, train_loss=431]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 416.44287109375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 397.546142578125
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 379.00274658203125
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 380.9888916015625
  5%|▌         | 42/810 [00:01<00:23, 32.02it/s, epoch=6, test_loss=427, train_loss=381]Starting epoch 8/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 344.76385498046875
Running validation...
Epoch 8, Step 0: Train Loss = 333.34979248046875, Test Loss = 338.2059020996094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 341.61358642578125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 328.9874267578125
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 301.7880859375
  6%|▌         | 46/810 [00:01<00:24, 31.51it/s, epoch=7, test_loss=338, train_loss=302]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 266.3915100097656
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 281.20892333984375
Starting epoch 9/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 259.1483154296875
Running validation...
Epoch 9, Step 0: Train Loss = 246.72386169433594, Test Loss = 245.896240234375
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 254.97000122070312
  6%|▌         | 50/810 [00:01<00:24, 31.19it/s, epoch=8, test_loss=246, train_loss=255]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 243.52304077148438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 220.63230895996094
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 202.82106018066406
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 216.94125366210938
  7%|▋         | 54/810 [00:02<00:23, 32.18it/s, epoch=8, test_loss=246, train_loss=217]Starting epoch 10/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.95994567871094
Running validation...
Epoch 10, Step 0: Train Loss = 199.63722229003906, Test Loss = 203.20359802246094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.28182983398438
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 197.01702880859375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 184.92767333984375
  7%|▋         | 58/810 [00:02<00:23, 31.58it/s, epoch=9, test_loss=203, train_loss=185]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 169.24844360351562
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 189.5946807861328
Starting epoch 11/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 179.98712158203125
Running validation...
Epoch 11, Step 0: Train Loss = 176.97799682617188, Test Loss = 179.6189727783203
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 182.48275756835938
  8%|▊         | 62/810 [00:02<00:23, 31.53it/s, epoch=10, test_loss=180, train_loss=182]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 178.44174194335938
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.31027221679688
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.911376953125
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 176.42193603515625
  8%|▊         | 66/810 [00:02<00:22, 32.48it/s, epoch=10, test_loss=180, train_loss=176]Starting epoch 12/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 166.73568725585938
Running validation...
Epoch 12, Step 0: Train Loss = 164.3126983642578, Test Loss = 170.84046936035156
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 170.85244750976562
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 168.5107879638672
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.88743591308594
  9%|▊         | 70/810 [00:02<00:23, 32.07it/s, epoch=11, test_loss=171, train_loss=159]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 143.8529815673828
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.13555908203125
Starting epoch 13/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 159.2316131591797
Running validation...
Epoch 13, Step 0: Train Loss = 157.41622924804688, Test Loss = 173.346923828125
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 163.84576416015625
  9%|▉         | 74/810 [00:02<00:23, 31.73it/s, epoch=12, test_loss=173, train_loss=164]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 162.42771911621094
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.21295166015625
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 137.35238647460938
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 161.45774841308594
 10%|▉         | 78/810 [00:02<00:22, 32.72it/s, epoch=12, test_loss=173, train_loss=161]Starting epoch 14/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.59767150878906
Running validation...
Epoch 14, Step 0: Train Loss = 153.5513916015625, Test Loss = 146.375244140625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.6011199951172
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 156.6780242919922
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 147.9248046875
 10%|█         | 82/810 [00:02<00:22, 32.08it/s, epoch=13, test_loss=146, train_loss=148]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 134.79603576660156
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 157.35035705566406
Starting epoch 15/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 151.56517028808594
Running validation...
Epoch 15, Step 0: Train Loss = 151.3948974609375, Test Loss = 144.89483642578125
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 155.71881103515625
 11%|█         | 86/810 [00:03<00:22, 31.63it/s, epoch=14, test_loss=145, train_loss=156]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.2907257080078
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 145.80589294433594
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 132.77528381347656
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.6964874267578
 11%|█         | 90/810 [00:03<00:22, 32.53it/s, epoch=14, test_loss=145, train_loss=155]

SOM initialization...

 25%|██▌       | 204/810 [00:04<00:05, 113.52it/s, epoch=3, test_loss=1.7, train_loss=1.68] 

Training...

 30%|███       | 247/810 [00:08<00:47, 11.83it/s, cah=[0.57590663], cr_ratio=50.5, cs_ratio=0.3, epoch=6, ssom=[2.2033277], test_loss=146, train_loss=145, vae=[141.92923], vc_ratio=221]  ]
INFO:tensorflow:Restoring parameters from ../models/hyperopt_100_3-3_2024-07-11_35310/hyperopt_100_3-3_2024-07-11_35310.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_100_3-3_2024-07-11_35310/hyperopt_100_3-3_2024-07-11_35310.ckpt
Evaluation...
data_val_pca.shape: (448, 2)
test_k_all: [0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 6, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 6, 0, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 6, 0, 1, 6, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 6, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 6, 0, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 3, 0, 0, 6, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 0]
 85%|████████▌ | 690/810 [00:52<00:09, 13.21it/s, cah=[0.3003433], cr_ratio=51.5, cs_ratio=0.21, epoch=79, ssom=[2.2084615], test_loss=135, train_loss=135, vae=[132.0363], vc_ratio=305]

 NMI: 0.030143501966106725, AMI: 0.0019042690767236698, PUR: 0.3794642857142857.  Name: %r.


 Time: 53.952499866485596
INFO - hyperopt - Result: {'NMI': 0.030143501966106725, 'Purity': 0.3794642857142857, 'AMI': 0.0019042690767236698, 'Silhouette Score': -0.13497297651470444, 'Calinski-Harabasz Index': 1.6066177235621735, 'Davies-Bouldin Index': 4.240377120263022}
INFO - hyperopt - Completed after 0:00:54
