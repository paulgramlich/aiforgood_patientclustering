INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "33"
using LBP
[ 1.          1.         -1.          1.          1.          0.66666667
  0.5         1.          1.          0.16666667  0.5         1.
  1.          0.83333333  1.          1.         -1.          1.
  0.83333333  0.66666667  0.5        -1.          0.83333333 -1.
  0.5         1.         -1.         -1.          0.83333333  1.
 -1.          0.5        -1.          1.          0.83333333 -1.
  0.83333333  0.5         0.83333333  1.          0.66666667  0.33333333
 -1.          0.5         1.         -1.         -1.         -1.
  1.          0.83333333  1.         -1.          1.         -1.
  1.          0.5         0.66666667  0.83333333  1.          0.33333333
  0.83333333  0.83333333 -1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         1.          1.          0.5
 -1.          0.5        -1.          1.          0.5         0.83333333
  1.          0.83333333  0.5         0.5         0.83333333  1.
  1.         -1.         -1.         -1.         -1.          0.66666667
  1.          1.         -1.          0.5        -1.          0.83333333
 -1.         -1.          1.          0.83333333  1.          0.83333333
  1.          1.         -1.          0.5         0.5         1.
  0.83333333  1.          0.83333333 -1.         -1.          0.83333333
  0.83333333 -1.          1.          1.         -1.         -1.
  1.          1.          1.          0.5         1.          0.83333333
 -1.         -1.          1.          0.5         1.         -1.
  1.         -1.          1.          1.         -1.          1.
  0.83333333 -1.          0.5        -1.          1.          0.83333333
 -1.          0.83333333  0.66666667 -1.          1.          1.
  0.83333333 -1.         -1.          0.5         1.          0.83333333
  1.          1.         -1.         -1.          0.5         1.
 -1.          0.5         0.83333333  1.          1.         -1.
  1.          0.83333333  0.33333333  0.83333333  0.83333333 -1.
  0.66666667  1.          1.          1.          1.          0.83333333
  1.          0.5         1.          1.          1.          1.
  0.5         1.          0.66666667  0.5        -1.          1.
  1.          0.66666667  1.          1.         -1.         -1.
  1.          1.         -1.          1.         -1.          1.
 -1.          0.5        -1.          1.          0.83333333 -1.
 -1.          0.33333333  1.          1.          0.33333333  1.
  0.83333333  0.33333333 -1.          0.5         1.          0.83333333
  0.33333333  1.         -1.          0.66666667  0.83333333  1.
  1.          1.          1.          0.83333333  0.5         0.66666667
  1.         -1.          1.          1.          0.83333333  0.83333333
  1.          1.          1.         -1.          1.          0.5
  1.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.33333333 -1.          0.83333333  1.          1.         -1.
  1.          1.          0.83333333  1.          1.          0.66666667
 -1.          1.          1.         -1.         -1.         -1.
 -1.          1.          1.         -1.          1.          0.83333333
  1.          1.         -1.          0.66666667 -1.          0.83333333
  1.         -1.          0.83333333  1.          0.5         0.66666667
 -1.          1.          0.83333333 -1.          1.          0.83333333
  1.          0.83333333  0.66666667  1.          1.          1.
  0.83333333 -1.         -1.          0.16666667  1.         -1.
 -1.          0.83333333  0.66666667 -1.          0.66666667  1.
  1.         -1.          1.         -1.          1.          1.
  1.          1.          1.         -1.          1.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  1.          1.         -1.         -1.
  0.5         0.83333333 -1.          1.          1.          1.
  0.83333333  1.          1.          0.5        -1.          1.
 -1.          1.         -1.          0.83333333 -1.         -1.
  0.83333333  0.5         1.          1.          0.5         0.83333333
  0.5         0.83333333  0.66666667 -1.          1.         -1.
  0.83333333  0.66666667  0.66666667  0.83333333 -1.          0.66666667
  0.83333333  1.         -1.          1.         -1.         -1.
  1.          0.83333333  1.          1.          0.66666667  0.66666667
  1.          0.83333333 -1.          0.83333333  1.          1.
  0.5         1.          0.66666667  1.          0.83333333  0.5
  0.83333333  0.5         0.83333333  1.         -1.          1.
  0.83333333  0.83333333  0.83333333  0.83333333  1.          0.66666667
 -1.          0.5         0.66666667  1.          1.          0.66666667
 -1.         -1.          1.          0.5        -1.          1.
  1.          0.83333333  1.          1.          0.5         0.66666667
  0.5         1.          1.          0.83333333  1.          1.
  0.83333333  1.          0.83333333  0.83333333  1.         -1.
  0.66666667  0.83333333 -1.          0.66666667  1.          1.
  0.5         1.         -1.          0.83333333  0.83333333 -1.
  0.66666667  1.          0.83333333  1.          1.          1.
  1.         -1.          1.          0.83333333  0.66666667  1.
  1.          0.5         1.         -1.          1.          0.83333333
  1.          0.83333333 -1.         -1.          0.83333333  0.66666667
  1.         -1.         -1.          1.         -1.          0.83333333
 -1.          0.83333333  1.         -1.          0.83333333  0.5
 -1.         -1.          0.83333333 -1.          0.83333333 -1.
 -1.          1.          1.          0.83333333  1.          1.
  1.          0.83333333  1.          0.16666667  1.          1.
  0.16666667  0.5         1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         0.83333333 -1.         -1.
  1.         -1.          1.          0.83333333  1.          0.83333333
  1.          0.83333333  0.5         0.83333333 -1.          1.
  1.          0.83333333  0.83333333  1.         -1.          0.5
  1.          0.83333333  1.          0.5         0.83333333  1.
  0.5         1.          1.          0.66666667 -1.         -1.
 -1.          1.          0.5         0.83333333  1.          1.
  1.          0.5        -1.         -1.          1.          1.
 -1.          0.83333333  1.          0.66666667 -1.         -1.
  0.83333333  1.         -1.          0.5        -1.          0.83333333
  1.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          1.          0.5         0.66666667
  1.          0.5         1.         -1.          1.         -1.
  0.83333333  0.66666667 -1.         -1.          1.          0.5
 -1.          0.5         1.         -1.          1.         -1.
 -1.          1.          1.          1.          0.83333333 -1.
 -1.          1.          1.          0.66666667  1.         -1.
 -1.         -1.          1.         -1.          0.83333333  1.
 -1.          0.83333333  0.66666667  1.          0.66666667 -1.
  0.83333333  0.83333333  0.5         1.          0.66666667  1.
 -1.          0.83333333 -1.          1.         -1.          0.5
  0.5         0.83333333  0.83333333 -1.          0.66666667  0.83333333
  0.33333333  0.83333333  1.          0.33333333  1.          0.83333333
  1.          0.66666667 -1.          0.83333333  1.          1.
  0.66666667 -1.         -1.          0.66666667  0.83333333  0.33333333
 -1.          0.         -1.         -1.          1.         -1.
  1.          0.83333333  1.          1.         -1.          0.5
  1.         -1.          0.83333333 -1.          0.83333333  0.5
 -1.          0.83333333  1.          1.          0.5         0.5
 -1.          1.          1.          0.83333333  0.66666667  0.83333333
  1.          1.          1.         -1.         -1.         -1.
  0.83333333 -1.          1.          1.         -1.          1.
  0.66666667  1.          1.          1.         -1.          1.
  0.83333333  0.83333333 -1.         -1.         -1.          0.66666667
  0.5        -1.          1.          1.          0.66666667  0.83333333
  1.          1.         -1.          0.83333333 -1.          1.
 -1.          0.83333333 -1.          1.         -1.         -1.
  1.          0.83333333 -1.          1.         -1.          0.83333333
  1.          0.5         1.          0.16666667  0.83333333  0.5
 -1.         -1.          1.          0.5        -1.          0.16666667
  1.         -1.          1.          0.83333333 -1.          1.
  0.83333333  1.          0.5         0.83333333 -1.          1.
  0.83333333  1.          1.         -1.         -1.         -1.
  1.          0.5         0.83333333 -1.          1.          0.5
  0.83333333  1.          0.5         0.83333333  0.5        -1.
 -1.         -1.          1.          0.83333333 -1.          0.83333333
  1.          0.83333333  0.5         0.83333333  0.5         1.
 -1.         -1.          0.5         0.66666667  0.83333333  1.
  0.5         0.5        -1.          1.         -1.          1.
  0.5         0.5         0.83333333  0.83333333  1.          1.
 -1.          1.          0.5        -1.          0.66666667  1.
  0.83333333  0.83333333  1.          1.          0.83333333  0.66666667
  0.83333333  0.83333333  0.66666667  1.          0.16666667  0.83333333
  1.          0.83333333  0.5        -1.          0.5         1.
  0.83333333 -1.          0.5         0.66666667  1.         -1.
  0.66666667  0.5        -1.         -1.          0.33333333  0.83333333
  0.83333333  1.          1.          0.66666667  1.          0.83333333
  0.5         1.          0.66666667 -1.         -1.         -1.
  0.          1.          1.          1.          1.          1.
  1.         -1.          1.         -1.         -1.          1.
  0.83333333  1.          0.83333333  1.          0.66666667  0.5
  1.          1.          1.          1.         -1.          0.83333333
  1.          0.83333333 -1.         -1.          1.         -1.
  0.5         1.         -1.          1.         -1.          1.
  1.          1.          1.          1.          1.         -1.
  1.         -1.         -1.          0.83333333  1.         -1.
  1.          0.83333333 -1.         -1.          0.66666667  0.83333333
 -1.          1.          0.83333333  1.         -1.          1.
  1.          0.5         0.83333333  1.          0.83333333  0.66666667
 -1.          0.83333333 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 14:56:30.955841: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-19_4e2df********* 

  0%|          | 0/355 [00:00<?, ?it/s]Number of batches: 1


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 547.67333984375
Running validation...
Epoch 1, Step 0: Train Loss = 546.7540893554688, Test Loss = 546.7672119140625
  0%|          | 1/355 [00:00<02:08,  2.76it/s, epoch=0, test_loss=547, train_loss=547]Starting epoch 2/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 546.8006591796875
Running validation...
Epoch 2, Step 0: Train Loss = 546.124755859375, Test Loss = 546.0986328125
Starting epoch 3/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 546.1029052734375
Running validation...
Epoch 3, Step 0: Train Loss = 545.5967407226562, Test Loss = 545.5982666015625
Starting epoch 4/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 545.5712280273438
Running validation...
Epoch 4, Step 0: Train Loss = 545.1785888671875, Test Loss = 545.1754150390625
  1%|          | 4/355 [00:00<00:38,  9.12it/s, epoch=3, test_loss=545, train_loss=545]Starting epoch 5/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 545.141845703125
Running validation...
Epoch 5, Step 0: Train Loss = 544.7736206054688, Test Loss = 544.810546875
Starting epoch 6/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 544.7991333007812
Running validation...
Epoch 6, Step 0: Train Loss = 544.4445190429688, Test Loss = 544.4300537109375
  2%|▏         | 6/355 [00:00<00:29, 11.94it/s, epoch=5, test_loss=544, train_loss=544]Starting epoch 7/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 544.4391479492188
Running validation...
Epoch 7, Step 0: Train Loss = 544.1112060546875, Test Loss = 544.1307373046875
Starting epoch 8/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 544.1112060546875
Running validation...
Epoch 8, Step 0: Train Loss = 543.7894897460938, Test Loss = 543.7743530273438
Starting epoch 9/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 543.79833984375
Running validation...
Epoch 9, Step 0: Train Loss = 543.4407958984375, Test Loss = 543.4595947265625
  3%|▎         | 9/355 [00:00<00:23, 15.01it/s, epoch=8, test_loss=543, train_loss=543]Starting epoch 10/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 543.4446411132812
Running validation...
Epoch 10, Step 0: Train Loss = 543.0952758789062, Test Loss = 543.123046875
Starting epoch 11/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 543.1006469726562
Running validation...
Epoch 11, Step 0: Train Loss = 542.7354125976562, Test Loss = 542.7442626953125
  3%|▎         | 11/355 [00:00<00:21, 16.03it/s, epoch=10, test_loss=543, train_loss=543]Starting epoch 12/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 542.7404174804688
Running validation...
Epoch 12, Step 0: Train Loss = 542.3428955078125, Test Loss = 542.3433837890625
Starting epoch 13/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 542.3565063476562
Running validation...
Epoch 13, Step 0: Train Loss = 541.9083251953125, Test Loss = 541.8785400390625
Starting epoch 14/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 541.9302368164062
Running validation...
Epoch 14, Step 0: Train Loss = 541.3876953125, Test Loss = 541.3849487304688
  4%|▍         | 14/355 [00:01<00:19, 17.41it/s, epoch=13, test_loss=541, train_loss=541]Starting epoch 15/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 541.4370727539062
Running validation...
Epoch 15, Step 0: Train Loss = 540.8857421875, Test Loss = 540.90771484375


SOM initialization...

 10%|▉         | 35/355 [00:01<00:09, 33.82it/s, epoch=4, test_loss=1.56, train_loss=1.41]

Training...

 17%|█▋        | 59/355 [00:08<01:27,  3.39it/s, cah=[6.5044074], cr_ratio=14.5, cs_ratio=7.53, epoch=23, ssom=[4.172383], test_loss=314, train_loss=310, vae=[294.7969], vc_ratio=17.5]  tio=1.5e+7]    