INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "60"
using LBP
num_features: 358
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-07-09 15:53:51.242873: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_8-8_2024-07-09_9702d********* 

  0%|          | 0/810 [00:00<?, ?it/s]Number of batches: 6


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 561.3461303710938
Running validation...
Epoch 1, Step 0: Train Loss = 558.4290771484375, Test Loss = 558.5822143554688
  0%|          | 1/810 [00:00<05:10,  2.60it/s, epoch=0, test_loss=559, train_loss=558]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 558.678955078125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 556.5214233398438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 555.0053100585938
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 553.5386352539062
  1%|          | 5/810 [00:00<01:05, 12.34it/s, epoch=0, test_loss=559, train_loss=554]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 552.3779296875
Starting epoch 2/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 551.4091186523438
Running validation...
Epoch 2, Step 0: Train Loss = 550.4956665039062, Test Loss = 550.5645141601562
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 550.46337890625
  1%|          | 8/810 [00:00<00:48, 16.45it/s, epoch=1, test_loss=551, train_loss=550]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 549.6864624023438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.926513671875
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.1290283203125
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 547.4733276367188
  1%|▏         | 12/810 [00:00<00:36, 21.58it/s, epoch=1, test_loss=551, train_loss=547]Starting epoch 3/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 546.5017700195312
Running validation...
Epoch 3, Step 0: Train Loss = 545.7259521484375, Test Loss = 545.76025390625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 545.6795654296875
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 544.7738647460938
  2%|▏         | 15/810 [00:00<00:34, 22.91it/s, epoch=2, test_loss=546, train_loss=545]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 543.7174682617188
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 542.6614990234375
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 541.669921875
Starting epoch 4/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 540.1671752929688
Running validation...
Epoch 4, Step 0: Train Loss = 538.8717651367188, Test Loss = 538.7446899414062
  2%|▏         | 19/810 [00:00<00:31, 25.22it/s, epoch=3, test_loss=539, train_loss=539]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 538.6103515625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 537.0289306640625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 535.2658081054688
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 532.8041381835938
  3%|▎         | 23/810 [00:01<00:28, 27.58it/s, epoch=3, test_loss=539, train_loss=533]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 531.2528076171875
Starting epoch 5/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 528.1815185546875
Running validation...
Epoch 5, Step 0: Train Loss = 525.2648315429688, Test Loss = 525.3646240234375
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 525.488525390625
  3%|▎         | 26/810 [00:01<00:28, 27.89it/s, epoch=4, test_loss=525, train_loss=525]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 521.9527587890625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 518.0077514648438
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 512.5491333007812
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 509.2414245605469
  4%|▎         | 30/810 [00:01<00:26, 29.31it/s, epoch=4, test_loss=525, train_loss=509]Starting epoch 6/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 502.99212646484375
Running validation...
Epoch 6, Step 0: Train Loss = 496.3337707519531, Test Loss = 495.99041748046875
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 496.7115173339844
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 486.5872802734375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 476.2886657714844
  4%|▍         | 34/810 [00:01<00:26, 28.78it/s, epoch=5, test_loss=496, train_loss=476]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 465.6485900878906
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 452.970947265625
Starting epoch 7/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 442.7645263671875
Running validation...
Epoch 7, Step 0: Train Loss = 424.69085693359375, Test Loss = 427.332275390625
  5%|▍         | 37/810 [00:01<00:27, 28.43it/s, epoch=6, test_loss=427, train_loss=425]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 430.77789306640625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 416.44287109375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 397.546142578125
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 379.00274658203125
  5%|▌         | 41/810 [00:01<00:25, 29.68it/s, epoch=6, test_loss=427, train_loss=379]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 380.9888916015625
Starting epoch 8/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 344.76385498046875
Running validation...
Epoch 8, Step 0: Train Loss = 333.34979248046875, Test Loss = 338.2059020996094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 341.61358642578125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 328.9874267578125
  6%|▌         | 45/810 [00:01<00:25, 30.01it/s, epoch=7, test_loss=338, train_loss=329]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 301.7880859375
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 266.3915100097656
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 281.20892333984375
Starting epoch 9/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 259.1483154296875
Running validation...
Epoch 9, Step 0: Train Loss = 246.72386169433594, Test Loss = 245.896240234375
  6%|▌         | 49/810 [00:01<00:25, 29.78it/s, epoch=8, test_loss=246, train_loss=247]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 254.97000122070312
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 243.52304077148438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 220.63230895996094
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 202.82106018066406
  7%|▋         | 53/810 [00:02<00:24, 31.32it/s, epoch=8, test_loss=246, train_loss=203]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 216.94125366210938
Starting epoch 10/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.95994567871094
Running validation...
Epoch 10, Step 0: Train Loss = 199.63722229003906, Test Loss = 203.20359802246094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.28182983398438
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 197.01702880859375
  7%|▋         | 57/810 [00:02<00:24, 30.68it/s, epoch=9, test_loss=203, train_loss=197]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 184.92767333984375
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 169.24844360351562
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 189.5946807861328
Starting epoch 11/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 179.98712158203125
Running validation...
Epoch 11, Step 0: Train Loss = 176.97799682617188, Test Loss = 179.6189727783203
  8%|▊         | 61/810 [00:02<00:24, 30.29it/s, epoch=10, test_loss=180, train_loss=177]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 182.48275756835938
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 178.44174194335938
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.31027221679688
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.911376953125
  8%|▊         | 65/810 [00:02<00:23, 31.27it/s, epoch=10, test_loss=180, train_loss=153]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 176.42193603515625
Starting epoch 12/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 166.73568725585938
Running validation...
Epoch 12, Step 0: Train Loss = 164.3126983642578, Test Loss = 170.84046936035156
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 170.85244750976562
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 168.5107879638672
  9%|▊         | 69/810 [00:02<00:25, 29.28it/s, epoch=11, test_loss=171, train_loss=169]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.88743591308594
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 143.8529815673828
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.13555908203125
Starting epoch 13/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 159.2316131591797
Running validation...
Epoch 13, Step 0: Train Loss = 157.41622924804688, Test Loss = 173.346923828125
  9%|▉         | 73/810 [00:02<00:25, 29.42it/s, epoch=12, test_loss=173, train_loss=157]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 163.84576416015625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 162.42771911621094
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.21295166015625
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 137.35238647460938
 10%|▉         | 77/810 [00:02<00:23, 30.99it/s, epoch=12, test_loss=173, train_loss=137]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 161.45774841308594
Starting epoch 14/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.59767150878906
Running validation...
Epoch 14, Step 0: Train Loss = 153.5513916015625, Test Loss = 146.375244140625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.6011199951172
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 156.6780242919922
 10%|█         | 81/810 [00:03<00:24, 30.14it/s, epoch=13, test_loss=146, train_loss=157]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 147.9248046875
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 134.79603576660156
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 157.35035705566406
Starting epoch 15/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 151.56517028808594
Running validation...
Epoch 15, Step 0: Train Loss = 151.3948974609375, Test Loss = 144.89483642578125
 10%|█         | 85/810 [00:03<00:24, 30.03it/s, epoch=14, test_loss=145, train_loss=151]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 155.71881103515625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.2907257080078
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 145.80589294433594
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 132.77528381347656
 11%|█         | 89/810 [00:03<00:23, 31.00it/s, epoch=14, test_loss=145, train_loss=133]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.6964874267578


SOM initialization...

 25%|██▍       | 199/810 [00:04<00:05, 108.86it/s, epoch=3, test_loss=2.48, train_loss=2.58]

Training...

 31%|███       | 251/810 [00:08<00:44, 12.48it/s, cah=[3.473845], cr_ratio=19.1, cs_ratio=0.893, epoch=6, ssom=[4.0082493], test_loss=151, train_loss=150, vae=[142.16182], vc_ratio=40.9] ]P
INFO:tensorflow:Restoring parameters from ../models/hyperopt_100_8-8_2024-07-09_9702d/hyperopt_100_8-8_2024-07-09_9702d.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_100_8-8_2024-07-09_9702d/hyperopt_100_8-8_2024-07-09_9702d.ckpt
Evaluation...
 85%|████████▌ | 690/810 [00:54<00:09, 12.74it/s, cah=[3.050927], cr_ratio=18.8, cs_ratio=0.821, epoch=79, ssom=[4.0426755], test_loss=143, train_loss=141, vae=[134.4849], vc_ratio=42.4]

 NMI: 0.07449573336399398, AMI: 0.015609532212514658, PUR: 0.390625.  Name: %r.


 Time: 54.99953889846802
INFO - hyperopt - Result: {'NMI': 0.07449573336399398, 'Purity': 0.390625, 'AMI': 0.015609532212514658, 'Silhouette Score': -0.14987797720630266, 'Calinski-Harabasz Index': 1.6352529229322326, 'Davies-Bouldin Index': 4.290598562640962}
INFO - hyperopt - Completed after 0:00:55
