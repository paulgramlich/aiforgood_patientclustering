INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "66"
using LBP
num_features: 358
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-07-12 15:34:20.385244: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_3-3_2024-07-12_4d6e7********* 

  0%|          | 0/810 [00:00<?, ?it/s]Number of batches: 6


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 561.3461303710938
Running validation...
Epoch 1, Step 0: Train Loss = 558.4290771484375, Test Loss = 558.5822143554688
  0%|          | 1/810 [00:00<05:05,  2.65it/s, epoch=0, test_loss=559, train_loss=558]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 558.678955078125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 556.5214233398438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 555.0053100585938
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 553.5386352539062
  1%|          | 5/810 [00:00<01:07, 11.95it/s, epoch=0, test_loss=559, train_loss=554]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 552.3779296875
Starting epoch 2/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 551.4091186523438
Running validation...
Epoch 2, Step 0: Train Loss = 550.4956665039062, Test Loss = 550.5645141601562
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 550.46337890625
  1%|          | 8/810 [00:00<00:50, 16.03it/s, epoch=1, test_loss=551, train_loss=550]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 549.6864624023438
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.926513671875
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 548.1290283203125
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 547.4733276367188
  1%|▏         | 12/810 [00:00<00:37, 21.28it/s, epoch=1, test_loss=551, train_loss=547]Starting epoch 3/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 546.5017700195312
Running validation...
Epoch 3, Step 0: Train Loss = 545.7259521484375, Test Loss = 545.76025390625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 545.6795654296875
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 544.7738647460938
  2%|▏         | 15/810 [00:00<00:34, 22.96it/s, epoch=2, test_loss=546, train_loss=545]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 543.7174682617188
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 542.6614990234375
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 541.669921875
Starting epoch 4/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 540.1671752929688
Running validation...
Epoch 4, Step 0: Train Loss = 538.8717651367188, Test Loss = 538.7446899414062
  2%|▏         | 19/810 [00:00<00:31, 25.18it/s, epoch=3, test_loss=539, train_loss=539]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 538.6103515625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 537.0289306640625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 535.2658081054688
  3%|▎         | 22/810 [00:01<00:29, 26.30it/s, epoch=3, test_loss=539, train_loss=535]Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 532.8041381835938
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 531.2528076171875
Starting epoch 5/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 528.1815185546875
Running validation...
Epoch 5, Step 0: Train Loss = 525.2648315429688, Test Loss = 525.3646240234375
  3%|▎         | 25/810 [00:01<00:29, 26.59it/s, epoch=4, test_loss=525, train_loss=525]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 525.488525390625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 521.9527587890625
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 518.0077514648438
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 512.5491333007812
  4%|▎         | 29/810 [00:01<00:27, 28.23it/s, epoch=4, test_loss=525, train_loss=513]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 509.2414245605469
Starting epoch 6/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 502.99212646484375
Running validation...
Epoch 6, Step 0: Train Loss = 496.3337707519531, Test Loss = 495.99041748046875
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 496.7115173339844
  4%|▍         | 32/810 [00:01<00:27, 28.09it/s, epoch=5, test_loss=496, train_loss=497]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 486.5872802734375
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 476.2886657714844
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 465.6485900878906
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 452.970947265625
  4%|▍         | 36/810 [00:01<00:26, 29.23it/s, epoch=5, test_loss=496, train_loss=453]Starting epoch 7/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 442.7645263671875
Running validation...
Epoch 7, Step 0: Train Loss = 424.69085693359375, Test Loss = 427.332275390625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 430.77789306640625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 416.44287109375
  5%|▍         | 39/810 [00:01<00:27, 28.39it/s, epoch=6, test_loss=427, train_loss=416]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 397.546142578125
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 379.00274658203125
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 380.9888916015625
  5%|▌         | 42/810 [00:01<00:27, 28.20it/s, epoch=6, test_loss=427, train_loss=381]Starting epoch 8/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 344.76385498046875
Running validation...
Epoch 8, Step 0: Train Loss = 333.34979248046875, Test Loss = 338.2059020996094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 341.61358642578125
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 328.9874267578125
  6%|▌         | 45/810 [00:01<00:28, 26.68it/s, epoch=7, test_loss=338, train_loss=329]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 301.7880859375
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 266.3915100097656
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 281.20892333984375
  6%|▌         | 48/810 [00:02<00:27, 27.41it/s, epoch=7, test_loss=338, train_loss=281]Starting epoch 9/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 259.1483154296875
Running validation...
Epoch 9, Step 0: Train Loss = 246.72386169433594, Test Loss = 245.896240234375
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 254.97000122070312
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 243.52304077148438
  6%|▋         | 51/810 [00:02<00:28, 26.30it/s, epoch=8, test_loss=246, train_loss=244]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 220.63230895996094
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 202.82106018066406
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 216.94125366210938
  7%|▋         | 54/810 [00:02<00:28, 26.81it/s, epoch=8, test_loss=246, train_loss=217]Starting epoch 10/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.95994567871094
Running validation...
Epoch 10, Step 0: Train Loss = 199.63722229003906, Test Loss = 203.20359802246094
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 204.28182983398438
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 197.01702880859375
  7%|▋         | 57/810 [00:02<00:28, 26.21it/s, epoch=9, test_loss=203, train_loss=197]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 184.92767333984375
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 169.24844360351562
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 189.5946807861328
Starting epoch 11/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 179.98712158203125
Running validation...
Epoch 11, Step 0: Train Loss = 176.97799682617188, Test Loss = 179.6189727783203
  8%|▊         | 61/810 [00:02<00:27, 27.40it/s, epoch=10, test_loss=180, train_loss=177]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 182.48275756835938
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 178.44174194335938
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.31027221679688
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.911376953125
  8%|▊         | 65/810 [00:02<00:25, 28.88it/s, epoch=10, test_loss=180, train_loss=153]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 176.42193603515625
Starting epoch 12/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 166.73568725585938
Running validation...
Epoch 12, Step 0: Train Loss = 164.3126983642578, Test Loss = 170.84046936035156
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 170.85244750976562
  8%|▊         | 68/810 [00:02<00:26, 27.93it/s, epoch=11, test_loss=171, train_loss=171]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 168.5107879638672
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.88743591308594
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 143.8529815673828
  9%|▉         | 71/810 [00:02<00:28, 25.82it/s, epoch=11, test_loss=171, train_loss=144]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 167.13555908203125
Starting epoch 13/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 159.2316131591797
Running validation...
Epoch 13, Step 0: Train Loss = 157.41622924804688, Test Loss = 173.346923828125
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 163.84576416015625
  9%|▉         | 74/810 [00:02<00:28, 25.64it/s, epoch=12, test_loss=173, train_loss=164]Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 162.42771911621094
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 152.21295166015625
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 137.35238647460938
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 161.45774841308594
 10%|▉         | 78/810 [00:03<00:26, 27.68it/s, epoch=12, test_loss=173, train_loss=161]Starting epoch 14/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.59767150878906
Running validation...
Epoch 14, Step 0: Train Loss = 153.5513916015625, Test Loss = 146.375244140625
Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 158.6011199951172
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 156.6780242919922
 10%|█         | 81/810 [00:03<00:27, 26.58it/s, epoch=13, test_loss=146, train_loss=157]Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 147.9248046875
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 134.79603576660156
Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 157.35035705566406
Starting epoch 15/15
Batch 1/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 151.56517028808594
Running validation...
Epoch 15, Step 0: Train Loss = 151.3948974609375, Test Loss = 144.89483642578125
 10%|█         | 85/810 [00:03<00:27, 26.77it/s, epoch=14, test_loss=145, train_loss=151]Batch 2/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 155.71881103515625
Batch 3/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.2907257080078
Batch 4/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 145.80589294433594
Batch 5/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 132.77528381347656
 11%|█         | 89/810 [00:03<00:25, 27.98it/s, epoch=14, test_loss=145, train_loss=133]Batch 6/6
Batch data shape: (64, 28, 28, 1)
Train Loss: 154.6964874267578


SOM initialization...

 25%|██▌       | 204/810 [00:05<00:06, 99.75it/s, epoch=3, test_loss=1.7, train_loss=1.68] 

Training...

 30%|███       | 244/810 [00:09<00:44, 12.60it/s, cah=[0.60971767], cr_ratio=50.4, cs_ratio=0.306, epoch=5, ssom=[2.2031772], test_loss=146, train_loss=145, vae=[142.58946], vc_ratio=217] 
INFO:tensorflow:Restoring parameters from ../models/hyperopt_100_3-3_2024-07-12_4d6e7/hyperopt_100_3-3_2024-07-12_4d6e7.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_100_3-3_2024-07-12_4d6e7/hyperopt_100_3-3_2024-07-12_4d6e7.ckpt
Evaluation...
 85%|████████▌ | 690/810 [00:56<00:09, 12.17it/s, cah=[0.3003433], cr_ratio=51.5, cs_ratio=0.21, epoch=79, ssom=[2.2084615], test_loss=135, train_loss=135, vae=[132.0363], vc_ratio=305]

 NMI: 0.030143501966106725, AMI: 0.0019042690767236698, PUR: 0.3794642857142857.  Name: %r.


 Time: 57.56997776031494
INFO - hyperopt - Result: {'NMI': 0.030143501966106725, 'Purity': 0.3794642857142857, 'AMI': 0.0019042690767236698, 'Silhouette Score': -0.13497297651470444, 'Calinski-Harabasz Index': 1.6066177235621735, 'Davies-Bouldin Index': 4.240377120263022}
INFO - hyperopt - Completed after 0:00:58
