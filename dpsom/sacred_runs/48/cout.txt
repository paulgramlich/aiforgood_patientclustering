INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "48"
using LBP
[8. 8. 0. 8. 8. 6. 5. 8. 8. 3. 5. 8. 8. 7. 8. 8. 0. 8. 7. 6. 5. 0. 7. 0.
 5. 8. 0. 0. 7. 8. 0. 5. 0. 8. 7. 0. 7. 5. 7. 8. 6. 4. 0. 5. 8. 0. 0. 0.
 8. 7. 8. 0. 8. 0. 8. 5. 6. 7. 8. 4. 7. 7. 0. 7. 6. 0. 7. 8. 5. 8. 8. 5.
 0. 5. 0. 8. 5. 7. 8. 7. 5. 5. 7. 8. 8. 0. 0. 0. 0. 6. 8. 8. 0. 5. 0. 7.
 0. 0. 8. 7. 8. 7. 8. 8. 0. 5. 5. 8. 7. 8. 7. 0. 0. 7. 7. 0. 8. 8. 0. 0.
 8. 8. 8. 5. 8. 7. 0. 0. 8. 5. 8. 0. 8. 0. 8. 8. 0. 8. 7. 0. 5. 0. 8. 7.
 0. 7. 6. 0. 8. 8. 7. 0. 0. 5. 8. 7. 8. 8. 0. 0. 5. 8. 0. 5. 7. 8. 8. 0.
 8. 7. 4. 7. 7. 0. 6. 8. 8. 8. 8. 7. 8. 5. 8. 8. 8. 8. 5. 8. 6. 5. 0. 8.
 8. 6. 8. 8. 0. 0. 8. 8. 0. 8. 0. 8. 0. 5. 0. 8. 7. 0. 0. 4. 8. 8. 4. 8.
 7. 4. 0. 5. 8. 7. 4. 8. 0. 6. 7. 8. 8. 8. 8. 7. 5. 6. 8. 0. 8. 8. 7. 7.
 8. 8. 8. 0. 8. 5. 8. 0. 6. 6. 0. 6. 4. 0. 7. 8. 8. 0. 8. 8. 7. 8. 8. 6.
 0. 8. 8. 0. 0. 0. 0. 8. 8. 0. 8. 7. 8. 8. 0. 6. 0. 7. 8. 0. 7. 8. 5. 6.
 0. 8. 7. 0. 8. 7. 8. 7. 6. 8. 8. 8. 7. 0. 0. 3. 8. 0. 0. 7. 6. 0. 6. 8.
 8. 0. 8. 0. 8. 8. 8. 8. 8. 0. 8. 0. 0. 6. 0. 0. 0. 0. 0. 6. 8. 8. 0. 0.
 5. 7. 0. 8. 8. 8. 7. 8. 8. 5. 0. 8. 0. 8. 0. 7. 0. 0. 7. 5. 8. 8. 5. 7.
 5. 7. 6. 0. 8. 0. 7. 6. 6. 7. 0. 6. 7. 8. 0. 8. 0. 0. 8. 7. 8. 8. 6. 6.
 8. 7. 0. 7. 8. 8. 5. 8. 6. 8. 7. 5. 7. 5. 7. 8. 0. 8. 7. 7. 7. 7. 8. 6.
 0. 5. 6. 8. 8. 6. 0. 0. 8. 5. 0. 8. 8. 7. 8. 8. 5. 6. 5. 8. 8. 7. 8. 8.
 7. 8. 7. 7. 8. 0. 6. 7. 0. 6. 8. 8. 5. 8. 0. 7. 7. 0. 6. 8. 7. 8. 8. 8.
 8. 0. 8. 7. 6. 8. 8. 5. 8. 0. 8. 7. 8. 7. 0. 0. 7. 6. 8. 0. 0. 8. 0. 7.
 0. 7. 8. 0. 7. 5. 0. 0. 7. 0. 7. 0. 0. 8. 8. 7. 8. 8. 8. 7. 8. 3. 8. 8.
 3. 5. 8. 7. 6. 0. 7. 8. 5. 7. 0. 0. 8. 0. 8. 7. 8. 7. 8. 7. 5. 7. 0. 8.
 8. 7. 7. 8. 0. 5. 8. 7. 8. 5. 7. 8. 5. 8. 8. 6. 0. 0. 0. 8. 5. 7. 8. 8.
 8. 5. 0. 0. 8. 8. 0. 7. 8. 6. 0. 0. 7. 8. 0. 5. 0. 7. 8. 0. 0. 0. 5. 0.
 0. 6. 0. 8. 5. 6. 8. 5. 8. 0. 8. 0. 7. 6. 0. 0. 8. 5. 0. 5. 8. 0. 8. 0.
 0. 8. 8. 8. 7. 0. 0. 8. 8. 6. 8. 0. 0. 0. 8. 0. 7. 8. 0. 7. 6. 8. 6. 0.
 7. 7. 5. 8. 6. 8. 0. 7. 0. 8. 0. 5. 5. 7. 7. 0. 6. 7. 4. 7. 8. 4. 8. 7.
 8. 6. 0. 7. 8. 8. 6. 0. 0. 6. 7. 4. 0. 2. 0. 0. 8. 0. 8. 7. 8. 8. 0. 5.
 8. 0. 7. 0. 7. 5. 0. 7. 8. 8. 5. 5. 0. 8. 8. 7. 6. 7. 8. 8. 8. 0. 0. 0.
 7. 0. 8. 8. 0. 8. 6. 8. 8. 8. 0. 8. 7. 7. 0. 0. 0. 6. 5. 0. 8. 8. 6. 7.
 8. 8. 0. 7. 0. 8. 0. 7. 0. 8. 0. 0. 8. 7. 0. 8. 0. 7. 8. 5. 8. 3. 7. 5.
 0. 0. 8. 5. 0. 3. 8. 0. 8. 7. 0. 8. 7. 8. 5. 7. 0. 8. 7. 8. 8. 0. 0. 0.
 8. 5. 7. 0. 8. 5. 7. 8. 5. 7. 5. 0. 0. 0. 8. 7. 0. 7. 8. 7. 5. 7. 5. 8.
 0. 0. 5. 6. 7. 8. 5. 5. 0. 8. 0. 8. 5. 5. 7. 7. 8. 8. 0. 8. 5. 0. 6. 8.
 7. 7. 8. 8. 7. 6. 7. 7. 6. 8. 3. 7. 8. 7. 5. 0. 5. 8. 7. 0. 5. 6. 8. 0.
 6. 5. 0. 0. 4. 7. 7. 8. 8. 6. 8. 7. 5. 8. 6. 0. 0. 0. 2. 8. 8. 8. 8. 8.
 8. 0. 8. 0. 0. 8. 7. 8. 7. 8. 6. 5. 8. 8. 8. 8. 0. 7. 8. 7. 0. 0. 8. 0.
 5. 8. 0. 8. 0. 8. 8. 8. 8. 8. 8. 0. 8. 0. 0. 7. 8. 0. 8. 7. 0. 0. 6. 7.
 0. 8. 7. 8. 0. 8. 8. 5. 7. 8. 7. 6. 0. 7. 0. 6.]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 16:50:28.387650: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_8-8_2024-06-19_dc711********* 

  0%|          | 0/155 [00:00<?, ?it/s]Number of batches: 1


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 567.9610595703125
Running validation...
Epoch 1, Step 0: Train Loss = 563.050048828125, Test Loss = 563.2180786132812
  1%|          | 1/155 [00:00<00:56,  2.74it/s, epoch=0, test_loss=563, train_loss=563]Starting epoch 2/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 563.0448608398438
Running validation...
Epoch 2, Step 0: Train Loss = 559.7911376953125, Test Loss = 559.6793212890625
Starting epoch 3/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 559.761962890625
Running validation...
Epoch 3, Step 0: Train Loss = 557.1946411132812, Test Loss = 557.25
  2%|▏         | 3/155 [00:00<00:25,  6.00it/s, epoch=2, test_loss=557, train_loss=557]Starting epoch 4/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 557.1932983398438
Running validation...
Epoch 4, Step 0: Train Loss = 555.2293701171875, Test Loss = 555.3028564453125
Starting epoch 5/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 555.1712036132812
Running validation...
Epoch 5, Step 0: Train Loss = 553.5532836914062, Test Loss = 553.6646728515625
  3%|▎         | 5/155 [00:00<00:19,  7.70it/s, epoch=4, test_loss=554, train_loss=554]Starting epoch 6/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 553.7018432617188
Running validation...
Epoch 6, Step 0: Train Loss = 552.29638671875, Test Loss = 552.3701782226562
  4%|▍         | 6/155 [00:00<00:18,  8.06it/s, epoch=5, test_loss=552, train_loss=552]Starting epoch 7/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 552.4468994140625
Running validation...
Epoch 7, Step 0: Train Loss = 551.2325439453125, Test Loss = 551.323486328125
Starting epoch 8/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 551.223876953125
Running validation...
Epoch 8, Step 0: Train Loss = 550.1254272460938, Test Loss = 550.310302734375
  5%|▌         | 8/155 [00:01<00:16,  8.67it/s, epoch=7, test_loss=550, train_loss=550]Starting epoch 9/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 550.180419921875
Running validation...
Epoch 9, Step 0: Train Loss = 549.1904907226562, Test Loss = 549.2554931640625
  6%|▌         | 9/155 [00:01<00:16,  8.69it/s, epoch=8, test_loss=549, train_loss=549]Starting epoch 10/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 549.1664428710938
Running validation...
Epoch 10, Step 0: Train Loss = 548.2098999023438, Test Loss = 548.3346557617188
  6%|▋         | 10/155 [00:01<00:16,  8.88it/s, epoch=9, test_loss=548, train_loss=548]Starting epoch 11/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 548.2171630859375
Running validation...
Epoch 11, Step 0: Train Loss = 547.2152709960938, Test Loss = 547.302978515625
  7%|▋         | 11/155 [00:01<00:15,  9.01it/s, epoch=10, test_loss=547, train_loss=547]Starting epoch 12/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 547.1827392578125
Running validation...
Epoch 12, Step 0: Train Loss = 546.1653442382812, Test Loss = 546.2623291015625
Starting epoch 13/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 546.1924438476562
Running validation...
Epoch 13, Step 0: Train Loss = 545.0310668945312, Test Loss = 545.1178588867188
  8%|▊         | 13/155 [00:01<00:15,  9.27it/s, epoch=12, test_loss=545, train_loss=545]Starting epoch 14/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 545.0499267578125
Running validation...
Epoch 14, Step 0: Train Loss = 543.9363403320312, Test Loss = 544.0667724609375
  9%|▉         | 14/155 [00:01<00:15,  9.29it/s, epoch=13, test_loss=544, train_loss=544]Starting epoch 15/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 543.8973999023438
Running validation...
Epoch 15, Step 0: Train Loss = 542.7127075195312, Test Loss = 542.625
 10%|▉         | 15/155 [00:01<00:15,  9.10it/s, epoch=14, test_loss=543, train_loss=543]

SOM initialization...

 22%|██▏       | 34/155 [00:02<00:06, 18.54it/s, epoch=3, test_loss=1.3, train_loss=1.29] 

Training...

 28%|██▊       | 44/155 [00:08<00:53,  2.06it/s, cah=[4.230843], cr_ratio=60, cs_ratio=1.12, epoch=8, ssom=[4.1552296], test_loss=523, train_loss=523, vae=[513.61847], vc_ratio=114]  =5.93e+4]    e+4]    