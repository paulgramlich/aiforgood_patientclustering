INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "24"
[ 0.          0.         -1.          0.          0.          0.66666667
  0.5         0.          0.          0.33333333  0.5         0.
  0.          0.16666667  0.          0.         -1.          0.
  0.16666667  0.66666667  0.5        -1.          0.16666667 -1.
  0.5         0.         -1.         -1.          0.16666667  0.
 -1.          0.5        -1.          0.          0.16666667 -1.
  0.16666667  0.5         0.16666667  0.          0.66666667  0.83333333
 -1.          0.5         0.         -1.         -1.         -1.
  0.          0.16666667  0.         -1.          0.         -1.
  0.          0.5         0.66666667  0.16666667  0.          0.83333333
  0.16666667  0.16666667 -1.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.          0.          0.5
 -1.          0.5        -1.          0.          0.5         0.16666667
  0.          0.16666667  0.5         0.5         0.16666667  0.
  0.         -1.         -1.         -1.         -1.          0.66666667
  0.          0.         -1.          0.5        -1.          0.16666667
 -1.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.         -1.          0.5         0.5         0.
  0.16666667  0.          0.16666667 -1.         -1.          0.16666667
  0.16666667 -1.          0.          0.         -1.         -1.
  0.          0.          0.          0.5         0.          0.16666667
 -1.         -1.          0.          0.5         0.         -1.
  0.         -1.          0.          0.         -1.          0.
  0.16666667 -1.          0.5        -1.          0.          0.16666667
 -1.          0.16666667  0.66666667 -1.          0.          0.
  0.16666667 -1.         -1.          0.5         0.          0.16666667
  0.          0.         -1.         -1.          0.5         0.
 -1.          0.5         0.16666667  0.          0.         -1.
  0.          0.16666667  0.83333333  0.16666667  0.16666667 -1.
  0.66666667  0.          0.          0.          0.          0.16666667
  0.          0.5         0.          0.          0.          0.
  0.5         0.          0.66666667  0.5        -1.          0.
  0.          0.66666667  0.          0.         -1.         -1.
  0.          0.         -1.          0.         -1.          0.
 -1.          0.5        -1.          0.          0.16666667 -1.
 -1.          0.83333333  0.          0.          0.83333333  0.
  0.16666667  0.83333333 -1.          0.5         0.          0.16666667
  0.83333333  0.         -1.          0.66666667  0.16666667  0.
  0.          0.          0.          0.16666667  0.5         0.66666667
  0.         -1.          0.          0.          0.16666667  0.16666667
  0.          0.          0.         -1.          0.          0.5
  0.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.83333333 -1.          0.16666667  0.          0.         -1.
  0.          0.          0.16666667  0.          0.          0.66666667
 -1.          0.          0.         -1.         -1.         -1.
 -1.          0.          0.         -1.          0.          0.16666667
  0.          0.         -1.          0.66666667 -1.          0.16666667
  0.         -1.          0.16666667  0.          0.5         0.66666667
 -1.          0.          0.16666667 -1.          0.          0.16666667
  0.          0.16666667  0.66666667  0.          0.          0.
  0.16666667 -1.         -1.          0.33333333  0.         -1.
 -1.          0.16666667  0.66666667 -1.          0.66666667  0.
  0.         -1.          0.         -1.          0.          0.
  0.          0.          0.         -1.          0.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  0.          0.         -1.         -1.
  0.5         0.16666667 -1.          0.          0.          0.
  0.16666667  0.          0.          0.5        -1.          0.
 -1.          0.         -1.          0.16666667 -1.         -1.
  0.16666667  0.5         0.          0.          0.5         0.16666667
  0.5         0.16666667  0.66666667 -1.          0.         -1.
  0.16666667  0.66666667  0.66666667  0.16666667 -1.          0.66666667
  0.16666667  0.         -1.          0.         -1.         -1.
  0.          0.16666667  0.          0.          0.66666667  0.66666667
  0.          0.16666667 -1.          0.16666667  0.          0.
  0.5         0.          0.66666667  0.          0.16666667  0.5
  0.16666667  0.5         0.16666667  0.         -1.          0.
  0.16666667  0.16666667  0.16666667  0.16666667  0.          0.66666667
 -1.          0.5         0.66666667  0.          0.          0.66666667
 -1.         -1.          0.          0.5        -1.          0.
  0.          0.16666667  0.          0.          0.5         0.66666667
  0.5         0.          0.          0.16666667  0.          0.
  0.16666667  0.          0.16666667  0.16666667  0.         -1.
  0.66666667  0.16666667 -1.          0.66666667  0.          0.
  0.5         0.         -1.          0.16666667  0.16666667 -1.
  0.66666667  0.          0.16666667  0.          0.          0.
  0.         -1.          0.          0.16666667  0.66666667  0.
  0.          0.5         0.         -1.          0.          0.16666667
  0.          0.16666667 -1.         -1.          0.16666667  0.66666667
  0.         -1.         -1.          0.         -1.          0.16666667
 -1.          0.16666667  0.         -1.          0.16666667  0.5
 -1.         -1.          0.16666667 -1.          0.16666667 -1.
 -1.          0.          0.          0.16666667  0.          0.
  0.          0.16666667  0.          0.33333333  0.          0.
  0.33333333  0.5         0.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.16666667 -1.         -1.
  0.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.16666667  0.5         0.16666667 -1.          0.
  0.          0.16666667  0.16666667  0.         -1.          0.5
  0.          0.16666667  0.          0.5         0.16666667  0.
  0.5         0.          0.          0.66666667 -1.         -1.
 -1.          0.          0.5         0.16666667  0.          0.
  0.          0.5        -1.         -1.          0.          0.
 -1.          0.16666667  0.          0.66666667 -1.         -1.
  0.16666667  0.         -1.          0.5        -1.          0.16666667
  0.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          0.          0.5         0.66666667
  0.          0.5         0.         -1.          0.         -1.
  0.16666667  0.66666667 -1.         -1.          0.          0.5
 -1.          0.5         0.         -1.          0.         -1.
 -1.          0.          0.          0.          0.16666667 -1.
 -1.          0.          0.          0.66666667  0.         -1.
 -1.         -1.          0.         -1.          0.16666667  0.
 -1.          0.16666667  0.66666667  0.          0.66666667 -1.
  0.16666667  0.16666667  0.5         0.          0.66666667  0.
 -1.          0.16666667 -1.          0.         -1.          0.5
  0.5         0.16666667  0.16666667 -1.          0.66666667  0.16666667
  0.83333333  0.16666667  0.          0.83333333  0.          0.16666667
  0.          0.66666667 -1.          0.16666667  0.          0.
  0.66666667 -1.         -1.          0.66666667  0.16666667  0.83333333
 -1.          1.         -1.         -1.          0.         -1.
  0.          0.16666667  0.          0.         -1.          0.5
  0.         -1.          0.16666667 -1.          0.16666667  0.5
 -1.          0.16666667  0.          0.          0.5         0.5
 -1.          0.          0.          0.16666667  0.66666667  0.16666667
  0.          0.          0.         -1.         -1.         -1.
  0.16666667 -1.          0.          0.         -1.          0.
  0.66666667  0.          0.          0.         -1.          0.
  0.16666667  0.16666667 -1.         -1.         -1.          0.66666667
  0.5        -1.          0.          0.          0.66666667  0.16666667
  0.          0.         -1.          0.16666667 -1.          0.
 -1.          0.16666667 -1.          0.         -1.         -1.
  0.          0.16666667 -1.          0.         -1.          0.16666667
  0.          0.5         0.          0.33333333  0.16666667  0.5
 -1.         -1.          0.          0.5        -1.          0.33333333
  0.         -1.          0.          0.16666667 -1.          0.
  0.16666667  0.          0.5         0.16666667 -1.          0.
  0.16666667  0.          0.         -1.         -1.         -1.
  0.          0.5         0.16666667 -1.          0.          0.5
  0.16666667  0.          0.5         0.16666667  0.5        -1.
 -1.         -1.          0.          0.16666667 -1.          0.16666667
  0.          0.16666667  0.5         0.16666667  0.5         0.
 -1.         -1.          0.5         0.66666667  0.16666667  0.
  0.5         0.5        -1.          0.         -1.          0.
  0.5         0.5         0.16666667  0.16666667  0.          0.
 -1.          0.          0.5        -1.          0.66666667  0.
  0.16666667  0.16666667  0.          0.          0.16666667  0.66666667
  0.16666667  0.16666667  0.66666667  0.          0.33333333  0.16666667
  0.          0.16666667  0.5        -1.          0.5         0.
  0.16666667 -1.          0.5         0.66666667  0.         -1.
  0.66666667  0.5        -1.         -1.          0.83333333  0.16666667
  0.16666667  0.          0.          0.66666667  0.          0.16666667
  0.5         0.          0.66666667 -1.         -1.         -1.
  1.          0.          0.          0.          0.          0.
  0.         -1.          0.         -1.         -1.          0.
  0.16666667  0.          0.16666667  0.          0.66666667  0.5
  0.          0.          0.          0.         -1.          0.16666667
  0.          0.16666667 -1.         -1.          0.         -1.
  0.5         0.         -1.          0.         -1.          0.
  0.          0.          0.          0.          0.         -1.
  0.         -1.         -1.          0.16666667  0.         -1.
  0.          0.16666667 -1.         -1.          0.66666667  0.16666667
 -1.          0.          0.16666667  0.         -1.          0.
  0.          0.5         0.16666667  0.          0.16666667  0.66666667
 -1.          0.16666667 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-18 15:10:27.063493: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-18_d20b6********* 

  0%|          | 0/6745 [00:00<?, ?it/s]Number of batches: 19


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 547.72265625
Running validation...
Epoch 1, Step 0: Train Loss = 546.9049682617188, Test Loss = 547.1630249023438
  0%|          | 1/6745 [00:00<27:43,  4.05it/s, epoch=0, test_loss=547, train_loss=547]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 547.0189208984375
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 546.3956909179688
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.9508666992188
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.4970703125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.15771484375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.7980346679688
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.5069580078125
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.122802734375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.8955078125
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.6456298828125
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.38720703125
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.0225830078125
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.7652587890625
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.4452514648438
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.1783447265625
  0%|          | 16/6745 [00:00<01:57, 57.38it/s, epoch=0, test_loss=547, train_loss=542]Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.7650756835938
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.355224609375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 540.859375
Starting epoch 2/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 540.4093017578125
Running validation...
Epoch 2, Step 0: Train Loss = 539.4838256835938, Test Loss = 539.1146850585938
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 539.5442504882812
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 538.2565307617188
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 538.1365356445312
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 536.9490966796875
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 536.4951171875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 534.4745483398438
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 533.4315185546875
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 532.404296875
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 529.9530029296875
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 525.0537109375
  0%|          | 30/6745 [00:00<01:20, 83.38it/s, epoch=1, test_loss=539, train_loss=525]Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 526.2614135742188
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 519.6222534179688
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 516.6856079101562
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 508.6494445800781
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 502.5243225097656
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 495.45159912109375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 493.510498046875
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 475.14654541015625
Starting epoch 3/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 464.34844970703125
Running validation...
Epoch 3, Step 0: Train Loss = 453.47943115234375, Test Loss = 459.1678466796875
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 446.3034973144531
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 411.50030517578125
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 393.9114685058594
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 386.8504333496094
  1%|          | 43/6745 [00:00<01:08, 98.03it/s, epoch=2, test_loss=459, train_loss=387]Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 388.0906982421875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 341.6806945800781
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 325.34832763671875
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 321.7932434082031
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 285.5275573730469
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 256.1271667480469
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 269.20611572265625
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.853759765625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 129.4800262451172
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 93.27377319335938
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.51644897460938
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.1820068359375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.3296356201172
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 46.28837585449219
  1%|          | 57/6745 [00:00<01:00, 111.12it/s, epoch=2, test_loss=459, train_loss=46.3]Starting epoch 4/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 27.940895080566406
Running validation...
Epoch 4, Step 0: Train Loss = 5.71319580078125, Test Loss = 45.28477096557617
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -56.77476501464844
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -108.2924575805664
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164.02386474609375
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -149.62371826171875
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -54.983001708984375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -95.54032897949219
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -142.25009155273438
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 105.24878692626953
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -133.62896728515625
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -203.06747436523438
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8.943519592285156
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -286.1212158203125
  1%|          | 70/6745 [00:00<00:57, 115.81it/s, epoch=3, test_loss=45.3, train_loss=-286]Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -831.9893188476562
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -907.06787109375
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -621.682373046875
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -554.2889404296875
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -393.8365478515625
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1026.60888671875
Starting epoch 5/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1374.5977783203125
Running validation...
Epoch 5, Step 0: Train Loss = -1102.249267578125, Test Loss = -2923.69677734375
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2112.310546875
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2344.397216796875
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4226.6044921875
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2911.7939453125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3303.0224609375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3333.330078125
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4444.1630859375
  1%|          | 84/6745 [00:00<00:55, 119.14it/s, epoch=4, test_loss=-2.92e+3, train_loss=-4.44e+3]Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -387.77239990234375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3782.73291015625
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4575.794921875
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1301.2745361328125
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7081.08447265625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11823.41796875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16802.53515625
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10925.908203125
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9649.2578125
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7160.859375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -22604.236328125
Starting epoch 6/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -20165.712890625
Running validation...
Epoch 6, Step 0: Train Loss = -20098.86328125, Test Loss = -37463.2890625
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42500.2578125
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36360.85546875
  1%|▏         | 98/6745 [00:00<00:53, 123.61it/s, epoch=5, test_loss=-3.75e+4, train_loss=-3.64e+4]Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -64650.20703125
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -45609.8203125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -44720.78125
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -38168.39453125
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -58073.59375
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6894.66796875
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -50269.5
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -56780.09765625
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16483.73046875
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -61240.40625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164188.796875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -192142.0625
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -129449.875
  2%|▏         | 111/6745 [00:01<00:56, 117.91it/s, epoch=5, test_loss=-3.75e+4, train_loss=-1.29e+5]Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -101198.8828125
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -73584.4296875
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -220815.03125
Starting epoch 7/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -194930.890625
Running validation...
Epoch 7, Step 0: Train Loss = -201720.65625, Test Loss = -178426.6875
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -378289.5
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -382605.4375
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -600786.5
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -383781.15625
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -379873.875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -349602.875
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -380344.75
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -45115.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -266621.71875
  2%|▏         | 124/6745 [00:01<01:00, 109.73it/s, epoch=6, test_loss=-1.78e+5, train_loss=-2.67e+5]Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -486113.84375
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -121802.71875
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -673279.5625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1048743.875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1081150.375
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1192991.25
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -465280.4375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -357573.375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1296339.75
Starting epoch 8/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1275638.0
Running validation...
Epoch 8, Step 0: Train Loss = -1338736.5, Test Loss = -1710606.375
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2412767.5
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2665411.0
  2%|▏         | 136/6745 [00:01<01:00, 108.90it/s, epoch=7, test_loss=-1.71e+6, train_loss=-2.67e+6]Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3172257.75
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2026342.5
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2360886.75
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1940635.75
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2797038.5
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -350825.125
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2295987.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2306805.5
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -691910.5625
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3127701.25
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6327953.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5697000.5
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4384390.5
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3730432.5
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2334931.0
  2%|▏         | 151/6745 [00:01<00:56, 117.74it/s, epoch=7, test_loss=-1.71e+6, train_loss=-2.33e+6]Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7286485.0
Starting epoch 9/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6462981.0
Running validation...
Epoch 9, Step 0: Train Loss = -7027909.5, Test Loss = -5182559.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11989474.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11397596.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -18537920.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12760576.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11831340.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8735044.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9717421.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1266155.375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10870212.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9423901.0
  2%|▏         | 163/6745 [00:01<00:56, 116.87it/s, epoch=8, test_loss=-5.18e+6, train_loss=-9.42e+6]Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3763274.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -15891853.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -21831008.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -25266684.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -20819910.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16159832.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10475923.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -25123440.0
Starting epoch 10/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -30629522.0
Running validation...
Epoch 10, Step 0: Train Loss = -31264076.0, Test Loss = -16700289.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -47690696.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -49371608.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -66022784.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42304160.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -51651400.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -37067748.0
  3%|▎         | 178/6745 [00:01<00:52, 124.74it/s, epoch=9, test_loss=-1.67e+7, train_loss=-3.71e+7]Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42154184.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4672983.5
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -31392942.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36817664.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12490858.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -55739596.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -95059376.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -100420784.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -72358168.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -47435212.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36933316.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -105674784.0
Starting epoch 11/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -105581288.0
Running validation...
Epoch 11, Step 0: Train Loss = -107645224.0, Test Loss = -47348016.0
  3%|▎         | 191/6745 [00:01<00:53, 122.36it/s, epoch=10, test_loss=-4.73e+7, train_loss=-1.08e+8]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -167170032.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -169187904.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -236688544.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164804128.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -179654576.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -132905744.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -146353312.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16518734.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -127807720.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -139157440.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -40951248.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -184495008.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -306799488.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -360300640.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -263916912.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -149435040.0
  3%|▎         | 207/6745 [00:01<00:49, 132.01it/s, epoch=10, test_loss=-4.73e+7, train_loss=-1.49e+8]Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -126757296.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -342039456.0
Starting epoch 12/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -301359456.0
Running validation...
Epoch 12, Step 0: Train Loss = -348182336.0, Test Loss = -473294912.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -525165760.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -570445632.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -726633728.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -498567488.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -530566048.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -380360384.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -478223360.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -50295576.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -383252896.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -361902592.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -131362120.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -522699200.0
  3%|▎         | 222/6745 [00:01<00:47, 136.66it/s, epoch=11, test_loss=-4.73e+8, train_loss=-5.23e+8]Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -935802304.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -882218048.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -775613120.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -476498496.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -313197184.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1017337920.0
Starting epoch 13/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -849398976.0
Running validation...
Epoch 13, Step 0: Train Loss = -933106688.0, Test Loss = -208748032.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1480766464.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1301373952.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2191039232.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1404136960.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1403421568.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -949826304.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1315981568.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -129470304.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -967009600.0
  4%|▎         | 238/6745 [00:02<00:46, 139.45it/s, epoch=12, test_loss=-2.09e+8, train_loss=-9.67e+8]Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1019010048.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -280233088.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1298979712.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2521062144.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2516123392.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1852917376.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1182737408.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -843271424.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2356545024.0
Starting epoch 14/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2495594496.0
Running validation...
Epoch 14, Step 0: Train Loss = -2588825088.0, Test Loss = -1496571392.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4132317184.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3820433920.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5288975360.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3723911424.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3393069568.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
  4%|▍         | 253/6745 [00:02<00:49, 131.02it/s, epoch=13, test_loss=-1.5e+9, train_loss=-3.39e+9] Train Loss: -2606440960.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3191878400.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -337737504.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2252061696.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2510252544.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -795351360.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3585154048.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5830885888.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6133655040.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4606285312.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3166077952.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1905243136.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5706739200.0
Starting epoch 15/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5973134336.0
Running validation...
Epoch 15, Step 0: Train Loss = -6054402048.0, Test Loss = -7550276608.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
  4%|▍         | 267/6745 [00:02<00:50, 128.04it/s, epoch=14, test_loss=-7.55e+9, train_loss=-6.05e+9]Train Loss: -9173644288.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9113729024.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11256199168.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9034121216.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7748928512.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6549342720.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6998781952.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -820712448.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6008066560.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6013573120.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1818929408.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8337475584.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -14067892224.0
  4%|▍         | 280/6745 [00:02<00:50, 128.14it/s, epoch=14, test_loss=-7.55e+9, train_loss=-1.41e+10]Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -14065863680.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11135331328.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6084350976.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4803056640.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12285235200.0


SOM initialization...

  9%|▉         | 609/6745 [00:03<00:10, 594.03it/s, epoch=2, test_loss=5.18e+9, train_loss=7.46e+9]    

Training...

 14%|█▍        | 932/6745 [00:09<02:19, 41.71it/s, cah=[nan], cr_ratio=nan, cs_ratio=nan, epoch=14, ssom=[nan], test_loss=nan, train_loss=nan, vae=[nan], vc_ratio=nan]]