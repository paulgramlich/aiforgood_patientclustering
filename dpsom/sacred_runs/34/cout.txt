INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "34"
using LBP
[ 1.          1.         -1.          1.          1.          0.66666667
  0.5         1.          1.          0.16666667  0.5         1.
  1.          0.83333333  1.          1.         -1.          1.
  0.83333333  0.66666667  0.5        -1.          0.83333333 -1.
  0.5         1.         -1.         -1.          0.83333333  1.
 -1.          0.5        -1.          1.          0.83333333 -1.
  0.83333333  0.5         0.83333333  1.          0.66666667  0.33333333
 -1.          0.5         1.         -1.         -1.         -1.
  1.          0.83333333  1.         -1.          1.         -1.
  1.          0.5         0.66666667  0.83333333  1.          0.33333333
  0.83333333  0.83333333 -1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         1.          1.          0.5
 -1.          0.5        -1.          1.          0.5         0.83333333
  1.          0.83333333  0.5         0.5         0.83333333  1.
  1.         -1.         -1.         -1.         -1.          0.66666667
  1.          1.         -1.          0.5        -1.          0.83333333
 -1.         -1.          1.          0.83333333  1.          0.83333333
  1.          1.         -1.          0.5         0.5         1.
  0.83333333  1.          0.83333333 -1.         -1.          0.83333333
  0.83333333 -1.          1.          1.         -1.         -1.
  1.          1.          1.          0.5         1.          0.83333333
 -1.         -1.          1.          0.5         1.         -1.
  1.         -1.          1.          1.         -1.          1.
  0.83333333 -1.          0.5        -1.          1.          0.83333333
 -1.          0.83333333  0.66666667 -1.          1.          1.
  0.83333333 -1.         -1.          0.5         1.          0.83333333
  1.          1.         -1.         -1.          0.5         1.
 -1.          0.5         0.83333333  1.          1.         -1.
  1.          0.83333333  0.33333333  0.83333333  0.83333333 -1.
  0.66666667  1.          1.          1.          1.          0.83333333
  1.          0.5         1.          1.          1.          1.
  0.5         1.          0.66666667  0.5        -1.          1.
  1.          0.66666667  1.          1.         -1.         -1.
  1.          1.         -1.          1.         -1.          1.
 -1.          0.5        -1.          1.          0.83333333 -1.
 -1.          0.33333333  1.          1.          0.33333333  1.
  0.83333333  0.33333333 -1.          0.5         1.          0.83333333
  0.33333333  1.         -1.          0.66666667  0.83333333  1.
  1.          1.          1.          0.83333333  0.5         0.66666667
  1.         -1.          1.          1.          0.83333333  0.83333333
  1.          1.          1.         -1.          1.          0.5
  1.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.33333333 -1.          0.83333333  1.          1.         -1.
  1.          1.          0.83333333  1.          1.          0.66666667
 -1.          1.          1.         -1.         -1.         -1.
 -1.          1.          1.         -1.          1.          0.83333333
  1.          1.         -1.          0.66666667 -1.          0.83333333
  1.         -1.          0.83333333  1.          0.5         0.66666667
 -1.          1.          0.83333333 -1.          1.          0.83333333
  1.          0.83333333  0.66666667  1.          1.          1.
  0.83333333 -1.         -1.          0.16666667  1.         -1.
 -1.          0.83333333  0.66666667 -1.          0.66666667  1.
  1.         -1.          1.         -1.          1.          1.
  1.          1.          1.         -1.          1.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  1.          1.         -1.         -1.
  0.5         0.83333333 -1.          1.          1.          1.
  0.83333333  1.          1.          0.5        -1.          1.
 -1.          1.         -1.          0.83333333 -1.         -1.
  0.83333333  0.5         1.          1.          0.5         0.83333333
  0.5         0.83333333  0.66666667 -1.          1.         -1.
  0.83333333  0.66666667  0.66666667  0.83333333 -1.          0.66666667
  0.83333333  1.         -1.          1.         -1.         -1.
  1.          0.83333333  1.          1.          0.66666667  0.66666667
  1.          0.83333333 -1.          0.83333333  1.          1.
  0.5         1.          0.66666667  1.          0.83333333  0.5
  0.83333333  0.5         0.83333333  1.         -1.          1.
  0.83333333  0.83333333  0.83333333  0.83333333  1.          0.66666667
 -1.          0.5         0.66666667  1.          1.          0.66666667
 -1.         -1.          1.          0.5        -1.          1.
  1.          0.83333333  1.          1.          0.5         0.66666667
  0.5         1.          1.          0.83333333  1.          1.
  0.83333333  1.          0.83333333  0.83333333  1.         -1.
  0.66666667  0.83333333 -1.          0.66666667  1.          1.
  0.5         1.         -1.          0.83333333  0.83333333 -1.
  0.66666667  1.          0.83333333  1.          1.          1.
  1.         -1.          1.          0.83333333  0.66666667  1.
  1.          0.5         1.         -1.          1.          0.83333333
  1.          0.83333333 -1.         -1.          0.83333333  0.66666667
  1.         -1.         -1.          1.         -1.          0.83333333
 -1.          0.83333333  1.         -1.          0.83333333  0.5
 -1.         -1.          0.83333333 -1.          0.83333333 -1.
 -1.          1.          1.          0.83333333  1.          1.
  1.          0.83333333  1.          0.16666667  1.          1.
  0.16666667  0.5         1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         0.83333333 -1.         -1.
  1.         -1.          1.          0.83333333  1.          0.83333333
  1.          0.83333333  0.5         0.83333333 -1.          1.
  1.          0.83333333  0.83333333  1.         -1.          0.5
  1.          0.83333333  1.          0.5         0.83333333  1.
  0.5         1.          1.          0.66666667 -1.         -1.
 -1.          1.          0.5         0.83333333  1.          1.
  1.          0.5        -1.         -1.          1.          1.
 -1.          0.83333333  1.          0.66666667 -1.         -1.
  0.83333333  1.         -1.          0.5        -1.          0.83333333
  1.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          1.          0.5         0.66666667
  1.          0.5         1.         -1.          1.         -1.
  0.83333333  0.66666667 -1.         -1.          1.          0.5
 -1.          0.5         1.         -1.          1.         -1.
 -1.          1.          1.          1.          0.83333333 -1.
 -1.          1.          1.          0.66666667  1.         -1.
 -1.         -1.          1.         -1.          0.83333333  1.
 -1.          0.83333333  0.66666667  1.          0.66666667 -1.
  0.83333333  0.83333333  0.5         1.          0.66666667  1.
 -1.          0.83333333 -1.          1.         -1.          0.5
  0.5         0.83333333  0.83333333 -1.          0.66666667  0.83333333
  0.33333333  0.83333333  1.          0.33333333  1.          0.83333333
  1.          0.66666667 -1.          0.83333333  1.          1.
  0.66666667 -1.         -1.          0.66666667  0.83333333  0.33333333
 -1.          0.         -1.         -1.          1.         -1.
  1.          0.83333333  1.          1.         -1.          0.5
  1.         -1.          0.83333333 -1.          0.83333333  0.5
 -1.          0.83333333  1.          1.          0.5         0.5
 -1.          1.          1.          0.83333333  0.66666667  0.83333333
  1.          1.          1.         -1.         -1.         -1.
  0.83333333 -1.          1.          1.         -1.          1.
  0.66666667  1.          1.          1.         -1.          1.
  0.83333333  0.83333333 -1.         -1.         -1.          0.66666667
  0.5        -1.          1.          1.          0.66666667  0.83333333
  1.          1.         -1.          0.83333333 -1.          1.
 -1.          0.83333333 -1.          1.         -1.         -1.
  1.          0.83333333 -1.          1.         -1.          0.83333333
  1.          0.5         1.          0.16666667  0.83333333  0.5
 -1.         -1.          1.          0.5        -1.          0.16666667
  1.         -1.          1.          0.83333333 -1.          1.
  0.83333333  1.          0.5         0.83333333 -1.          1.
  0.83333333  1.          1.         -1.         -1.         -1.
  1.          0.5         0.83333333 -1.          1.          0.5
  0.83333333  1.          0.5         0.83333333  0.5        -1.
 -1.         -1.          1.          0.83333333 -1.          0.83333333
  1.          0.83333333  0.5         0.83333333  0.5         1.
 -1.         -1.          0.5         0.66666667  0.83333333  1.
  0.5         0.5        -1.          1.         -1.          1.
  0.5         0.5         0.83333333  0.83333333  1.          1.
 -1.          1.          0.5        -1.          0.66666667  1.
  0.83333333  0.83333333  1.          1.          0.83333333  0.66666667
  0.83333333  0.83333333  0.66666667  1.          0.16666667  0.83333333
  1.          0.83333333  0.5        -1.          0.5         1.
  0.83333333 -1.          0.5         0.66666667  1.         -1.
  0.66666667  0.5        -1.         -1.          0.33333333  0.83333333
  0.83333333  1.          1.          0.66666667  1.          0.83333333
  0.5         1.          0.66666667 -1.         -1.         -1.
  0.          1.          1.          1.          1.          1.
  1.         -1.          1.         -1.         -1.          1.
  0.83333333  1.          0.83333333  1.          0.66666667  0.5
  1.          1.          1.          1.         -1.          0.83333333
  1.          0.83333333 -1.         -1.          1.         -1.
  0.5         1.         -1.          1.         -1.          1.
  1.          1.          1.          1.          1.         -1.
  1.         -1.         -1.          0.83333333  1.         -1.
  1.          0.83333333 -1.         -1.          0.66666667  0.83333333
 -1.          1.          0.83333333  1.         -1.          1.
  1.          0.5         0.83333333  1.          0.83333333  0.66666667
 -1.          0.83333333 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 15:11:00.680632: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_8-8_2024-06-19_1a53e********* 

  0%|          | 0/355 [00:00<?, ?it/s]Number of batches: 1


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 567.6574096679688
Running validation...
Epoch 1, Step 0: Train Loss = 562.88720703125, Test Loss = 563.052978515625
  0%|          | 1/355 [00:00<03:08,  1.88it/s, epoch=0, test_loss=563, train_loss=563]Starting epoch 2/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 562.7880859375
Running validation...
Epoch 2, Step 0: Train Loss = 559.4169921875, Test Loss = 559.54638671875
  1%|          | 2/355 [00:00<01:50,  3.19it/s, epoch=1, test_loss=560, train_loss=559]Starting epoch 3/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 559.3909301757812
Running validation...
Epoch 3, Step 0: Train Loss = 556.8450927734375, Test Loss = 557.0187377929688
  1%|          | 3/355 [00:00<01:23,  4.23it/s, epoch=2, test_loss=557, train_loss=557]Starting epoch 4/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 556.9512939453125
Running validation...
Epoch 4, Step 0: Train Loss = 555.0484619140625, Test Loss = 555.0929565429688
  1%|          | 4/355 [00:00<01:12,  4.84it/s, epoch=3, test_loss=555, train_loss=555]Starting epoch 5/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 554.9791870117188
Running validation...
Epoch 5, Step 0: Train Loss = 553.4345703125, Test Loss = 553.505126953125
  1%|▏         | 5/355 [00:01<01:05,  5.37it/s, epoch=4, test_loss=554, train_loss=553]Starting epoch 6/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 553.4385986328125
Running validation...
Epoch 6, Step 0: Train Loss = 552.14794921875, Test Loss = 552.2716064453125
  2%|▏         | 6/355 [00:01<00:59,  5.83it/s, epoch=5, test_loss=552, train_loss=552]Starting epoch 7/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 552.0202026367188
Running validation...
Epoch 7, Step 0: Train Loss = 551.0560302734375, Test Loss = 551.0626831054688
  2%|▏         | 7/355 [00:01<00:58,  5.91it/s, epoch=6, test_loss=551, train_loss=551]Starting epoch 8/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 551.0075073242188
Running validation...
Epoch 8, Step 0: Train Loss = 550.0365600585938, Test Loss = 550.0254516601562
  2%|▏         | 8/355 [00:01<00:55,  6.26it/s, epoch=7, test_loss=550, train_loss=550]Starting epoch 9/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 550.0023193359375
Running validation...
Epoch 9, Step 0: Train Loss = 548.998046875, Test Loss = 549.129150390625
  3%|▎         | 9/355 [00:01<00:54,  6.36it/s, epoch=8, test_loss=549, train_loss=549]Starting epoch 10/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 548.939208984375
Running validation...
Epoch 10, Step 0: Train Loss = 548.0218505859375, Test Loss = 548.1030883789062
  3%|▎         | 10/355 [00:01<00:54,  6.31it/s, epoch=9, test_loss=548, train_loss=548]Starting epoch 11/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 547.9746704101562
Running validation...
Epoch 11, Step 0: Train Loss = 547.0679931640625, Test Loss = 547.074951171875
  3%|▎         | 11/355 [00:02<00:53,  6.43it/s, epoch=10, test_loss=547, train_loss=547]Starting epoch 12/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 546.9853515625
Running validation...
Epoch 12, Step 0: Train Loss = 545.9312744140625, Test Loss = 545.950927734375
  3%|▎         | 12/355 [00:02<00:54,  6.31it/s, epoch=11, test_loss=546, train_loss=546]Starting epoch 13/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 545.9402465820312
Running validation...
Epoch 13, Step 0: Train Loss = 544.8202514648438, Test Loss = 544.8952026367188
  4%|▎         | 13/355 [00:02<00:52,  6.56it/s, epoch=12, test_loss=545, train_loss=545]Starting epoch 14/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 544.7661743164062
Running validation...
Epoch 14, Step 0: Train Loss = 543.5076904296875, Test Loss = 543.5904541015625
  4%|▍         | 14/355 [00:02<00:52,  6.54it/s, epoch=13, test_loss=544, train_loss=544]Starting epoch 15/15
Batch 1/1
Batch data shape: (300, 28, 28, 1)
Train Loss: 543.6530151367188
Running validation...
Epoch 15, Step 0: Train Loss = 542.2598266601562, Test Loss = 542.2803955078125
  4%|▍         | 15/355 [00:02<00:51,  6.62it/s, epoch=14, test_loss=542, train_loss=542]

SOM initialization...

 10%|▉         | 34/355 [00:04<00:25, 12.80it/s, epoch=3, test_loss=1.26, train_loss=1.29]

Training...

 12%|█▏        | 42/355 [00:08<02:40,  1.95it/s, cah=[4.114307], cr_ratio=62.4, cs_ratio=1.05, epoch=6, ssom=[4.155761], test_loss=531, train_loss=531, vae=[522.87744], vc_ratio=122]  .48e+4]   5.18e+4]+4]    +8] 