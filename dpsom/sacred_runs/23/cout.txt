INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "23"
[ 0.          0.         -1.          0.          0.          0.66666667
  0.5         0.          0.          0.33333333  0.5         0.
  0.          0.16666667  0.          0.         -1.          0.
  0.16666667  0.66666667  0.5        -1.          0.16666667 -1.
  0.5         0.         -1.         -1.          0.16666667  0.
 -1.          0.5        -1.          0.          0.16666667 -1.
  0.16666667  0.5         0.16666667  0.          0.66666667  0.83333333
 -1.          0.5         0.         -1.         -1.         -1.
  0.          0.16666667  0.         -1.          0.         -1.
  0.          0.5         0.66666667  0.16666667  0.          0.83333333
  0.16666667  0.16666667 -1.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.          0.          0.5
 -1.          0.5        -1.          0.          0.5         0.16666667
  0.          0.16666667  0.5         0.5         0.16666667  0.
  0.         -1.         -1.         -1.         -1.          0.66666667
  0.          0.         -1.          0.5        -1.          0.16666667
 -1.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.         -1.          0.5         0.5         0.
  0.16666667  0.          0.16666667 -1.         -1.          0.16666667
  0.16666667 -1.          0.          0.         -1.         -1.
  0.          0.          0.          0.5         0.          0.16666667
 -1.         -1.          0.          0.5         0.         -1.
  0.         -1.          0.          0.         -1.          0.
  0.16666667 -1.          0.5        -1.          0.          0.16666667
 -1.          0.16666667  0.66666667 -1.          0.          0.
  0.16666667 -1.         -1.          0.5         0.          0.16666667
  0.          0.         -1.         -1.          0.5         0.
 -1.          0.5         0.16666667  0.          0.         -1.
  0.          0.16666667  0.83333333  0.16666667  0.16666667 -1.
  0.66666667  0.          0.          0.          0.          0.16666667
  0.          0.5         0.          0.          0.          0.
  0.5         0.          0.66666667  0.5        -1.          0.
  0.          0.66666667  0.          0.         -1.         -1.
  0.          0.         -1.          0.         -1.          0.
 -1.          0.5        -1.          0.          0.16666667 -1.
 -1.          0.83333333  0.          0.          0.83333333  0.
  0.16666667  0.83333333 -1.          0.5         0.          0.16666667
  0.83333333  0.         -1.          0.66666667  0.16666667  0.
  0.          0.          0.          0.16666667  0.5         0.66666667
  0.         -1.          0.          0.          0.16666667  0.16666667
  0.          0.          0.         -1.          0.          0.5
  0.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.83333333 -1.          0.16666667  0.          0.         -1.
  0.          0.          0.16666667  0.          0.          0.66666667
 -1.          0.          0.         -1.         -1.         -1.
 -1.          0.          0.         -1.          0.          0.16666667
  0.          0.         -1.          0.66666667 -1.          0.16666667
  0.         -1.          0.16666667  0.          0.5         0.66666667
 -1.          0.          0.16666667 -1.          0.          0.16666667
  0.          0.16666667  0.66666667  0.          0.          0.
  0.16666667 -1.         -1.          0.33333333  0.         -1.
 -1.          0.16666667  0.66666667 -1.          0.66666667  0.
  0.         -1.          0.         -1.          0.          0.
  0.          0.          0.         -1.          0.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  0.          0.         -1.         -1.
  0.5         0.16666667 -1.          0.          0.          0.
  0.16666667  0.          0.          0.5        -1.          0.
 -1.          0.         -1.          0.16666667 -1.         -1.
  0.16666667  0.5         0.          0.          0.5         0.16666667
  0.5         0.16666667  0.66666667 -1.          0.         -1.
  0.16666667  0.66666667  0.66666667  0.16666667 -1.          0.66666667
  0.16666667  0.         -1.          0.         -1.         -1.
  0.          0.16666667  0.          0.          0.66666667  0.66666667
  0.          0.16666667 -1.          0.16666667  0.          0.
  0.5         0.          0.66666667  0.          0.16666667  0.5
  0.16666667  0.5         0.16666667  0.         -1.          0.
  0.16666667  0.16666667  0.16666667  0.16666667  0.          0.66666667
 -1.          0.5         0.66666667  0.          0.          0.66666667
 -1.         -1.          0.          0.5        -1.          0.
  0.          0.16666667  0.          0.          0.5         0.66666667
  0.5         0.          0.          0.16666667  0.          0.
  0.16666667  0.          0.16666667  0.16666667  0.         -1.
  0.66666667  0.16666667 -1.          0.66666667  0.          0.
  0.5         0.         -1.          0.16666667  0.16666667 -1.
  0.66666667  0.          0.16666667  0.          0.          0.
  0.         -1.          0.          0.16666667  0.66666667  0.
  0.          0.5         0.         -1.          0.          0.16666667
  0.          0.16666667 -1.         -1.          0.16666667  0.66666667
  0.         -1.         -1.          0.         -1.          0.16666667
 -1.          0.16666667  0.         -1.          0.16666667  0.5
 -1.         -1.          0.16666667 -1.          0.16666667 -1.
 -1.          0.          0.          0.16666667  0.          0.
  0.          0.16666667  0.          0.33333333  0.          0.
  0.33333333  0.5         0.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.16666667 -1.         -1.
  0.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.16666667  0.5         0.16666667 -1.          0.
  0.          0.16666667  0.16666667  0.         -1.          0.5
  0.          0.16666667  0.          0.5         0.16666667  0.
  0.5         0.          0.          0.66666667 -1.         -1.
 -1.          0.          0.5         0.16666667  0.          0.
  0.          0.5        -1.         -1.          0.          0.
 -1.          0.16666667  0.          0.66666667 -1.         -1.
  0.16666667  0.         -1.          0.5        -1.          0.16666667
  0.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          0.          0.5         0.66666667
  0.          0.5         0.         -1.          0.         -1.
  0.16666667  0.66666667 -1.         -1.          0.          0.5
 -1.          0.5         0.         -1.          0.         -1.
 -1.          0.          0.          0.          0.16666667 -1.
 -1.          0.          0.          0.66666667  0.         -1.
 -1.         -1.          0.         -1.          0.16666667  0.
 -1.          0.16666667  0.66666667  0.          0.66666667 -1.
  0.16666667  0.16666667  0.5         0.          0.66666667  0.
 -1.          0.16666667 -1.          0.         -1.          0.5
  0.5         0.16666667  0.16666667 -1.          0.66666667  0.16666667
  0.83333333  0.16666667  0.          0.83333333  0.          0.16666667
  0.          0.66666667 -1.          0.16666667  0.          0.
  0.66666667 -1.         -1.          0.66666667  0.16666667  0.83333333
 -1.          1.         -1.         -1.          0.         -1.
  0.          0.16666667  0.          0.         -1.          0.5
  0.         -1.          0.16666667 -1.          0.16666667  0.5
 -1.          0.16666667  0.          0.          0.5         0.5
 -1.          0.          0.          0.16666667  0.66666667  0.16666667
  0.          0.          0.         -1.         -1.         -1.
  0.16666667 -1.          0.          0.         -1.          0.
  0.66666667  0.          0.          0.         -1.          0.
  0.16666667  0.16666667 -1.         -1.         -1.          0.66666667
  0.5        -1.          0.          0.          0.66666667  0.16666667
  0.          0.         -1.          0.16666667 -1.          0.
 -1.          0.16666667 -1.          0.         -1.         -1.
  0.          0.16666667 -1.          0.         -1.          0.16666667
  0.          0.5         0.          0.33333333  0.16666667  0.5
 -1.         -1.          0.          0.5        -1.          0.33333333
  0.         -1.          0.          0.16666667 -1.          0.
  0.16666667  0.          0.5         0.16666667 -1.          0.
  0.16666667  0.          0.         -1.         -1.         -1.
  0.          0.5         0.16666667 -1.          0.          0.5
  0.16666667  0.          0.5         0.16666667  0.5        -1.
 -1.         -1.          0.          0.16666667 -1.          0.16666667
  0.          0.16666667  0.5         0.16666667  0.5         0.
 -1.         -1.          0.5         0.66666667  0.16666667  0.
  0.5         0.5        -1.          0.         -1.          0.
  0.5         0.5         0.16666667  0.16666667  0.          0.
 -1.          0.          0.5        -1.          0.66666667  0.
  0.16666667  0.16666667  0.          0.          0.16666667  0.66666667
  0.16666667  0.16666667  0.66666667  0.          0.33333333  0.16666667
  0.          0.16666667  0.5        -1.          0.5         0.
  0.16666667 -1.          0.5         0.66666667  0.         -1.
  0.66666667  0.5        -1.         -1.          0.83333333  0.16666667
  0.16666667  0.          0.          0.66666667  0.          0.16666667
  0.5         0.          0.66666667 -1.         -1.         -1.
  1.          0.          0.          0.          0.          0.
  0.         -1.          0.         -1.         -1.          0.
  0.16666667  0.          0.16666667  0.          0.66666667  0.5
  0.          0.          0.          0.         -1.          0.16666667
  0.          0.16666667 -1.         -1.          0.         -1.
  0.5         0.         -1.          0.         -1.          0.
  0.          0.          0.          0.          0.         -1.
  0.         -1.         -1.          0.16666667  0.         -1.
  0.          0.16666667 -1.         -1.          0.66666667  0.16666667
 -1.          0.          0.16666667  0.         -1.          0.
  0.          0.5         0.16666667  0.          0.16666667  0.66666667
 -1.          0.16666667 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-12 20:31:00.686825: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-12_189a0********* 

  0%|          | 0/10545 [00:00<?, ?it/s]Number of batches: 19


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 547.72265625
Running validation...
Epoch 1, Step 0: Train Loss = 546.9049682617188, Test Loss = 547.1630249023438
  0%|          | 1/10545 [00:00<36:55,  4.76it/s, epoch=0, test_loss=547, train_loss=547]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 547.0189208984375
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 546.3956909179688
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.9508666992188
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.4970703125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.15771484375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.7980346679688
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.5069580078125
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.122802734375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.8955078125
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.6456298828125
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.38720703125
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.0225830078125
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.7652587890625
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.4452514648438
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.1783447265625
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.7650756835938
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.355224609375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 540.859375
  0%|          | 19/10545 [00:00<02:21, 74.34it/s, epoch=0, test_loss=547, train_loss=541]Starting epoch 2/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 540.4093017578125
Running validation...
Epoch 2, Step 0: Train Loss = 539.4838256835938, Test Loss = 539.1146850585938
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 539.5442504882812
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 538.2565307617188
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 538.1365356445312
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 536.9490966796875
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 536.4951171875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 534.4745483398438
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 533.4315185546875
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 532.404296875
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 529.9530029296875
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 525.0537109375
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 526.2614135742188
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 519.6222534179688
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 516.6856079101562
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 508.6494445800781
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 502.5243225097656
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 495.45159912109375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 493.510498046875
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 475.14654541015625
Starting epoch 3/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 464.34844970703125
Running validation...
Epoch 3, Step 0: Train Loss = 453.47943115234375, Test Loss = 459.1678466796875
  0%|          | 39/10545 [00:00<01:28, 118.68it/s, epoch=2, test_loss=459, train_loss=453]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 446.3034973144531
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 411.50030517578125
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 393.9114685058594
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 386.8504333496094
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 388.0906982421875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 341.6806945800781
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 325.34832763671875
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 321.7932434082031
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 285.5275573730469
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 256.1271667480469
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 269.20611572265625
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.853759765625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 129.4800262451172
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 93.27377319335938
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.51644897460938
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.1820068359375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.3296356201172
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 46.28837585449219
Starting epoch 4/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 27.940895080566406
Running validation...
Epoch 4, Step 0: Train Loss = 5.71319580078125, Test Loss = 45.28477096557617
  1%|          | 58/10545 [00:00<01:15, 139.18it/s, epoch=3, test_loss=45.3, train_loss=5.71]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -56.77476501464844
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -108.2924575805664
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164.02386474609375
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -149.62371826171875
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -54.983001708984375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -95.54032897949219
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -142.25009155273438
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: 105.24878692626953
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -133.62896728515625
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -203.06747436523438
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8.943519592285156
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -286.1212158203125
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -831.9893188476562
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -907.06787109375
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -621.682373046875
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -554.2889404296875
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -393.8365478515625
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1026.60888671875
Starting epoch 5/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1374.5977783203125
Running validation...
Epoch 5, Step 0: Train Loss = -1102.249267578125, Test Loss = -2923.69677734375
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2112.310546875
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2344.397216796875
  1%|          | 79/10545 [00:00<01:05, 159.28it/s, epoch=4, test_loss=-2.92e+3, train_loss=-2.34e+3]Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4226.6044921875
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2911.7939453125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3303.0224609375
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3333.330078125
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4444.1630859375
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -387.77239990234375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3782.73291015625
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4575.794921875
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1301.2745361328125
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7081.08447265625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11823.41796875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16802.53515625
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10925.908203125
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9649.2578125
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7160.859375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -22604.236328125
Starting epoch 6/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -20165.712890625
Running validation...
Epoch 6, Step 0: Train Loss = -20098.86328125, Test Loss = -37463.2890625
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42500.2578125
  1%|          | 97/10545 [00:00<01:03, 164.78it/s, epoch=5, test_loss=-3.75e+4, train_loss=-4.25e+4]Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36360.85546875
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -64650.20703125
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -45609.8203125
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -44720.78125
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -38168.39453125
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -58073.59375
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6894.66796875
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -50269.5
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -56780.09765625
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16483.73046875
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -61240.40625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164188.796875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -192142.0625
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -129449.875
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -101198.8828125
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -73584.4296875
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -220815.03125
Starting epoch 7/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -194930.890625
Running validation...
Epoch 7, Step 0: Train Loss = -201720.65625, Test Loss = -178426.6875
  1%|          | 115/10545 [00:00<01:01, 168.96it/s, epoch=6, test_loss=-1.78e+5, train_loss=-2.02e+5]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -378289.5
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -382605.4375
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -600786.5
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -383781.15625
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -379873.875
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -349602.875
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -380344.75
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -45115.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -266621.71875
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -486113.84375
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -121802.71875
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -673279.5625
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1048743.875
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1081150.375
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1192991.25
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -465280.4375
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -357573.375
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1296339.75
Starting epoch 8/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1275638.0
Running validation...
Epoch 8, Step 0: Train Loss = -1338736.5, Test Loss = -1710606.375
  1%|▏         | 134/10545 [00:00<01:00, 172.95it/s, epoch=7, test_loss=-1.71e+6, train_loss=-1.34e+6]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2412767.5
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2665411.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3172257.75
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2026342.5
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2360886.75
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1940635.75
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2797038.5
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -350825.125
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2295987.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2306805.5
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -691910.5625
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3127701.25
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6327953.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5697000.5
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4384390.5
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3730432.5
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2334931.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7286485.0
Starting epoch 9/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6462981.0
Running validation...
Epoch 9, Step 0: Train Loss = -7027909.5, Test Loss = -5182559.0
  1%|▏         | 153/10545 [00:01<00:58, 177.78it/s, epoch=8, test_loss=-5.18e+6, train_loss=-7.03e+6]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11989474.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11397596.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -18537920.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12760576.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11831340.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8735044.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9717421.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1266155.375
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10870212.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9423901.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3763274.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -15891853.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -21831008.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -25266684.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -20819910.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16159832.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -10475923.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -25123440.0
Starting epoch 10/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -30629522.0
Running validation...
Epoch 10, Step 0: Train Loss = -31264076.0, Test Loss = -16700289.0
  2%|▏         | 172/10545 [00:01<00:57, 180.84it/s, epoch=9, test_loss=-1.67e+7, train_loss=-3.13e+7]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -47690696.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -49371608.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -66022784.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42304160.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -51651400.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -37067748.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -42154184.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4672983.5
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -31392942.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36817664.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12490858.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -55739596.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -95059376.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -100420784.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -72358168.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -47435212.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -36933316.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -105674784.0
Starting epoch 11/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -105581288.0
Running validation...
Epoch 11, Step 0: Train Loss = -107645224.0, Test Loss = -47348016.0
  2%|▏         | 191/10545 [00:01<00:57, 179.80it/s, epoch=10, test_loss=-4.73e+7, train_loss=-1.08e+8]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -167170032.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -169187904.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -236688544.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -164804128.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -179654576.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -132905744.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -146353312.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -16518734.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -127807720.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -139157440.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -40951248.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -184495008.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -306799488.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -360300640.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -263916912.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -149435040.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -126757296.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -342039456.0
Starting epoch 12/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -301359456.0
Running validation...
Epoch 12, Step 0: Train Loss = -348182336.0, Test Loss = -473294912.0
  2%|▏         | 210/10545 [00:01<00:58, 177.33it/s, epoch=11, test_loss=-4.73e+8, train_loss=-3.48e+8]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -525165760.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -570445632.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -726633728.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -498567488.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -530566048.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -380360384.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -478223360.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -50295576.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -383252896.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -361902592.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -131362120.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -522699200.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -935802304.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -882218048.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -775613120.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -476498496.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -313197184.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1017337920.0
Starting epoch 13/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -849398976.0
Running validation...
Epoch 13, Step 0: Train Loss = -933106688.0, Test Loss = -208748032.0
  2%|▏         | 229/10545 [00:01<00:58, 176.75it/s, epoch=12, test_loss=-2.09e+8, train_loss=-9.33e+8]Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1480766464.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1301373952.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2191039232.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1404136960.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1403421568.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -949826304.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1315981568.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -129470304.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -967009600.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1019010048.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -280233088.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1298979712.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2521062144.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2516123392.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1852917376.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1182737408.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -843271424.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2356545024.0
  2%|▏         | 247/10545 [00:01<00:58, 177.40it/s, epoch=12, test_loss=-2.09e+8, train_loss=-2.36e+9]Starting epoch 14/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2495594496.0
Running validation...
Epoch 14, Step 0: Train Loss = -2588825088.0, Test Loss = -1496571392.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4132317184.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3820433920.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5288975360.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3723911424.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3393069568.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2606440960.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3191878400.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -337737504.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2252061696.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -2510252544.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -795351360.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3585154048.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5830885888.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6133655040.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4606285312.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -3166077952.0
Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1905243136.0
  3%|▎         | 265/10545 [00:01<00:58, 175.30it/s, epoch=13, test_loss=-1.5e+9, train_loss=-1.91e+9] Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5706739200.0
Starting epoch 15/15
Batch 1/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -5973134336.0
Running validation...
Epoch 15, Step 0: Train Loss = -6054402048.0, Test Loss = -7550276608.0
Batch 2/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9173644288.0
Batch 3/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9113729024.0
Batch 4/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11256199168.0
Batch 5/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -9034121216.0
Batch 6/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -7748928512.0
Batch 7/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6549342720.0
Batch 8/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6998781952.0
Batch 9/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -820712448.0
Batch 10/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6008066560.0
Batch 11/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6013573120.0
Batch 12/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -1818929408.0
Batch 13/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -8337475584.0
Batch 14/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -14067892224.0
Batch 15/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -14065863680.0
Batch 16/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -11135331328.0
Batch 17/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -6084350976.0
  3%|▎         | 283/10545 [00:01<00:58, 174.18it/s, epoch=14, test_loss=-7.55e+9, train_loss=-6.08e+9]Batch 18/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -4803056640.0
Batch 19/19
Batch data shape: (20, 28, 28, 1)
Train Loss: -12285235200.0


SOM initialization...

  6%|▌         | 634/10545 [00:02<00:12, 766.51it/s, epoch=3, test_loss=4.17e+9, train_loss=6.87e+9]   

Training...

 10%|▉         | 1008/10545 [00:09<03:28, 45.69it/s, cah=[nan], cr_ratio=nan, cs_ratio=nan, epoch=18, ssom=[nan], test_loss=nan, train_loss=nan, vae=[nan], vc_ratio=nan]]]INFO:tensorflow:Restoring parameters from ../models/hyperopt_10_8-8_2024-06-12_189a0/hyperopt_10_8-8_2024-06-12_189a0.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_10_8-8_2024-06-12_189a0/hyperopt_10_8-8_2024-06-12_189a0.ckpt
Evaluation...
/Users/paulgramlich/PycharmProjects/pythonProject/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and binary values for target
  warnings.warn(msg, UserWarning)
/Users/paulgramlich/PycharmProjects/pythonProject/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received binary values for label, and continuous values for target
  warnings.warn(msg, UserWarning)
 96%|█████████▋| 10165/10545 [03:31<00:07, 48.17it/s, cah=[nan], cr_ratio=nan, cs_ratio=nan, epoch=499, ssom=[nan], test_loss=nan, train_loss=nan, vae=[nan], vc_ratio=nan]

 NMI: 0.0, AMI: 0.0, PUR: 0.7152173913043478.  Name: %r.


 Time: 211.69688606262207
INFO - hyperopt - Result: {'NMI': 0.0, 'Purity': 0.7152173913043478, 'AMI': 0.0}
INFO - hyperopt - Completed after 0:03:32
