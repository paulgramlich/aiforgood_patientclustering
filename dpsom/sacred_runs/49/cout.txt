INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "49"
using LBP
[8. 8. 0. 8. 8. 6. 5. 8. 8. 3. 5. 8. 8. 7. 8. 8. 0. 8. 7. 6. 5. 0. 7. 0.
 5. 8. 0. 0. 7. 8. 0. 5. 0. 8. 7. 0. 7. 5. 7. 8. 6. 4. 0. 5. 8. 0. 0. 0.
 8. 7. 8. 0. 8. 0. 8. 5. 6. 7. 8. 4. 7. 7. 0. 7. 6. 0. 7. 8. 5. 8. 8. 5.
 0. 5. 0. 8. 5. 7. 8. 7. 5. 5. 7. 8. 8. 0. 0. 0. 0. 6. 8. 8. 0. 5. 0. 7.
 0. 0. 8. 7. 8. 7. 8. 8. 0. 5. 5. 8. 7. 8. 7. 0. 0. 7. 7. 0. 8. 8. 0. 0.
 8. 8. 8. 5. 8. 7. 0. 0. 8. 5. 8. 0. 8. 0. 8. 8. 0. 8. 7. 0. 5. 0. 8. 7.
 0. 7. 6. 0. 8. 8. 7. 0. 0. 5. 8. 7. 8. 8. 0. 0. 5. 8. 0. 5. 7. 8. 8. 0.
 8. 7. 4. 7. 7. 0. 6. 8. 8. 8. 8. 7. 8. 5. 8. 8. 8. 8. 5. 8. 6. 5. 0. 8.
 8. 6. 8. 8. 0. 0. 8. 8. 0. 8. 0. 8. 0. 5. 0. 8. 7. 0. 0. 4. 8. 8. 4. 8.
 7. 4. 0. 5. 8. 7. 4. 8. 0. 6. 7. 8. 8. 8. 8. 7. 5. 6. 8. 0. 8. 8. 7. 7.
 8. 8. 8. 0. 8. 5. 8. 0. 6. 6. 0. 6. 4. 0. 7. 8. 8. 0. 8. 8. 7. 8. 8. 6.
 0. 8. 8. 0. 0. 0. 0. 8. 8. 0. 8. 7. 8. 8. 0. 6. 0. 7. 8. 0. 7. 8. 5. 6.
 0. 8. 7. 0. 8. 7. 8. 7. 6. 8. 8. 8. 7. 0. 0. 3. 8. 0. 0. 7. 6. 0. 6. 8.
 8. 0. 8. 0. 8. 8. 8. 8. 8. 0. 8. 0. 0. 6. 0. 0. 0. 0. 0. 6. 8. 8. 0. 0.
 5. 7. 0. 8. 8. 8. 7. 8. 8. 5. 0. 8. 0. 8. 0. 7. 0. 0. 7. 5. 8. 8. 5. 7.
 5. 7. 6. 0. 8. 0. 7. 6. 6. 7. 0. 6. 7. 8. 0. 8. 0. 0. 8. 7. 8. 8. 6. 6.
 8. 7. 0. 7. 8. 8. 5. 8. 6. 8. 7. 5. 7. 5. 7. 8. 0. 8. 7. 7. 7. 7. 8. 6.
 0. 5. 6. 8. 8. 6. 0. 0. 8. 5. 0. 8. 8. 7. 8. 8. 5. 6. 5. 8. 8. 7. 8. 8.
 7. 8. 7. 7. 8. 0. 6. 7. 0. 6. 8. 8. 5. 8. 0. 7. 7. 0. 6. 8. 7. 8. 8. 8.
 8. 0. 8. 7. 6. 8. 8. 5. 8. 0. 8. 7. 8. 7. 0. 0. 7. 6. 8. 0. 0. 8. 0. 7.
 0. 7. 8. 0. 7. 5. 0. 0. 7. 0. 7. 0. 0. 8. 8. 7. 8. 8. 8. 7. 8. 3. 8. 8.
 3. 5. 8. 7. 6. 0. 7. 8. 5. 7. 0. 0. 8. 0. 8. 7. 8. 7. 8. 7. 5. 7. 0. 8.
 8. 7. 7. 8. 0. 5. 8. 7. 8. 5. 7. 8. 5. 8. 8. 6. 0. 0. 0. 8. 5. 7. 8. 8.
 8. 5. 0. 0. 8. 8. 0. 7. 8. 6. 0. 0. 7. 8. 0. 5. 0. 7. 8. 0. 0. 0. 5. 0.
 0. 6. 0. 8. 5. 6. 8. 5. 8. 0. 8. 0. 7. 6. 0. 0. 8. 5. 0. 5. 8. 0. 8. 0.
 0. 8. 8. 8. 7. 0. 0. 8. 8. 6. 8. 0. 0. 0. 8. 0. 7. 8. 0. 7. 6. 8. 6. 0.
 7. 7. 5. 8. 6. 8. 0. 7. 0. 8. 0. 5. 5. 7. 7. 0. 6. 7. 4. 7. 8. 4. 8. 7.
 8. 6. 0. 7. 8. 8. 6. 0. 0. 6. 7. 4. 0. 2. 0. 0. 8. 0. 8. 7. 8. 8. 0. 5.
 8. 0. 7. 0. 7. 5. 0. 7. 8. 8. 5. 5. 0. 8. 8. 7. 6. 7. 8. 8. 8. 0. 0. 0.
 7. 0. 8. 8. 0. 8. 6. 8. 8. 8. 0. 8. 7. 7. 0. 0. 0. 6. 5. 0. 8. 8. 6. 7.
 8. 8. 0. 7. 0. 8. 0. 7. 0. 8. 0. 0. 8. 7. 0. 8. 0. 7. 8. 5. 8. 3. 7. 5.
 0. 0. 8. 5. 0. 3. 8. 0. 8. 7. 0. 8. 7. 8. 5. 7. 0. 8. 7. 8. 8. 0. 0. 0.
 8. 5. 7. 0. 8. 5. 7. 8. 5. 7. 5. 0. 0. 0. 8. 7. 0. 7. 8. 7. 5. 7. 5. 8.
 0. 0. 5. 6. 7. 8. 5. 5. 0. 8. 0. 8. 5. 5. 7. 7. 8. 8. 0. 8. 5. 0. 6. 8.
 7. 7. 8. 8. 7. 6. 7. 7. 6. 8. 3. 7. 8. 7. 5. 0. 5. 8. 7. 0. 5. 6. 8. 0.
 6. 5. 0. 0. 4. 7. 7. 8. 8. 6. 8. 7. 5. 8. 6. 0. 0. 0. 2. 8. 8. 8. 8. 8.
 8. 0. 8. 0. 0. 8. 7. 8. 7. 8. 6. 5. 8. 8. 8. 8. 0. 7. 8. 7. 0. 0. 8. 0.
 5. 8. 0. 8. 0. 8. 8. 8. 8. 8. 8. 0. 8. 0. 0. 7. 8. 0. 8. 7. 0. 0. 6. 7.
 0. 8. 7. 8. 0. 8. 8. 5. 7. 8. 7. 6. 0. 7. 0. 6.]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 16:51:28.081697: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_8-8_2024-06-19_0cc8d********* 

  0%|          | 0/135 [00:00<?, ?it/s]Number of batches: 1


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 567.9610595703125
Running validation...
Epoch 1, Step 0: Train Loss = 563.050048828125, Test Loss = 563.2180786132812
  1%|          | 1/135 [00:00<00:53,  2.52it/s, epoch=0, test_loss=563, train_loss=563]Starting epoch 2/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 563.0448608398438
Running validation...
Epoch 2, Step 0: Train Loss = 559.7911376953125, Test Loss = 559.6793212890625
Starting epoch 3/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 559.761962890625
Running validation...
Epoch 3, Step 0: Train Loss = 557.1946411132812, Test Loss = 557.25
  2%|▏         | 3/135 [00:00<00:23,  5.73it/s, epoch=2, test_loss=557, train_loss=557]Starting epoch 4/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 557.1932983398438
Running validation...
Epoch 4, Step 0: Train Loss = 555.2293701171875, Test Loss = 555.3028564453125
Starting epoch 5/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 555.1712036132812
Running validation...
Epoch 5, Step 0: Train Loss = 553.5532836914062, Test Loss = 553.6646728515625
  4%|▎         | 5/135 [00:00<00:16,  7.67it/s, epoch=4, test_loss=554, train_loss=554]Starting epoch 6/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 553.7018432617188
Running validation...
Epoch 6, Step 0: Train Loss = 552.29638671875, Test Loss = 552.3701782226562
  4%|▍         | 6/135 [00:00<00:16,  8.01it/s, epoch=5, test_loss=552, train_loss=552]Starting epoch 7/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 552.4468994140625
Running validation...
Epoch 7, Step 0: Train Loss = 551.2325439453125, Test Loss = 551.323486328125
Starting epoch 8/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 551.223876953125
Running validation...
Epoch 8, Step 0: Train Loss = 550.1254272460938, Test Loss = 550.310302734375
  6%|▌         | 8/135 [00:01<00:13,  9.10it/s, epoch=7, test_loss=550, train_loss=550]Starting epoch 9/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 550.180419921875
Running validation...
Epoch 9, Step 0: Train Loss = 549.1904907226562, Test Loss = 549.2554931640625
Starting epoch 10/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 549.1664428710938
Running validation...
Epoch 10, Step 0: Train Loss = 548.2098999023438, Test Loss = 548.3346557617188
  7%|▋         | 10/135 [00:01<00:12,  9.69it/s, epoch=9, test_loss=548, train_loss=548]Starting epoch 11/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 548.2171630859375
Running validation...
Epoch 11, Step 0: Train Loss = 547.2152709960938, Test Loss = 547.302978515625
Starting epoch 12/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 547.1827392578125
Running validation...
Epoch 12, Step 0: Train Loss = 546.1653442382812, Test Loss = 546.2623291015625
  9%|▉         | 12/135 [00:01<00:12,  9.96it/s, epoch=11, test_loss=546, train_loss=546]Starting epoch 13/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 546.1924438476562
Running validation...
Epoch 13, Step 0: Train Loss = 545.0310668945312, Test Loss = 545.1178588867188
Starting epoch 14/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 545.0499267578125
Running validation...
Epoch 14, Step 0: Train Loss = 543.9363403320312, Test Loss = 544.0667724609375
 10%|█         | 14/135 [00:01<00:12, 10.01it/s, epoch=13, test_loss=544, train_loss=544]Starting epoch 15/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 543.8973999023438
Running validation...
Epoch 15, Step 0: Train Loss = 542.7127075195312, Test Loss = 542.625


SOM initialization...

 25%|██▌       | 34/135 [00:02<00:05, 18.66it/s, epoch=3, test_loss=1.3, train_loss=1.29] 

Training...

 36%|███▌      | 48/135 [00:09<00:38,  2.29it/s, cah=[2.881862], cr_ratio=61.4, cs_ratio=1.03, epoch=12, ssom=[4.1506886], test_loss=486, train_loss=484, vae=[476.65744], vc_ratio=123] .57e+4]    +4]     5]  LBP
INFO:tensorflow:Restoring parameters from ../models/hyperopt_100_8-8_2024-06-19_0cc8d/hyperopt_100_8-8_2024-06-19_0cc8d.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_100_8-8_2024-06-19_0cc8d/hyperopt_100_8-8_2024-06-19_0cc8d.ckpt
Evaluation...
 85%|████████▌ | 115/135 [00:42<00:07,  2.73it/s, cah=[0.08035075], cr_ratio=-3.15e+4, cs_ratio=0.222, epoch=79, ssom=[4.1595445], test_loss=-1.52e+6, train_loss=-1.49e+6, vae=[-1503684.8], vc_ratio=-1.58e+6]

 NMI: 0.11550469934169612, AMI: 0.05216454320807493, PUR: 0.3875.  Name: %r.


 Time: 43.049453020095825
INFO - hyperopt - Result: {'NMI': 0.11550469934169612, 'Purity': 0.3875, 'AMI': 0.05216454320807493}
INFO - hyperopt - Completed after 0:00:43
