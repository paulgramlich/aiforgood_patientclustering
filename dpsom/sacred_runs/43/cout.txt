INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "43"
using LBP
[8. 8. 0. 8. 8. 6. 5. 8. 8. 3. 5. 8. 8. 7. 8. 8. 0. 8. 7. 6. 5. 0. 7. 0.
 5. 8. 0. 0. 7. 8. 0. 5. 0. 8. 7. 0. 7. 5. 7. 8. 6. 4. 0. 5. 8. 0. 0. 0.
 8. 7. 8. 0. 8. 0. 8. 5. 6. 7. 8. 4. 7. 7. 0. 7. 6. 0. 7. 8. 5. 8. 8. 5.
 0. 5. 0. 8. 5. 7. 8. 7. 5. 5. 7. 8. 8. 0. 0. 0. 0. 6. 8. 8. 0. 5. 0. 7.
 0. 0. 8. 7. 8. 7. 8. 8. 0. 5. 5. 8. 7. 8. 7. 0. 0. 7. 7. 0. 8. 8. 0. 0.
 8. 8. 8. 5. 8. 7. 0. 0. 8. 5. 8. 0. 8. 0. 8. 8. 0. 8. 7. 0. 5. 0. 8. 7.
 0. 7. 6. 0. 8. 8. 7. 0. 0. 5. 8. 7. 8. 8. 0. 0. 5. 8. 0. 5. 7. 8. 8. 0.
 8. 7. 4. 7. 7. 0. 6. 8. 8. 8. 8. 7. 8. 5. 8. 8. 8. 8. 5. 8. 6. 5. 0. 8.
 8. 6. 8. 8. 0. 0. 8. 8. 0. 8. 0. 8. 0. 5. 0. 8. 7. 0. 0. 4. 8. 8. 4. 8.
 7. 4. 0. 5. 8. 7. 4. 8. 0. 6. 7. 8. 8. 8. 8. 7. 5. 6. 8. 0. 8. 8. 7. 7.
 8. 8. 8. 0. 8. 5. 8. 0. 6. 6. 0. 6. 4. 0. 7. 8. 8. 0. 8. 8. 7. 8. 8. 6.
 0. 8. 8. 0. 0. 0. 0. 8. 8. 0. 8. 7. 8. 8. 0. 6. 0. 7. 8. 0. 7. 8. 5. 6.
 0. 8. 7. 0. 8. 7. 8. 7. 6. 8. 8. 8. 7. 0. 0. 3. 8. 0. 0. 7. 6. 0. 6. 8.
 8. 0. 8. 0. 8. 8. 8. 8. 8. 0. 8. 0. 0. 6. 0. 0. 0. 0. 0. 6. 8. 8. 0. 0.
 5. 7. 0. 8. 8. 8. 7. 8. 8. 5. 0. 8. 0. 8. 0. 7. 0. 0. 7. 5. 8. 8. 5. 7.
 5. 7. 6. 0. 8. 0. 7. 6. 6. 7. 0. 6. 7. 8. 0. 8. 0. 0. 8. 7. 8. 8. 6. 6.
 8. 7. 0. 7. 8. 8. 5. 8. 6. 8. 7. 5. 7. 5. 7. 8. 0. 8. 7. 7. 7. 7. 8. 6.
 0. 5. 6. 8. 8. 6. 0. 0. 8. 5. 0. 8. 8. 7. 8. 8. 5. 6. 5. 8. 8. 7. 8. 8.
 7. 8. 7. 7. 8. 0. 6. 7. 0. 6. 8. 8. 5. 8. 0. 7. 7. 0. 6. 8. 7. 8. 8. 8.
 8. 0. 8. 7. 6. 8. 8. 5. 8. 0. 8. 7. 8. 7. 0. 0. 7. 6. 8. 0. 0. 8. 0. 7.
 0. 7. 8. 0. 7. 5. 0. 0. 7. 0. 7. 0. 0. 8. 8. 7. 8. 8. 8. 7. 8. 3. 8. 8.
 3. 5. 8. 7. 6. 0. 7. 8. 5. 7. 0. 0. 8. 0. 8. 7. 8. 7. 8. 7. 5. 7. 0. 8.
 8. 7. 7. 8. 0. 5. 8. 7. 8. 5. 7. 8. 5. 8. 8. 6. 0. 0. 0. 8. 5. 7. 8. 8.
 8. 5. 0. 0. 8. 8. 0. 7. 8. 6. 0. 0. 7. 8. 0. 5. 0. 7. 8. 0. 0. 0. 5. 0.
 0. 6. 0. 8. 5. 6. 8. 5. 8. 0. 8. 0. 7. 6. 0. 0. 8. 5. 0. 5. 8. 0. 8. 0.
 0. 8. 8. 8. 7. 0. 0. 8. 8. 6. 8. 0. 0. 0. 8. 0. 7. 8. 0. 7. 6. 8. 6. 0.
 7. 7. 5. 8. 6. 8. 0. 7. 0. 8. 0. 5. 5. 7. 7. 0. 6. 7. 4. 7. 8. 4. 8. 7.
 8. 6. 0. 7. 8. 8. 6. 0. 0. 6. 7. 4. 0. 2. 0. 0. 8. 0. 8. 7. 8. 8. 0. 5.
 8. 0. 7. 0. 7. 5. 0. 7. 8. 8. 5. 5. 0. 8. 8. 7. 6. 7. 8. 8. 8. 0. 0. 0.
 7. 0. 8. 8. 0. 8. 6. 8. 8. 8. 0. 8. 7. 7. 0. 0. 0. 6. 5. 0. 8. 8. 6. 7.
 8. 8. 0. 7. 0. 8. 0. 7. 0. 8. 0. 0. 8. 7. 0. 8. 0. 7. 8. 5. 8. 3. 7. 5.
 0. 0. 8. 5. 0. 3. 8. 0. 8. 7. 0. 8. 7. 8. 5. 7. 0. 8. 7. 8. 8. 0. 0. 0.
 8. 5. 7. 0. 8. 5. 7. 8. 5. 7. 5. 0. 0. 0. 8. 7. 0. 7. 8. 7. 5. 7. 5. 8.
 0. 0. 5. 6. 7. 8. 5. 5. 0. 8. 0. 8. 5. 5. 7. 7. 8. 8. 0. 8. 5. 0. 6. 8.
 7. 7. 8. 8. 7. 6. 7. 7. 6. 8. 3. 7. 8. 7. 5. 0. 5. 8. 7. 0. 5. 6. 8. 0.
 6. 5. 0. 0. 4. 7. 7. 8. 8. 6. 8. 7. 5. 8. 6. 0. 0. 0. 2. 8. 8. 8. 8. 8.
 8. 0. 8. 0. 0. 8. 7. 8. 7. 8. 6. 5. 8. 8. 8. 8. 0. 7. 8. 7. 0. 0. 8. 0.
 5. 8. 0. 8. 0. 8. 8. 8. 8. 8. 8. 0. 8. 0. 0. 7. 8. 0. 8. 7. 0. 0. 6. 7.
 0. 8. 7. 8. 0. 8. 8. 5. 7. 8. 7. 6. 0. 7. 0. 6.]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 16:43:20.914108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_100_8-8_2024-06-19_23ee2********* 

  0%|          | 0/355 [00:00<?, ?it/s]Number of batches: 1


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 567.9610595703125
Running validation...
Epoch 1, Step 0: Train Loss = 563.050048828125, Test Loss = 563.2180786132812
  0%|          | 1/355 [00:00<02:55,  2.01it/s, epoch=0, test_loss=563, train_loss=563]Starting epoch 2/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 563.0448608398438
Running validation...
Epoch 2, Step 0: Train Loss = 559.7911376953125, Test Loss = 559.6793212890625
Starting epoch 3/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 559.761962890625
Running validation...
Epoch 3, Step 0: Train Loss = 557.1946411132812, Test Loss = 557.25
  1%|          | 3/355 [00:00<01:11,  4.95it/s, epoch=2, test_loss=557, train_loss=557]Starting epoch 4/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 557.1932983398438
Running validation...
Epoch 4, Step 0: Train Loss = 555.2293701171875, Test Loss = 555.3028564453125
  1%|          | 4/355 [00:00<00:59,  5.94it/s, epoch=3, test_loss=555, train_loss=555]Starting epoch 5/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 555.1712036132812
Running validation...
Epoch 5, Step 0: Train Loss = 553.5532836914062, Test Loss = 553.6646728515625
Starting epoch 6/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 553.7018432617188
Running validation...
Epoch 6, Step 0: Train Loss = 552.29638671875, Test Loss = 552.3701782226562
  2%|▏         | 6/355 [00:00<00:45,  7.65it/s, epoch=5, test_loss=552, train_loss=552]Starting epoch 7/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 552.4468994140625
Running validation...
Epoch 7, Step 0: Train Loss = 551.2325439453125, Test Loss = 551.323486328125
Starting epoch 8/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 551.223876953125
Running validation...
Epoch 8, Step 0: Train Loss = 550.1254272460938, Test Loss = 550.310302734375
  2%|▏         | 8/355 [00:01<00:39,  8.68it/s, epoch=7, test_loss=550, train_loss=550]Starting epoch 9/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 550.180419921875
Running validation...
Epoch 9, Step 0: Train Loss = 549.1904907226562, Test Loss = 549.2554931640625
Starting epoch 10/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 549.1664428710938
Running validation...
Epoch 10, Step 0: Train Loss = 548.2098999023438, Test Loss = 548.3346557617188
  3%|▎         | 10/355 [00:01<00:36,  9.42it/s, epoch=9, test_loss=548, train_loss=548]Starting epoch 11/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 548.2171630859375
Running validation...
Epoch 11, Step 0: Train Loss = 547.2152709960938, Test Loss = 547.302978515625
Starting epoch 12/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 547.1827392578125
Running validation...
Epoch 12, Step 0: Train Loss = 546.1653442382812, Test Loss = 546.2623291015625
  3%|▎         | 12/355 [00:01<00:34,  9.82it/s, epoch=11, test_loss=546, train_loss=546]Starting epoch 13/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 546.1924438476562
Running validation...
Epoch 13, Step 0: Train Loss = 545.0310668945312, Test Loss = 545.1178588867188
Starting epoch 14/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 545.0499267578125
Running validation...
Epoch 14, Step 0: Train Loss = 543.9363403320312, Test Loss = 544.0667724609375
  4%|▍         | 14/355 [00:01<00:34,  9.97it/s, epoch=13, test_loss=544, train_loss=544]Starting epoch 15/15
Batch 1/1
Batch data shape: (200, 28, 28, 1)
Train Loss: 543.8973999023438
Running validation...
Epoch 15, Step 0: Train Loss = 542.7127075195312, Test Loss = 542.625


SOM initialization...

  9%|▉         | 33/355 [00:02<00:17, 18.72it/s, epoch=2, test_loss=1.3, train_loss=1.28] 

Training...

 13%|█▎        | 47/355 [00:08<02:08,  2.39it/s, cah=[3.4126358], cr_ratio=60.9, cs_ratio=1.06, epoch=11, ssom=[4.1489024], test_loss=497, train_loss=497, vae=[490.06027], vc_ratio=119]e+4]       e+4]    ]   ]  