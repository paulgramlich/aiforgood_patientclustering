INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "22"
labels_test.shape: (10000,)
data_test.shape: (10000, 28, 28, 1)
data_train.shape: (51000, 28, 28, 1)
data_val.shape: (10000, 28, 28, 1)
labels_train.shape: (51000, 28, 28, 1)
labels_val.shape: (10000, 28, 28, 1)
Initializing global variables...
2024-06-12 20:30:16.734503: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-12_da1ec********* 

  0%|          | 0/1415250 [00:00<?, ?it/s]Number of batches: 2550


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 546.7124633789062
Running validation...
Epoch 1, Step 0: Train Loss = 546.1866455078125, Test Loss = 546.2977294921875
  0%|          | 1/1415250 [00:00<138:59:12,  2.83it/s, epoch=0, test_loss=546, train_loss=546]Batch 2/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 546.05322265625
Batch 3/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.6306762695312
Batch 4/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.333740234375
Batch 5/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 545.0217895507812
Batch 6/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.7254638671875
Batch 7/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.4791259765625
Batch 8/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 544.1785888671875
Batch 9/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.944091796875
Batch 10/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.6617431640625
Batch 11/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.3866577148438
Batch 12/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 543.202392578125
Batch 13/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.9120483398438
Batch 14/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.74462890625
Batch 15/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.5185546875
Batch 16/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 542.1373291015625
Batch 17/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.7238159179688
Batch 18/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.4751586914062
Batch 19/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 541.1890258789062
Batch 20/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 540.65966796875
  0%|          | 20/1415250 [00:00<7:00:05, 56.15it/s, epoch=0, test_loss=546, train_loss=541] Batch 21/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 539.94921875
Batch 22/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 539.2587890625
Batch 23/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 539.1512451171875
Batch 24/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 537.8074951171875
Batch 25/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 538.177734375
Batch 26/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 536.5838012695312
Batch 27/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 535.734130859375
Batch 28/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 533.92578125
Batch 29/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 532.1915893554688
Batch 30/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 530.9186401367188
Batch 31/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 529.6534423828125
Batch 32/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 524.4796142578125
Batch 33/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 525.0416259765625
Batch 34/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 521.882080078125
Batch 35/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 519.26318359375
Batch 36/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 512.6013793945312
Batch 37/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 505.5873718261719
Batch 38/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 497.5174255371094
Batch 39/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 496.5007019042969
Batch 40/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 488.6167907714844
Batch 41/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 480.9788513183594
Batch 42/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 467.7923583984375
  0%|          | 42/1415250 [00:00<3:51:24, 101.93it/s, epoch=0, test_loss=546, train_loss=468]Batch 43/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 455.988037109375
Batch 44/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 445.3453063964844
Batch 45/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 419.7415771484375
Batch 46/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 410.7588806152344
Batch 47/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 406.1662292480469
Batch 48/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 400.2475280761719
Batch 49/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 371.6849365234375
Batch 50/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 371.0348815917969
Batch 51/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 360.44378662109375
Batch 52/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 348.8221130371094
Batch 53/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 332.4841003417969
Batch 54/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 314.8349609375
Batch 55/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 318.07861328125
Batch 56/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 283.9450988769531
Batch 57/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 289.2908020019531
Batch 58/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 304.1119384765625
Batch 59/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 278.2170715332031
  0%|          | 59/1415250 [00:00<3:16:26, 120.06it/s, epoch=0, test_loss=546, train_loss=278]Batch 60/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 271.93316650390625
Batch 61/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 280.88623046875
Batch 62/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 270.60003662109375
Batch 63/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 263.03167724609375
Batch 64/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 239.44744873046875
Batch 65/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 257.951171875
Batch 66/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 258.78497314453125
Batch 67/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 254.82818603515625
Batch 68/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 263.9627685546875
Batch 69/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 252.8548126220703
Batch 70/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 247.6282958984375
Batch 71/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 232.8574676513672
Batch 72/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 261.8472595214844
Batch 73/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.85696411132812
Batch 74/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.99534606933594
Batch 75/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.49407958984375
Batch 76/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 233.82969665527344
Batch 77/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 224.0297393798828
Batch 78/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 219.89590454101562
Batch 79/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.86671447753906
Batch 80/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.23292541503906
  0%|          | 80/1415250 [00:00<2:43:20, 144.40it/s, epoch=0, test_loss=546, train_loss=216]Batch 81/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.92556762695312
Batch 82/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.01646423339844
Batch 83/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 225.71583557128906
Batch 84/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.89324951171875
Batch 85/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.70849609375
Batch 86/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 238.02806091308594
Batch 87/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 255.89083862304688
Batch 88/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 242.24395751953125
Batch 89/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 228.32740783691406
Batch 90/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.2705078125
Batch 91/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.36851501464844
Batch 92/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.35983276367188
Batch 93/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 237.019775390625
Batch 94/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.42022705078125
Batch 95/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.71322631835938
Batch 96/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.51429748535156
Batch 97/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 230.66639709472656
Batch 98/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 225.4151153564453
  0%|          | 98/1415250 [00:00<2:34:40, 152.48it/s, epoch=0, test_loss=546, train_loss=225]Batch 99/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.51954650878906
Batch 100/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.92735290527344
Batch 101/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.794921875
Running validation...
Epoch 1, Step 100: Train Loss = 220.0042266845703, Test Loss = 205.575439453125
Batch 102/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.9243927001953
Batch 103/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.90286254882812
Batch 104/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.94654846191406
Batch 105/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.3682861328125
Batch 106/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.2811279296875
Batch 107/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.7913055419922
Batch 108/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.3203887939453
Batch 109/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.7002716064453
Batch 110/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.72740173339844
Batch 111/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.2483367919922
Batch 112/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.41769409179688
Batch 113/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.83053588867188
Batch 114/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.82669067382812
Batch 115/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.63616943359375
Batch 116/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.81494140625
  0%|          | 116/1415250 [00:00<2:29:54, 157.34it/s, epoch=0, test_loss=206, train_loss=199]Batch 117/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.3722686767578
Batch 118/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.9796905517578
Batch 119/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.04100036621094
Batch 120/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 227.9688720703125
Batch 121/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.57485961914062
Batch 122/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.90098571777344
Batch 123/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.6828155517578
Batch 124/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.7659149169922
Batch 125/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.99417114257812
Batch 126/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.14370727539062
Batch 127/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 224.00350952148438
Batch 128/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.7261505126953
Batch 129/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.41908264160156
Batch 130/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.4744110107422
Batch 131/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.19931030273438
Batch 132/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 227.36692810058594
Batch 133/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.27476501464844
Batch 134/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.40272521972656
  0%|          | 134/1415250 [00:01<2:25:53, 161.66it/s, epoch=0, test_loss=206, train_loss=197]Batch 135/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.14620971679688
Batch 136/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.85391235351562
Batch 137/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.36862182617188
Batch 138/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.34210205078125
Batch 139/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.55563354492188
Batch 140/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.35940551757812
Batch 141/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.5655059814453
Batch 142/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.93060302734375
Batch 143/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.56692504882812
Batch 144/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.61990356445312
Batch 145/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6949462890625
Batch 146/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.0812225341797
Batch 147/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.8417205810547
Batch 148/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.07408142089844
Batch 149/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.45591735839844
Batch 150/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.85594177246094
Batch 151/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.01622009277344
Batch 152/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.2110137939453
  0%|          | 152/1415250 [00:01<2:21:45, 166.38it/s, epoch=0, test_loss=206, train_loss=210]Batch 153/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.69680786132812
Batch 154/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.2570343017578
Batch 155/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.72264099121094
Batch 156/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.43162536621094
Batch 157/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.1010284423828
Batch 158/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.068359375
Batch 159/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.55850219726562
Batch 160/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.6490478515625
Batch 161/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.85693359375
Batch 162/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.61839294433594
Batch 163/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.28274536132812
Batch 164/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.11279296875
Batch 165/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.0283966064453
Batch 166/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.36647033691406
Batch 167/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.16983032226562
Batch 168/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.89698791503906
Batch 169/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.34422302246094
Batch 170/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.5305938720703
Batch 171/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.87745666503906
  0%|          | 171/1415250 [00:01<2:17:06, 172.01it/s, epoch=0, test_loss=206, train_loss=214]Batch 172/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.32742309570312
Batch 173/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.15667724609375
Batch 174/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.3824462890625
Batch 175/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.9366455078125
Batch 176/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.01870727539062
Batch 177/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.29177856445312
Batch 178/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.68228149414062
Batch 179/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.91697692871094
Batch 180/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.18492126464844
Batch 181/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.8494110107422
Batch 182/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.74656677246094
Batch 183/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.79515075683594
Batch 184/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.5799102783203
Batch 185/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.33132934570312
Batch 186/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.83950805664062
Batch 187/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.94635009765625
Batch 188/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.91189575195312
Batch 189/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.1334991455078
Batch 190/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.61129760742188
Batch 191/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.81710815429688
  0%|          | 191/1415250 [00:01<2:12:57, 177.37it/s, epoch=0, test_loss=206, train_loss=198]Batch 192/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.17601013183594
Batch 193/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.00929260253906
Batch 194/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.2725067138672
Batch 195/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.02259826660156
Batch 196/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.02928161621094
Batch 197/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.29281616210938
Batch 198/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 235.1189727783203
Batch 199/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 219.116943359375
Batch 200/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.33450317382812
Batch 201/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.23098754882812
Running validation...
Epoch 1, Step 200: Train Loss = 212.35166931152344, Test Loss = 189.8633575439453
Batch 202/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.64395141601562
Batch 203/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.8658905029297
Batch 204/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.1053466796875
Batch 205/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.32240295410156
Batch 206/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.1707763671875
Batch 207/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.7367401123047
Batch 208/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.5170440673828
Batch 209/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.34805297851562
Batch 210/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.4494171142578
  0%|          | 210/1415250 [00:01<2:11:24, 179.46it/s, epoch=0, test_loss=190, train_loss=221]Batch 211/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.34979248046875
Batch 212/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.97657775878906
Batch 213/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.62258911132812
Batch 214/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.5095977783203
Batch 215/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.3436737060547
Batch 216/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.85073852539062
Batch 217/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.25930786132812
Batch 218/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.41055297851562
Batch 219/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.65066528320312
Batch 220/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.5226593017578
Batch 221/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.96340942382812
Batch 222/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.29547119140625
Batch 223/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.9748992919922
Batch 224/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.211181640625
Batch 225/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.413818359375
Batch 226/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.06809997558594
Batch 227/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.9781951904297
Batch 228/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.664306640625
Batch 229/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.4329071044922
  0%|          | 229/1415250 [00:01<2:09:41, 181.85it/s, epoch=0, test_loss=190, train_loss=191]Batch 230/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.73233032226562
Batch 231/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.44053649902344
Batch 232/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.90927124023438
Batch 233/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.07342529296875
Batch 234/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.24441528320312
Batch 235/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.08233642578125
Batch 236/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.3126220703125
Batch 237/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.62977600097656
Batch 238/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.6342010498047
Batch 239/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.0782928466797
Batch 240/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.24374389648438
Batch 241/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.69598388671875
Batch 242/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.0863800048828
Batch 243/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.94293212890625
Batch 244/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.5028076171875
Batch 245/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.87030029296875
Batch 246/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.56350708007812
Batch 247/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.89093017578125
Batch 248/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.685791015625
  0%|          | 248/1415250 [00:01<2:11:10, 179.79it/s, epoch=0, test_loss=190, train_loss=197]Batch 249/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.41769409179688
Batch 250/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.90914916992188
Batch 251/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.0796356201172
Batch 252/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.63101196289062
Batch 253/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.49188232421875
Batch 254/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.2143096923828
Batch 255/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 225.75424194335938
Batch 256/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.67910766601562
Batch 257/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.31602478027344
Batch 258/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.82159423828125
Batch 259/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.419677734375
Batch 260/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.5055694580078
Batch 261/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.45118713378906
Batch 262/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.31056213378906
Batch 263/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.71070861816406
Batch 264/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.5095977783203
Batch 265/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.5433807373047
Batch 266/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.59548950195312
Batch 267/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.67926025390625
  0%|          | 267/1415250 [00:01<2:15:18, 174.29it/s, epoch=0, test_loss=190, train_loss=187]Batch 268/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.08592224121094
Batch 269/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.87738037109375
Batch 270/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.2037811279297
Batch 271/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.692626953125
Batch 272/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.34317016601562
Batch 273/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.9970703125
Batch 274/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.91549682617188
Batch 275/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.3734893798828
Batch 276/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.42047119140625
Batch 277/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.83741760253906
Batch 278/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.90618896484375
Batch 279/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.15476989746094
Batch 280/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.21624755859375
Batch 281/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.71865844726562
Batch 282/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.6055450439453
Batch 283/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.363037109375
Batch 284/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.4529571533203
Batch 285/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.9064483642578
  0%|          | 285/1415250 [00:01<2:16:05, 173.28it/s, epoch=0, test_loss=190, train_loss=203]Batch 286/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.2705535888672
Batch 287/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.41647338867188
Batch 288/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.2849578857422
Batch 289/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.70541381835938
Batch 290/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.7560577392578
Batch 291/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.4659881591797
Batch 292/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.41021728515625
Batch 293/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.22573852539062
Batch 294/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.76161193847656
Batch 295/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.43814086914062
Batch 296/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.16574096679688
Batch 297/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.40191650390625
Batch 298/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.22816467285156
Batch 299/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.29058837890625
Batch 300/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.53814697265625
Batch 301/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.8418731689453
Running validation...
Epoch 1, Step 300: Train Loss = 186.2183837890625, Test Loss = 204.75857543945312
Batch 302/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.7839813232422
Batch 303/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.11520385742188
  0%|          | 303/1415250 [00:02<2:19:15, 169.35it/s, epoch=0, test_loss=205, train_loss=202]Batch 304/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.66314697265625
Batch 305/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.0501251220703
Batch 306/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.0348358154297
Batch 307/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.4636688232422
Batch 308/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.2700958251953
Batch 309/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.81951904296875
Batch 310/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.43666076660156
Batch 311/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.1175079345703
Batch 312/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.3494873046875
Batch 313/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.2188720703125
Batch 314/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.01100158691406
Batch 315/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.5467987060547
Batch 316/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.73692321777344
Batch 317/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.8931121826172
Batch 318/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.37179565429688
Batch 319/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.95018005371094
Batch 320/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.0550079345703
Batch 321/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.88613891601562
  0%|          | 321/1415250 [00:02<2:16:50, 172.33it/s, epoch=0, test_loss=205, train_loss=187]Batch 322/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.20474243164062
Batch 323/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.5833740234375
Batch 324/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.21588134765625
Batch 325/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.5960235595703
Batch 326/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.63995361328125
Batch 327/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.32791137695312
Batch 328/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 227.7311248779297
Batch 329/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.51177978515625
Batch 330/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.16587829589844
Batch 331/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.02471923828125
Batch 332/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.2172088623047
Batch 333/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.66212463378906
Batch 334/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.75828552246094
Batch 335/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.75901794433594
Batch 336/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.4927215576172
Batch 337/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.81295776367188
Batch 338/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.35203552246094
Batch 339/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.5911102294922
Batch 340/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.99583435058594
  0%|          | 340/1415250 [00:02<2:14:29, 175.33it/s, epoch=0, test_loss=205, train_loss=193]Batch 341/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.5874786376953
Batch 342/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.87551879882812
Batch 343/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.58131408691406
Batch 344/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.30081176757812
Batch 345/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.77450561523438
Batch 346/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.34341430664062
Batch 347/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.17552185058594
Batch 348/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.57557678222656
Batch 349/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.66485595703125
Batch 350/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.3073272705078
Batch 351/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.36647033691406
Batch 352/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.74383544921875
Batch 353/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.9468536376953
Batch 354/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.79586791992188
Batch 355/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 219.97813415527344
Batch 356/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.25079345703125
Batch 357/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.2524871826172
Batch 358/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.868896484375
Batch 359/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.488037109375
  0%|          | 359/1415250 [00:02<2:11:23, 179.48it/s, epoch=0, test_loss=205, train_loss=206]Batch 360/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.21295166015625
Batch 361/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.80807495117188
Batch 362/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.25958251953125
Batch 363/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.46388244628906
Batch 364/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.6961669921875
Batch 365/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.76304626464844
Batch 366/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.8804168701172
Batch 367/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.10650634765625
Batch 368/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.13729858398438
Batch 369/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.49700927734375
Batch 370/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.1635284423828
Batch 371/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.1856689453125
Batch 372/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.77732849121094
Batch 373/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.73915100097656
Batch 374/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.21804809570312
Batch 375/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.68931579589844
Batch 376/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.2623291015625
Batch 377/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.727783203125
Batch 378/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.88929748535156
  0%|          | 378/1415250 [00:02<2:09:26, 182.17it/s, epoch=0, test_loss=205, train_loss=205]Batch 379/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.46600341796875
Batch 380/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.72007751464844
Batch 381/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.96707153320312
Batch 382/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.792724609375
Batch 383/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.00408935546875
Batch 384/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.35946655273438
Batch 385/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.68385314941406
Batch 386/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.33670043945312
Batch 387/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.5506591796875
Batch 388/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.42164611816406
Batch 389/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.3874969482422
Batch 390/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.62841796875
Batch 391/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.97608947753906
Batch 392/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 234.18768310546875
Batch 393/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.20208740234375
Batch 394/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.3630828857422
Batch 395/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.127685546875
Batch 396/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.1793212890625
Batch 397/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.4222412109375
  0%|          | 397/1415250 [00:02<2:08:29, 183.52it/s, epoch=0, test_loss=205, train_loss=192]Batch 398/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.3007049560547
Batch 399/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.47610473632812
Batch 400/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.71371459960938
Batch 401/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.14170837402344
Running validation...
Epoch 1, Step 400: Train Loss = 203.86968994140625, Test Loss = 192.39407348632812
Batch 402/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6717071533203
Batch 403/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.91502380371094
Batch 404/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 231.82667541503906
Batch 405/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.69046020507812
Batch 406/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.09400939941406
Batch 407/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.17840576171875
Batch 408/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.06088256835938
Batch 409/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.36962890625
Batch 410/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.43951416015625
Batch 411/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.83729553222656
Batch 412/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.30673217773438
Batch 413/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.3702392578125
Batch 414/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.6092987060547
Batch 415/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.11973571777344
Batch 416/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.86390686035156
  0%|          | 416/1415250 [00:02<2:11:59, 178.66it/s, epoch=0, test_loss=192, train_loss=193]Batch 417/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.19998168945312
Batch 418/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.2395782470703
Batch 419/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.6883544921875
Batch 420/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.29786682128906
Batch 421/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.08509826660156
Batch 422/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.6957550048828
Batch 423/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.89108276367188
Batch 424/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.2998504638672
Batch 425/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.3865509033203
Batch 426/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.956298828125
Batch 427/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 219.35633850097656
Batch 428/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.4053192138672
Batch 429/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.01248168945312
Batch 430/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.87156677246094
Batch 431/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.4739990234375
Batch 432/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.66612243652344
Batch 433/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.91299438476562
Batch 434/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.1407470703125
  0%|          | 434/1415250 [00:02<2:12:44, 177.64it/s, epoch=0, test_loss=192, train_loss=214]Batch 435/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.1089630126953
Batch 436/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.8025360107422
Batch 437/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.7691192626953
Batch 438/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.44276428222656
Batch 439/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.45562744140625
Batch 440/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.541748046875
Batch 441/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.4779815673828
Batch 442/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.71554565429688
Batch 443/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.94178771972656
Batch 444/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.91551208496094
Batch 445/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.65048217773438
Batch 446/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.05746459960938
Batch 447/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.59059143066406
Batch 448/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.69992065429688
Batch 449/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.78053283691406
Batch 450/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.80648803710938
Batch 451/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.43016052246094
Batch 452/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.2823944091797
Batch 453/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.4131622314453
  0%|          | 453/1415250 [00:02<2:10:35, 180.56it/s, epoch=0, test_loss=192, train_loss=193]Batch 454/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.7300262451172
Batch 455/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.35452270507812
Batch 456/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.72836303710938
Batch 457/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.98194885253906
Batch 458/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.27601623535156
Batch 459/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.88772583007812
Batch 460/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.81202697753906
Batch 461/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.33058166503906
Batch 462/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.82554626464844
Batch 463/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.02708435058594
Batch 464/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.04022216796875
Batch 465/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.45623779296875
Batch 466/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.387939453125
Batch 467/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.60006713867188
Batch 468/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.7288818359375
Batch 469/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.98995971679688
Batch 470/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.4078826904297
Batch 471/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.9783172607422
Batch 472/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.34181213378906
  0%|          | 472/1415250 [00:02<2:09:03, 182.71it/s, epoch=0, test_loss=192, train_loss=191]Batch 473/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.48277282714844
Batch 474/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.52767944335938
Batch 475/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.24710083007812
Batch 476/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.63870239257812
Batch 477/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.2613525390625
Batch 478/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.77334594726562
Batch 479/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.50262451171875
Batch 480/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.44491577148438
Batch 481/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.11471557617188
Batch 482/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6970672607422
Batch 483/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.2730712890625
Batch 484/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.7017059326172
Batch 485/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.52330017089844
Batch 486/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.83258056640625
Batch 487/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.24790954589844
Batch 488/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.87631225585938
Batch 489/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.5123291015625
Batch 490/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.36221313476562
Batch 491/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.3223876953125
  0%|          | 491/1415250 [00:03<2:07:51, 184.41it/s, epoch=0, test_loss=192, train_loss=177]Batch 492/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.49656677246094
Batch 493/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.86380004882812
Batch 494/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.74522399902344
Batch 495/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.93975830078125
Batch 496/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.7147216796875
Batch 497/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.94859313964844
Batch 498/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.05555725097656
Batch 499/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.4208221435547
Batch 500/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.4312286376953
Batch 501/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.09617614746094
Running validation...
Epoch 1, Step 500: Train Loss = 199.72955322265625, Test Loss = 182.96705627441406
Batch 502/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.57691955566406
Batch 503/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.4407501220703
Batch 504/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.82577514648438
Batch 505/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.8730926513672
Batch 506/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.80474853515625
Batch 507/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.8593292236328
Batch 508/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.44866943359375
Batch 509/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.1556854248047
Batch 510/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.01100158691406
  0%|          | 510/1415250 [00:03<2:08:14, 183.86it/s, epoch=0, test_loss=183, train_loss=194]Batch 511/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.70950317382812
Batch 512/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.19564819335938
Batch 513/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.92953491210938
Batch 514/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.9642333984375
Batch 515/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.32916259765625
Batch 516/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.14952087402344
Batch 517/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.09188842773438
Batch 518/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.35858154296875
Batch 519/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.4918975830078
Batch 520/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.59425354003906
Batch 521/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.99627685546875
Batch 522/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.4710693359375
Batch 523/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.7386016845703
Batch 524/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0978546142578
Batch 525/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.94085693359375
Batch 526/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.51988220214844
Batch 527/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.8155059814453
Batch 528/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.293212890625
Batch 529/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.21035766601562
  0%|          | 529/1415250 [00:03<2:08:07, 184.04it/s, epoch=0, test_loss=183, train_loss=198]Batch 530/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.9887237548828
Batch 531/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.76602172851562
Batch 532/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.5705108642578
Batch 533/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.18309020996094
Batch 534/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.59478759765625
Batch 535/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.8014373779297
Batch 536/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.83157348632812
Batch 537/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.60084533691406
Batch 538/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.50042724609375
Batch 539/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.01968383789062
Batch 540/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.9163055419922
Batch 541/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.23435974121094
Batch 542/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.26007080078125
Batch 543/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.33799743652344
Batch 544/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.73385620117188
Batch 545/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.34205627441406
Batch 546/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.38558959960938
Batch 547/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.5365447998047
Batch 548/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.26153564453125
  0%|          | 548/1415250 [00:03<2:07:08, 185.45it/s, epoch=0, test_loss=183, train_loss=210]Batch 549/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.48756408691406
Batch 550/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.8050994873047
Batch 551/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.86581420898438
Batch 552/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.42115783691406
Batch 553/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.16043090820312
Batch 554/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.94793701171875
Batch 555/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.29928588867188
Batch 556/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.69337463378906
Batch 557/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.72537231445312
Batch 558/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.11581420898438
Batch 559/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.1614227294922
Batch 560/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.24447631835938
Batch 561/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.87728881835938
Batch 562/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.16995239257812
Batch 563/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.83277893066406
Batch 564/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.5975341796875
Batch 565/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.0430450439453
Batch 566/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.4845733642578
Batch 567/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.8823699951172
  0%|          | 567/1415250 [00:03<2:06:28, 186.44it/s, epoch=0, test_loss=183, train_loss=210]Batch 568/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.59982299804688
Batch 569/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.70028686523438
Batch 570/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 223.56756591796875
Batch 571/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.0726318359375
Batch 572/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.92442321777344
Batch 573/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.66552734375
Batch 574/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.68174743652344
Batch 575/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.5259246826172
Batch 576/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.59852600097656
Batch 577/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.76185607910156
Batch 578/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.46080017089844
Batch 579/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.3069305419922
Batch 580/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.9765625
Batch 581/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.64405822753906
Batch 582/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.1512451171875
Batch 583/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.13784790039062
Batch 584/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.89437866210938
Batch 585/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.96897888183594
Batch 586/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.8952178955078
  0%|          | 586/1415250 [00:03<2:05:59, 187.13it/s, epoch=0, test_loss=183, train_loss=194]Batch 587/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.72201538085938
Batch 588/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.2770538330078
Batch 589/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.68605041503906
Batch 590/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.3677978515625
Batch 591/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.48483276367188
Batch 592/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.10621643066406
Batch 593/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.18585205078125
Batch 594/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.8144989013672
Batch 595/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.0792694091797
Batch 596/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.22991943359375
Batch 597/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.1575927734375
Batch 598/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.4672393798828
Batch 599/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.46664428710938
Batch 600/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.84567260742188
Batch 601/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.02340698242188
Running validation...
Epoch 1, Step 600: Train Loss = 209.4883270263672, Test Loss = 197.5507049560547
Batch 602/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.9703826904297
Batch 603/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.1659698486328
Batch 604/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.34561157226562
Batch 605/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.03993225097656
  0%|          | 605/1415250 [00:03<2:06:56, 185.74it/s, epoch=0, test_loss=198, train_loss=196]Batch 606/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.6543731689453
Batch 607/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.80059814453125
Batch 608/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.80482482910156
Batch 609/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.2375946044922
Batch 610/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.8357696533203
Batch 611/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.75491333007812
Batch 612/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.71066284179688
Batch 613/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.1834716796875
Batch 614/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.23809814453125
Batch 615/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.0381317138672
Batch 616/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.8251190185547
Batch 617/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.68743896484375
Batch 618/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.2764129638672
Batch 619/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.5673370361328
Batch 620/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.37210083007812
Batch 621/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.1324005126953
Batch 622/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.29833984375
Batch 623/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.39891052246094
Batch 624/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.48382568359375
  0%|          | 624/1415250 [00:03<2:07:24, 185.04it/s, epoch=0, test_loss=198, train_loss=181]Batch 625/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.16351318359375
Batch 626/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.42636108398438
Batch 627/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.55296325683594
Batch 628/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.2565460205078
Batch 629/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.61068725585938
Batch 630/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.99525451660156
Batch 631/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.31419372558594
Batch 632/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.51861572265625
Batch 633/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.01686096191406
Batch 634/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.3896484375
Batch 635/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.3871307373047
Batch 636/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.42672729492188
Batch 637/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.6952362060547
Batch 638/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.6914520263672
Batch 639/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.6884307861328
Batch 640/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.90325927734375
Batch 641/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.1996612548828
Batch 642/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.5480194091797
Batch 643/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.436279296875
  0%|          | 643/1415250 [00:03<2:06:52, 185.82it/s, epoch=0, test_loss=198, train_loss=189]Batch 644/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.48028564453125
Batch 645/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.13116455078125
Batch 646/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.06268310546875
Batch 647/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.77525329589844
Batch 648/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 228.55276489257812
Batch 649/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.76902770996094
Batch 650/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.55714416503906
Batch 651/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.49295043945312
Batch 652/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.73455810546875
Batch 653/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.66835021972656
Batch 654/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.03103637695312
Batch 655/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.77195739746094
Batch 656/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.84207153320312
Batch 657/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.9032440185547
Batch 658/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.52490234375
Batch 659/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.42752075195312
Batch 660/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.86294555664062
Batch 661/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.4524688720703
Batch 662/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 225.6420440673828
  0%|          | 662/1415250 [00:03<2:07:12, 185.34it/s, epoch=0, test_loss=198, train_loss=226]Batch 663/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.95213317871094
Batch 664/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.6432342529297
Batch 665/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.93702697753906
Batch 666/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.31106567382812
Batch 667/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.9716033935547
Batch 668/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.2455291748047
Batch 669/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.16563415527344
Batch 670/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.7488555908203
Batch 671/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.95152282714844
Batch 672/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.65184020996094
Batch 673/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.7310333251953
Batch 674/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.54110717773438
Batch 675/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.65164184570312
Batch 676/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.44015502929688
Batch 677/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.4635467529297
Batch 678/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.68426513671875
Batch 679/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.91587829589844
Batch 680/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.13658142089844
Batch 681/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.9407958984375
  0%|          | 681/1415250 [00:04<2:06:27, 186.44it/s, epoch=0, test_loss=198, train_loss=196]Batch 682/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.16131591796875
Batch 683/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.48385620117188
Batch 684/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.77076721191406
Batch 685/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.1188507080078
Batch 686/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.19772338867188
Batch 687/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.70835876464844
Batch 688/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.0731201171875
Batch 689/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.67776489257812
Batch 690/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.29904174804688
Batch 691/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.81387329101562
Batch 692/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.5222625732422
Batch 693/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.51010131835938
Batch 694/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.50872802734375
Batch 695/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.60714721679688
Batch 696/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.00421142578125
Batch 697/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.6933135986328
Batch 698/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.03555297851562
Batch 699/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.3961181640625
Batch 700/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.07508850097656
  0%|          | 700/1415250 [00:04<2:06:09, 186.87it/s, epoch=0, test_loss=198, train_loss=195]Batch 701/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.44863891601562
Running validation...
Epoch 1, Step 700: Train Loss = 192.30979919433594, Test Loss = 185.04019165039062
Batch 702/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.67970275878906
Batch 703/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.20648193359375
Batch 704/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.95901489257812
Batch 705/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.82583618164062
Batch 706/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.33883666992188
Batch 707/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.8308563232422
Batch 708/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.75894165039062
Batch 709/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.24398803710938
Batch 710/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.580078125
Batch 711/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.4697265625
Batch 712/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.2156524658203
Batch 713/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.87046813964844
Batch 714/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.90296936035156
Batch 715/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.91262817382812
Batch 716/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.06765747070312
Batch 717/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.36520385742188
Batch 718/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.10711669921875
Batch 719/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.02735900878906
  0%|          | 719/1415250 [00:04<2:06:53, 185.79it/s, epoch=0, test_loss=185, train_loss=201]Batch 720/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.15176391601562
Batch 721/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.27101135253906
Batch 722/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.63926696777344
Batch 723/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.04373168945312
Batch 724/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.8881378173828
Batch 725/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.29165649414062
Batch 726/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.06890869140625
Batch 727/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.1848602294922
Batch 728/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.1229248046875
Batch 729/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.8348846435547
Batch 730/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.44070434570312
Batch 731/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.8283233642578
Batch 732/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.74916076660156
Batch 733/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.38343811035156
Batch 734/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.5885009765625
Batch 735/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.18896484375
Batch 736/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.4687042236328
Batch 737/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.2833709716797
Batch 738/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.04222106933594
  0%|          | 738/1415250 [00:04<2:07:25, 185.02it/s, epoch=0, test_loss=185, train_loss=194]Batch 739/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.51092529296875
Batch 740/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.410888671875
Batch 741/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.117919921875
Batch 742/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.45510864257812
Batch 743/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.61695861816406
Batch 744/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.46823120117188
Batch 745/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.7866973876953
Batch 746/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.8191375732422
Batch 747/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.49649047851562
Batch 748/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.7443084716797
Batch 749/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.897216796875
Batch 750/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.6980743408203
Batch 751/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.80726623535156
Batch 752/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.47349548339844
Batch 753/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.4137725830078
Batch 754/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.45040893554688
Batch 755/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.9725799560547
Batch 756/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.9589080810547
Batch 757/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.19725036621094
Batch 758/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.72044372558594
  0%|          | 758/1415250 [00:04<2:06:00, 187.10it/s, epoch=0, test_loss=185, train_loss=185]Batch 759/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.8622283935547
Batch 760/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.94424438476562
Batch 761/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.86058044433594
Batch 762/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.33624267578125
Batch 763/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.5623016357422
Batch 764/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.41714477539062
Batch 765/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.15879821777344
Batch 766/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.26168823242188
Batch 767/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.87875366210938
Batch 768/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.13511657714844
Batch 769/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.2041778564453
Batch 770/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.98289489746094
Batch 771/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.30020141601562
Batch 772/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.84552001953125
Batch 773/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.11358642578125
Batch 774/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.35597229003906
Batch 775/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.89413452148438
Batch 776/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.34912109375
Batch 777/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.97779846191406
  0%|          | 777/1415250 [00:04<2:08:58, 182.77it/s, epoch=0, test_loss=185, train_loss=204]Batch 778/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.93356323242188
Batch 779/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.2710723876953
Batch 780/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.86993408203125
Batch 781/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.95303344726562
Batch 782/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.56210327148438
Batch 783/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.2960205078125
Batch 784/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.2303466796875
Batch 785/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.25445556640625
Batch 786/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.58856201171875
Batch 787/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.32162475585938
Batch 788/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.30108642578125
Batch 789/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.5184783935547
Batch 790/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.39599609375
Batch 791/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.30831909179688
Batch 792/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.53512573242188
Batch 793/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.33340454101562
Batch 794/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.3800506591797
Batch 795/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.8435821533203
Batch 796/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.4051513671875
  0%|          | 796/1415250 [00:04<2:08:10, 183.91it/s, epoch=0, test_loss=185, train_loss=210]Batch 797/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.3177947998047
Batch 798/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.64739990234375
Batch 799/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.86192321777344
Batch 800/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.30943298339844
Batch 801/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.77761840820312
Running validation...
Epoch 1, Step 800: Train Loss = 193.87791442871094, Test Loss = 177.77735900878906
Batch 802/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 220.29812622070312
Batch 803/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.32418823242188
Batch 804/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.99224853515625
Batch 805/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.09742736816406
Batch 806/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.5064239501953
Batch 807/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.34832763671875
Batch 808/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.861572265625
Batch 809/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.20944213867188
Batch 810/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.33258056640625
Batch 811/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.75830078125
Batch 812/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.96945190429688
Batch 813/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.7025146484375
Batch 814/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.48797607421875
Batch 815/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.45291137695312
  0%|          | 815/1415250 [00:04<2:10:37, 180.46it/s, epoch=0, test_loss=178, train_loss=200]Batch 816/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.82968139648438
Batch 817/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.79623413085938
Batch 818/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.28195190429688
Batch 819/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.98545837402344
Batch 820/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.115234375
Batch 821/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.75621032714844
Batch 822/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.9996337890625
Batch 823/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.0271453857422
Batch 824/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.80319213867188
Batch 825/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.0573272705078
Batch 826/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.2215118408203
Batch 827/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.82493591308594
Batch 828/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.17428588867188
Batch 829/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.5079803466797
Batch 830/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.5513916015625
Batch 831/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.47927856445312
Batch 832/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.36761474609375
Batch 833/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.481689453125
Batch 834/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.59619140625
  0%|          | 834/1415250 [00:04<2:09:00, 182.73it/s, epoch=0, test_loss=178, train_loss=173]Batch 835/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.1138458251953
Batch 836/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 226.35255432128906
Batch 837/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.8463134765625
Batch 838/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.62173461914062
Batch 839/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.662109375
Batch 840/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.1907196044922
Batch 841/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.77346801757812
Batch 842/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.99485778808594
Batch 843/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.9337158203125
Batch 844/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.27281188964844
Batch 845/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.48651123046875
Batch 846/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.74794006347656
Batch 847/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.4485626220703
Batch 848/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.58236694335938
Batch 849/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.6505584716797
Batch 850/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.7350311279297
Batch 851/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.62745666503906
Batch 852/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.79818725585938
Batch 853/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.7759552001953
  0%|          | 853/1415250 [00:05<2:08:56, 182.83it/s, epoch=0, test_loss=178, train_loss=212]Batch 854/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.18650817871094
Batch 855/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.29469299316406
Batch 856/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.3463592529297
Batch 857/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.83334350585938
Batch 858/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.26034545898438
Batch 859/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.7957763671875
Batch 860/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.28579711914062
Batch 861/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.983642578125
Batch 862/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.05841064453125
Batch 863/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.157470703125
Batch 864/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.5736846923828
Batch 865/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.0604705810547
Batch 866/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.96971130371094
Batch 867/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.04420471191406
Batch 868/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.1822967529297
Batch 869/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.3964385986328
Batch 870/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.54437255859375
Batch 871/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.92703247070312
Batch 872/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.10049438476562
  0%|          | 872/1415250 [00:05<2:08:26, 183.53it/s, epoch=0, test_loss=178, train_loss=194]Batch 873/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.6444854736328
Batch 874/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.05516052246094
Batch 875/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.2880096435547
Batch 876/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.22813415527344
Batch 877/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.0943145751953
Batch 878/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6847686767578
Batch 879/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.7054901123047
Batch 880/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.15469360351562
Batch 881/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.07943725585938
Batch 882/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0626220703125
Batch 883/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.7624969482422
Batch 884/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.991943359375
Batch 885/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.81175231933594
Batch 886/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.47848510742188
Batch 887/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.41319274902344
Batch 888/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.98452758789062
Batch 889/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.2620849609375
Batch 890/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.01153564453125
Batch 891/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.90296936035156
  0%|          | 891/1415250 [00:05<2:08:23, 183.59it/s, epoch=0, test_loss=178, train_loss=198]Batch 892/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.70892333984375
Batch 893/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.82965087890625
Batch 894/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.2413330078125
Batch 895/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.68026733398438
Batch 896/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.1024169921875
Batch 897/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.02964782714844
Batch 898/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.32972717285156
Batch 899/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.12139892578125
Batch 900/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.71136474609375
Batch 901/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.05828857421875
Running validation...
Epoch 1, Step 900: Train Loss = 209.6018524169922, Test Loss = 180.26068115234375
Batch 902/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.92127990722656
Batch 903/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.1968994140625
Batch 904/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.97323608398438
Batch 905/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.00086975097656
Batch 906/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.56430053710938
Batch 907/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.5277862548828
Batch 908/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.27789306640625
Batch 909/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.04257202148438
Batch 910/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.9373321533203
  0%|          | 910/1415250 [00:05<2:10:08, 181.13it/s, epoch=0, test_loss=180, train_loss=211]Batch 911/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.78697204589844
Batch 912/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.95623779296875
Batch 913/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.2047576904297
Batch 914/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.57870483398438
Batch 915/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.95339965820312
Batch 916/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.79241943359375
Batch 917/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.7720489501953
Batch 918/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.61148071289062
Batch 919/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.45330810546875
Batch 920/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.32691955566406
Batch 921/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.11065673828125
Batch 922/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.1287078857422
Batch 923/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.41937255859375
Batch 924/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.93325805664062
Batch 925/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.11886596679688
Batch 926/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.6864471435547
Batch 927/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.56790161132812
Batch 928/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.8815460205078
Batch 929/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.91891479492188
  0%|          | 929/1415250 [00:05<2:09:58, 181.37it/s, epoch=0, test_loss=180, train_loss=198]Batch 930/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.8182373046875
Batch 931/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.42637634277344
Batch 932/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.18470764160156
Batch 933/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.30694580078125
Batch 934/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.3115234375
Batch 935/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.87608337402344
Batch 936/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.84226989746094
Batch 937/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.7523651123047
Batch 938/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.6356201171875
Batch 939/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.75820922851562
Batch 940/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.26129150390625
Batch 941/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.56365966796875
Batch 942/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.71990966796875
Batch 943/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0983428955078
Batch 944/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.27137756347656
Batch 945/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.2169647216797
Batch 946/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.8442840576172
Batch 947/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.51858520507812
Batch 948/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.89096069335938
  0%|          | 948/1415250 [00:05<2:08:41, 183.17it/s, epoch=0, test_loss=180, train_loss=191]Batch 949/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.9695587158203
Batch 950/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.43324279785156
Batch 951/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.5396270751953
Batch 952/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.87869262695312
Batch 953/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.88580322265625
Batch 954/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.02273559570312
Batch 955/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.93069458007812
Batch 956/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.19369506835938
Batch 957/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0760498046875
Batch 958/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.8111114501953
Batch 959/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.3515167236328
Batch 960/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.30296325683594
Batch 961/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.63412475585938
Batch 962/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.37486267089844
Batch 963/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.8769073486328
Batch 964/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.679931640625
Batch 965/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 219.121337890625
Batch 966/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.29779052734375
Batch 967/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.63583374023438
  0%|          | 967/1415250 [00:05<2:08:36, 183.28it/s, epoch=0, test_loss=180, train_loss=180]Batch 968/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.1529083251953
Batch 969/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.3638153076172
Batch 970/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.6241912841797
Batch 971/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.21307373046875
Batch 972/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.07630920410156
Batch 973/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.47027587890625
Batch 974/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.0373077392578
Batch 975/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.04751586914062
Batch 976/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.85983276367188
Batch 977/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.68789672851562
Batch 978/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.98834228515625
Batch 979/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.51075744628906
Batch 980/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.55569458007812
Batch 981/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.02757263183594
Batch 982/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.17356872558594
Batch 983/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.1608428955078
Batch 984/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.53260803222656
Batch 985/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.62400817871094
Batch 986/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.4396514892578
  0%|          | 986/1415250 [00:05<2:08:14, 183.79it/s, epoch=0, test_loss=180, train_loss=196]Batch 987/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6813507080078
Batch 988/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.4823455810547
Batch 989/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.04591369628906
Batch 990/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.05860900878906
Batch 991/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.93707275390625
Batch 992/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.44329833984375
Batch 993/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.07220458984375
Batch 994/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.70452880859375
Batch 995/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.005859375
Batch 996/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.56800842285156
Batch 997/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 222.10601806640625
Batch 998/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.39462280273438
Batch 999/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.04066467285156
Batch 1000/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.5840606689453
Batch 1001/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.2859649658203
Running validation...
Epoch 1, Step 1000: Train Loss = 205.64695739746094, Test Loss = 184.44895935058594
Batch 1002/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.13307189941406
Batch 1003/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.1417236328125
Batch 1004/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.56005859375
Batch 1005/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.0709686279297
  0%|          | 1005/1415250 [00:05<2:08:40, 183.19it/s, epoch=0, test_loss=184, train_loss=172]Batch 1006/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.36306762695312
Batch 1007/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.51092529296875
Batch 1008/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.0462646484375
Batch 1009/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.55628967285156
Batch 1010/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.80706787109375
Batch 1011/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.2268524169922
Batch 1012/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.1300048828125
Batch 1013/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.38157653808594
Batch 1014/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.22555541992188
Batch 1015/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.9756317138672
Batch 1016/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.1797332763672
Batch 1017/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.70228576660156
Batch 1018/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.6482391357422
Batch 1019/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.29953002929688
Batch 1020/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.2613983154297
Batch 1021/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.6647491455078
Batch 1022/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.9884033203125
Batch 1023/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.55679321289062
Batch 1024/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.13832092285156
  0%|          | 1024/1415250 [00:05<2:07:26, 184.96it/s, epoch=0, test_loss=184, train_loss=180]Batch 1025/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.6695556640625
Batch 1026/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.31813049316406
Batch 1027/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.55067443847656
Batch 1028/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.3866424560547
Batch 1029/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.0031280517578
Batch 1030/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.91893005371094
Batch 1031/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.95729064941406
Batch 1032/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.11448669433594
Batch 1033/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.25633239746094
Batch 1034/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.69879150390625
Batch 1035/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.3209686279297
Batch 1036/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.16099548339844
Batch 1037/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.4718780517578
Batch 1038/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.38229370117188
Batch 1039/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.62698364257812
Batch 1040/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.52320861816406
Batch 1041/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.6837158203125
Batch 1042/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.58042907714844
Batch 1043/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.25308227539062
  0%|          | 1043/1415250 [00:06<2:07:13, 185.26it/s, epoch=0, test_loss=184, train_loss=211]Batch 1044/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.8484344482422
Batch 1045/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.87484741210938
Batch 1046/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.2557373046875
Batch 1047/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.9414520263672
Batch 1048/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.82542419433594
Batch 1049/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.59906005859375
Batch 1050/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.9403839111328
Batch 1051/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.413818359375
Batch 1052/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.21363830566406
Batch 1053/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.1954345703125
Batch 1054/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.3549346923828
Batch 1055/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.5064239501953
Batch 1056/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.7198486328125
Batch 1057/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.21621704101562
Batch 1058/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.16082763671875
Batch 1059/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.7248992919922
Batch 1060/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.7200469970703
Batch 1061/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.04776000976562
Batch 1062/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.9870147705078
  0%|          | 1062/1415250 [00:06<2:06:24, 186.47it/s, epoch=0, test_loss=184, train_loss=206]Batch 1063/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.34500122070312
Batch 1064/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.1583709716797
Batch 1065/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.1472930908203
Batch 1066/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.1089630126953
Batch 1067/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.27821350097656
Batch 1068/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.21315002441406
Batch 1069/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.32481384277344
Batch 1070/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.3433380126953
Batch 1071/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.12420654296875
Batch 1072/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.81918334960938
Batch 1073/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.1829376220703
Batch 1074/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.404296875
Batch 1075/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.28677368164062
Batch 1076/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.0054168701172
Batch 1077/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.5507354736328
Batch 1078/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.24331665039062
Batch 1079/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.08316040039062
Batch 1080/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.93748474121094
Batch 1081/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.43663024902344
  0%|          | 1081/1415250 [00:06<2:06:36, 186.17it/s, epoch=0, test_loss=184, train_loss=185]Batch 1082/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.5343017578125
Batch 1083/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.20960998535156
Batch 1084/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.4241485595703
Batch 1085/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.66172790527344
Batch 1086/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.5416259765625
Batch 1087/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.2759246826172
Batch 1088/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 217.34010314941406
Batch 1089/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.949951171875
Batch 1090/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.3289794921875
Batch 1091/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.16830444335938
Batch 1092/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.10897827148438
Batch 1093/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.54489135742188
Batch 1094/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.49095153808594
Batch 1095/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 218.1768798828125
Batch 1096/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.1874237060547
Batch 1097/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.80921936035156
Batch 1098/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.1685333251953
Batch 1099/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.78411865234375
Batch 1100/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.62451171875
  0%|          | 1100/1415250 [00:06<2:05:53, 187.21it/s, epoch=0, test_loss=184, train_loss=190]Batch 1101/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.6478271484375
Running validation...
Epoch 1, Step 1100: Train Loss = 185.64788818359375, Test Loss = 171.67855834960938
Batch 1102/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.35931396484375
Batch 1103/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.29159545898438
Batch 1104/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.44190979003906
Batch 1105/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.4521942138672
Batch 1106/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.0042724609375
Batch 1107/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0965118408203
Batch 1108/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.73387145996094
Batch 1109/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.25152587890625
Batch 1110/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.7465362548828
Batch 1111/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.36642456054688
Batch 1112/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.58448791503906
Batch 1113/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.44956970214844
Batch 1114/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.7281951904297
Batch 1115/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.55284118652344
Batch 1116/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.70172119140625
Batch 1117/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.32679748535156
Batch 1118/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.1800079345703
Batch 1119/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.6598358154297
  0%|          | 1119/1415250 [00:06<2:07:35, 184.71it/s, epoch=0, test_loss=172, train_loss=188]Batch 1120/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.7140350341797
Batch 1121/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.40768432617188
Batch 1122/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.27867126464844
Batch 1123/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.98171997070312
Batch 1124/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.1453857421875
Batch 1125/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.0153350830078
Batch 1126/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.44790649414062
Batch 1127/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.1021728515625
Batch 1128/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.33648681640625
Batch 1129/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.29212951660156
Batch 1130/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.1347198486328
Batch 1131/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.3512725830078
Batch 1132/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.6576690673828
Batch 1133/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.08688354492188
Batch 1134/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.30906677246094
Batch 1135/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.40711975097656
Batch 1136/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.7567596435547
Batch 1137/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.51536560058594
Batch 1138/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.9120635986328
  0%|          | 1138/1415250 [00:06<2:07:34, 184.75it/s, epoch=0, test_loss=172, train_loss=193]Batch 1139/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.71340942382812
Batch 1140/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.0975341796875
Batch 1141/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.42298889160156
Batch 1142/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.69540405273438
Batch 1143/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.38975524902344
Batch 1144/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.78402709960938
Batch 1145/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.00682067871094
Batch 1146/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.11798095703125
Batch 1147/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.2652130126953
Batch 1148/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.72225952148438
Batch 1149/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.9363250732422
Batch 1150/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.09536743164062
Batch 1151/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.07408142089844
Batch 1152/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.20018005371094
Batch 1153/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.21102905273438
Batch 1154/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.06954956054688
Batch 1155/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.1666717529297
Batch 1156/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.1165771484375
Batch 1157/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.90370178222656
  0%|          | 1157/1415250 [00:06<2:08:28, 183.45it/s, epoch=0, test_loss=172, train_loss=203]Batch 1158/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.13449096679688
Batch 1159/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.09153747558594
Batch 1160/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.46360778808594
Batch 1161/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.1777801513672
Batch 1162/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.5321502685547
Batch 1163/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.84793090820312
Batch 1164/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.0594940185547
Batch 1165/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.401611328125
Batch 1166/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.4348602294922
Batch 1167/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.02647399902344
Batch 1168/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.8821563720703
Batch 1169/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.54714965820312
Batch 1170/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.92874145507812
Batch 1171/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.59080505371094
Batch 1172/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.2898406982422
Batch 1173/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.95306396484375
Batch 1174/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.21295166015625
Batch 1175/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.41839599609375
Batch 1176/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.34730529785156
  0%|          | 1176/1415250 [00:06<2:10:04, 181.20it/s, epoch=0, test_loss=172, train_loss=197]Batch 1177/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.6019744873047
Batch 1178/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.1049041748047
Batch 1179/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.1177215576172
Batch 1180/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.69320678710938
Batch 1181/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.39175415039062
Batch 1182/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.27525329589844
Batch 1183/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.8020782470703
Batch 1184/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.74893188476562
Batch 1185/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.56961059570312
Batch 1186/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.0701141357422
Batch 1187/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.01588439941406
Batch 1188/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.92486572265625
Batch 1189/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.8131103515625
Batch 1190/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.79104614257812
Batch 1191/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.76571655273438
Batch 1192/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.8727264404297
Batch 1193/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2326202392578
Batch 1194/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.64955139160156
Batch 1195/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.5065460205078
  0%|          | 1195/1415250 [00:06<2:08:57, 182.76it/s, epoch=0, test_loss=172, train_loss=201]Batch 1196/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.34942626953125
Batch 1197/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.39584350585938
Batch 1198/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.1560516357422
Batch 1199/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.31527709960938
Batch 1200/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.69529724121094
Batch 1201/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.91192626953125
Running validation...
Epoch 1, Step 1200: Train Loss = 188.16307067871094, Test Loss = 191.22039794921875
Batch 1202/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.01614379882812
Batch 1203/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.58001708984375
Batch 1204/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.98876953125
Batch 1205/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.64306640625
Batch 1206/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.6045379638672
Batch 1207/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.20469665527344
Batch 1208/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.42137145996094
Batch 1209/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.65565490722656
Batch 1210/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.13482666015625
Batch 1211/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.28871154785156
Batch 1212/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.257080078125
Batch 1213/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.44705200195312
Batch 1214/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.4480438232422
  0%|          | 1214/1415250 [00:06<2:08:45, 183.03it/s, epoch=0, test_loss=191, train_loss=197]Batch 1215/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.16021728515625
Batch 1216/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.55868530273438
Batch 1217/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.367431640625
Batch 1218/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.23831176757812
Batch 1219/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.9502410888672
Batch 1220/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.96983337402344
Batch 1221/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.2533721923828
Batch 1222/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.1805419921875
Batch 1223/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.1225128173828
Batch 1224/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.96383666992188
Batch 1225/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.90379333496094
Batch 1226/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.53884887695312
Batch 1227/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.15234375
Batch 1228/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.2833709716797
Batch 1229/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.24972534179688
Batch 1230/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.41757202148438
Batch 1231/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.9171600341797
Batch 1232/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.47853088378906
Batch 1233/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.2721405029297
  0%|          | 1233/1415250 [00:07<2:07:52, 184.29it/s, epoch=0, test_loss=191, train_loss=194]Batch 1234/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.2986602783203
Batch 1235/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.78700256347656
Batch 1236/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.02456665039062
Batch 1237/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.48960876464844
Batch 1238/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.26719665527344
Batch 1239/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.91879272460938
Batch 1240/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.931640625
Batch 1241/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.84796142578125
Batch 1242/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.3251190185547
Batch 1243/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.1969757080078
Batch 1244/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.5895233154297
Batch 1245/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.11265563964844
Batch 1246/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.58116149902344
Batch 1247/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.94058227539062
Batch 1248/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.75779724121094
Batch 1249/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.4201202392578
Batch 1250/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.46868896484375
Batch 1251/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.78500366210938
Batch 1252/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.22547912597656
Batch 1253/2550
Batch data shape: (20, 28, 28, 1)
  0%|          | 1252/1415250 [00:07<2:07:03, 185.47it/s, epoch=0, test_loss=191, train_loss=171]Train Loss: 198.72727966308594
Batch 1254/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.51766967773438
Batch 1255/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.79991149902344
Batch 1256/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.8880157470703
Batch 1257/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 221.7088623046875
Batch 1258/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.56968688964844
Batch 1259/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.66110229492188
Batch 1260/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.33909606933594
Batch 1261/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.5582275390625
Batch 1262/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.76539611816406
Batch 1263/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.52476501464844
Batch 1264/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.93019104003906
Batch 1265/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.83627319335938
Batch 1266/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.8985595703125
Batch 1267/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.1407012939453
Batch 1268/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.5535888671875
Batch 1269/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.7041473388672
Batch 1270/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.13694763183594
Batch 1271/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.58074951171875
  0%|          | 1271/1415250 [00:07<2:07:22, 185.03it/s, epoch=0, test_loss=191, train_loss=171]Batch 1272/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.68548583984375
Batch 1273/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.8507537841797
Batch 1274/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.27638244628906
Batch 1275/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.08511352539062
Batch 1276/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.32440185546875
Batch 1277/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.9983367919922
Batch 1278/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.58529663085938
Batch 1279/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1481475830078
Batch 1280/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.52915954589844
Batch 1281/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.1563720703125
Batch 1282/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.19302368164062
Batch 1283/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.0740966796875
Batch 1284/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.8628692626953
Batch 1285/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.69534301757812
Batch 1286/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.32701110839844
Batch 1287/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.80572509765625
Batch 1288/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.2986297607422
Batch 1289/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.62722778320312
Batch 1290/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.39869689941406
Batch 1291/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.17445373535156
  0%|          | 1291/1415250 [00:07<2:06:28, 186.32it/s, epoch=0, test_loss=191, train_loss=190]Batch 1292/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.58840942382812
Batch 1293/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.84768676757812
Batch 1294/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.95286560058594
Batch 1295/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.53750610351562
Batch 1296/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.2003936767578
Batch 1297/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.04576110839844
Batch 1298/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0238037109375
Batch 1299/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.83981323242188
Batch 1300/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.14553833007812
Batch 1301/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.98777770996094
Running validation...
Epoch 1, Step 1300: Train Loss = 177.2063446044922, Test Loss = 173.5820770263672
Batch 1302/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.78134155273438
Batch 1303/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7474365234375
Batch 1304/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.62466430664062
Batch 1305/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.94691467285156
Batch 1306/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.3463134765625
Batch 1307/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.36016845703125
Batch 1308/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.00575256347656
Batch 1309/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.9842071533203
Batch 1310/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.61500549316406
  0%|          | 1310/1415250 [00:07<2:07:48, 184.39it/s, epoch=0, test_loss=174, train_loss=207]Batch 1311/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.9332733154297
Batch 1312/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.59515380859375
Batch 1313/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.84271240234375
Batch 1314/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.5423126220703
Batch 1315/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.7683563232422
Batch 1316/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.3678436279297
Batch 1317/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.3429412841797
Batch 1318/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.60678100585938
Batch 1319/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.12271118164062
Batch 1320/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.86227416992188
Batch 1321/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.9701385498047
Batch 1322/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.8003692626953
Batch 1323/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.0219268798828
Batch 1324/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.2726593017578
Batch 1325/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.77749633789062
Batch 1326/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.3748779296875
Batch 1327/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.60696411132812
Batch 1328/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.9203643798828
Batch 1329/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.42642211914062
  0%|          | 1329/1415250 [00:07<2:07:26, 184.92it/s, epoch=0, test_loss=174, train_loss=190]Batch 1330/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.8905029296875
Batch 1331/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.2644805908203
Batch 1332/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.85301208496094
Batch 1333/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.004150390625
Batch 1334/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.00877380371094
Batch 1335/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.76368713378906
Batch 1336/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.54159545898438
Batch 1337/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.01206970214844
Batch 1338/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.78146362304688
Batch 1339/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.9974822998047
Batch 1340/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.22702026367188
Batch 1341/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.0545654296875
Batch 1342/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.5440673828125
Batch 1343/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.48130798339844
Batch 1344/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.7923583984375
Batch 1345/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.86122131347656
Batch 1346/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.72544860839844
Batch 1347/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 213.4259490966797
Batch 1348/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.00827026367188
  0%|          | 1348/1415250 [00:07<2:07:49, 184.34it/s, epoch=0, test_loss=174, train_loss=174]Batch 1349/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.41744995117188
Batch 1350/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.28216552734375
Batch 1351/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.18893432617188
Batch 1352/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.69671630859375
Batch 1353/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 208.80792236328125
Batch 1354/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.55416870117188
Batch 1355/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.24188232421875
Batch 1356/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.4762725830078
Batch 1357/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.15423583984375
Batch 1358/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.56283569335938
Batch 1359/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.02381896972656
Batch 1360/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.23194885253906
Batch 1361/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.34039306640625
Batch 1362/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.09178161621094
Batch 1363/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.62319946289062
Batch 1364/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.36822509765625
Batch 1365/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.71658325195312
Batch 1366/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.64707946777344
Batch 1367/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.75184631347656
  0%|          | 1367/1415250 [00:07<2:10:41, 180.31it/s, epoch=0, test_loss=174, train_loss=184]Batch 1368/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.65908813476562
Batch 1369/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.36630249023438
Batch 1370/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.86212158203125
Batch 1371/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.70750427246094
Batch 1372/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.251953125
Batch 1373/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.52651977539062
Batch 1374/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0608367919922
Batch 1375/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.09825134277344
Batch 1376/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.38821411132812
Batch 1377/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.8454132080078
Batch 1378/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.72073364257812
Batch 1379/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.4690399169922
Batch 1380/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.82276916503906
Batch 1381/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.39459228515625
Batch 1382/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.1580810546875
Batch 1383/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.95460510253906
Batch 1384/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.5244140625
Batch 1385/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.4529266357422
Batch 1386/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.25518798828125
  0%|          | 1386/1415250 [00:07<2:10:52, 180.06it/s, epoch=0, test_loss=174, train_loss=173]Batch 1387/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.3589324951172
Batch 1388/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.58778381347656
Batch 1389/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.06906127929688
Batch 1390/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.16151428222656
Batch 1391/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.02699279785156
Batch 1392/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.9040985107422
Batch 1393/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.09033203125
Batch 1394/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.02659606933594
Batch 1395/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.72943115234375
Batch 1396/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.6861572265625
Batch 1397/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.6153106689453
Batch 1398/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.57142639160156
Batch 1399/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.19593811035156
Batch 1400/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.47877502441406
Batch 1401/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.47056579589844
Running validation...
Epoch 1, Step 1400: Train Loss = 186.90399169921875, Test Loss = 195.71693420410156
Batch 1402/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.35755920410156
Batch 1403/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.87290954589844
Batch 1404/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.1063995361328
Batch 1405/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.0885467529297
  0%|          | 1405/1415250 [00:08<2:13:35, 176.39it/s, epoch=0, test_loss=196, train_loss=193]Batch 1406/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.01022338867188
Batch 1407/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.11221313476562
Batch 1408/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.607177734375
Batch 1409/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.3350372314453
Batch 1410/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.5996856689453
Batch 1411/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.14500427246094
Batch 1412/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.75489807128906
Batch 1413/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.82608032226562
Batch 1414/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.90684509277344
Batch 1415/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.02664184570312
Batch 1416/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.82521057128906
Batch 1417/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.80392456054688
Batch 1418/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.46290588378906
Batch 1419/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.83082580566406
Batch 1420/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.8726043701172
Batch 1421/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.70103454589844
Batch 1422/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.42416381835938
Batch 1423/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.7528076171875
Batch 1424/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.17056274414062
  0%|          | 1424/1415250 [00:08<2:12:33, 177.76it/s, epoch=0, test_loss=196, train_loss=199]Batch 1425/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.00575256347656
Batch 1426/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.42947387695312
Batch 1427/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.84068298339844
Batch 1428/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1385498046875
Batch 1429/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.77603149414062
Batch 1430/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.67196655273438
Batch 1431/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.756591796875
Batch 1432/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.44248962402344
Batch 1433/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.08213806152344
Batch 1434/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.95718383789062
Batch 1435/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.87306213378906
Batch 1436/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.79913330078125
Batch 1437/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.32339477539062
Batch 1438/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.44161987304688
Batch 1439/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.38372802734375
Batch 1440/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.4303436279297
Batch 1441/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.0559539794922
Batch 1442/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.37869262695312
Batch 1443/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.86770629882812
  0%|          | 1443/1415250 [00:08<2:12:09, 178.30it/s, epoch=0, test_loss=196, train_loss=183]Batch 1444/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.72914123535156
Batch 1445/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.22821044921875
Batch 1446/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.2272186279297
Batch 1447/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.60562133789062
Batch 1448/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.22695922851562
Batch 1449/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.54359436035156
Batch 1450/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.702880859375
Batch 1451/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.42503356933594
Batch 1452/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.07342529296875
Batch 1453/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.11355590820312
Batch 1454/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.14100646972656
Batch 1455/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.08375549316406
Batch 1456/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.2648162841797
Batch 1457/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.55145263671875
Batch 1458/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.78115844726562
Batch 1459/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.44989013671875
Batch 1460/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.4106903076172
Batch 1461/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.24411010742188
Batch 1462/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.0165252685547
  0%|          | 1462/1415250 [00:08<2:10:09, 181.04it/s, epoch=0, test_loss=196, train_loss=173]Batch 1463/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.28115844726562
Batch 1464/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.46963500976562
Batch 1465/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.6348419189453
Batch 1466/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.34320068359375
Batch 1467/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.8617706298828
Batch 1468/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.4627227783203
Batch 1469/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.1474151611328
Batch 1470/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.97120666503906
Batch 1471/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.15594482421875
Batch 1472/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.4513702392578
Batch 1473/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.09629821777344
Batch 1474/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.0745391845703
Batch 1475/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.70082092285156
Batch 1476/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.52442932128906
Batch 1477/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.02328491210938
Batch 1478/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.950927734375
Batch 1479/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.12811279296875
Batch 1480/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.8149871826172
Batch 1481/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.13946533203125
  0%|          | 1481/1415250 [00:08<2:08:52, 182.84it/s, epoch=0, test_loss=196, train_loss=166]Batch 1482/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.16075134277344
Batch 1483/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.22549438476562
Batch 1484/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.6241912841797
Batch 1485/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.84767150878906
Batch 1486/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.19073486328125
Batch 1487/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.82568359375
Batch 1488/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.64122009277344
Batch 1489/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.631591796875
Batch 1490/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.15359497070312
Batch 1491/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.76470947265625
Batch 1492/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.78521728515625
Batch 1493/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.83322143554688
Batch 1494/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.90895080566406
Batch 1495/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.69020080566406
Batch 1496/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.67788696289062
Batch 1497/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.477783203125
Batch 1498/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.18629455566406
Batch 1499/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.70126342773438
Batch 1500/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.64231872558594
  0%|          | 1500/1415250 [00:08<2:07:45, 184.42it/s, epoch=0, test_loss=196, train_loss=194]Batch 1501/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.94363403320312
Running validation...
Epoch 1, Step 1500: Train Loss = 196.41323852539062, Test Loss = 184.84390258789062
Batch 1502/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.54180908203125
Batch 1503/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.16297912597656
Batch 1504/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 211.2128143310547
Batch 1505/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.02125549316406
Batch 1506/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.03700256347656
Batch 1507/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.8882293701172
Batch 1508/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.27552795410156
Batch 1509/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.6063232421875
Batch 1510/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.67442321777344
Batch 1511/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.27391052246094
Batch 1512/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.63626098632812
Batch 1513/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.0719451904297
Batch 1514/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.2106475830078
Batch 1515/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.3927764892578
Batch 1516/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.33558654785156
Batch 1517/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.1038818359375
Batch 1518/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.42022705078125
Batch 1519/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.74270629882812
  0%|          | 1519/1415250 [00:08<2:08:41, 183.09it/s, epoch=0, test_loss=185, train_loss=198]Batch 1520/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.44252014160156
Batch 1521/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.2021942138672
Batch 1522/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.974609375
Batch 1523/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.42955017089844
Batch 1524/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.30442810058594
Batch 1525/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.820556640625
Batch 1526/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.79701232910156
Batch 1527/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.81690979003906
Batch 1528/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.26437377929688
Batch 1529/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.79287719726562
Batch 1530/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.674072265625
Batch 1531/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.2327880859375
Batch 1532/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.7430877685547
Batch 1533/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.85638427734375
Batch 1534/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.6346893310547
Batch 1535/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.7964324951172
Batch 1536/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.84249877929688
Batch 1537/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.85630798339844
Batch 1538/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.13037109375
  0%|          | 1538/1415250 [00:08<2:10:24, 180.67it/s, epoch=0, test_loss=185, train_loss=177]Batch 1539/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.57041931152344
Batch 1540/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.668701171875
Batch 1541/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.99920654296875
Batch 1542/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.2730712890625
Batch 1543/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.68003845214844
Batch 1544/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.9728546142578
Batch 1545/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.79608154296875
Batch 1546/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.96665954589844
Batch 1547/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.5830078125
Batch 1548/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.6698455810547
Batch 1549/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.53929138183594
Batch 1550/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.0574188232422
Batch 1551/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.20953369140625
Batch 1552/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.84043884277344
Batch 1553/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.3981475830078
Batch 1554/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.13551330566406
Batch 1555/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.33261108398438
Batch 1556/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.6726837158203
Batch 1557/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.804931640625
  0%|          | 1557/1415250 [00:08<2:10:01, 181.21it/s, epoch=0, test_loss=185, train_loss=188]Batch 1558/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.78367614746094
Batch 1559/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.9810333251953
Batch 1560/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.6002960205078
Batch 1561/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.96820068359375
Batch 1562/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.57379150390625
Batch 1563/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.01565551757812
Batch 1564/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.8155975341797
Batch 1565/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.93817138671875
Batch 1566/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.052734375
Batch 1567/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.68722534179688
Batch 1568/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.5348663330078
Batch 1569/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.906494140625
Batch 1570/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.24468994140625
Batch 1571/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.72113037109375
Batch 1572/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.42523193359375
Batch 1573/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.306396484375
Batch 1574/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.55715942382812
Batch 1575/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.1314697265625
Batch 1576/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.6612091064453
  0%|          | 1576/1415250 [00:08<2:08:59, 182.65it/s, epoch=0, test_loss=185, train_loss=172]Batch 1577/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.39271545410156
Batch 1578/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.10040283203125
Batch 1579/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.69520568847656
Batch 1580/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.55844116210938
Batch 1581/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.5576171875
Batch 1582/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.0614013671875
Batch 1583/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.21017456054688
Batch 1584/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.43209838867188
Batch 1585/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.501953125
Batch 1586/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.34979248046875
Batch 1587/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.68385314941406
Batch 1588/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.56251525878906
Batch 1589/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.03514099121094
Batch 1590/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.24032592773438
Batch 1591/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.86119079589844
Batch 1592/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.06390380859375
Batch 1593/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.35614013671875
Batch 1594/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.76197814941406
Batch 1595/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.79173278808594
  0%|          | 1595/1415250 [00:09<2:08:51, 182.85it/s, epoch=0, test_loss=185, train_loss=180]Batch 1596/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.24765014648438
Batch 1597/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.5924530029297
Batch 1598/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.11976623535156
Batch 1599/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.61404418945312
Batch 1600/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.46534729003906
Batch 1601/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.76296997070312
Running validation...
Epoch 1, Step 1600: Train Loss = 167.9484405517578, Test Loss = 164.21615600585938
Batch 1602/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.65065002441406
Batch 1603/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.73899841308594
Batch 1604/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.98983764648438
Batch 1605/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.2877960205078
Batch 1606/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.4591064453125
Batch 1607/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.1702117919922
Batch 1608/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.2776336669922
Batch 1609/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.3909912109375
Batch 1610/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.69464111328125
Batch 1611/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.9746856689453
Batch 1612/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.67872619628906
Batch 1613/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.42990112304688
Batch 1614/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.3939208984375
  0%|          | 1614/1415250 [00:09<2:11:06, 179.70it/s, epoch=0, test_loss=164, train_loss=179]Batch 1615/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.95980834960938
Batch 1616/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.44760131835938
Batch 1617/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.86798095703125
Batch 1618/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.4382781982422
Batch 1619/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.22821044921875
Batch 1620/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.74148559570312
Batch 1621/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.01681518554688
Batch 1622/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.45542907714844
Batch 1623/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.7158660888672
Batch 1624/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.3932342529297
Batch 1625/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.24037170410156
Batch 1626/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.38038635253906
Batch 1627/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.0194549560547
Batch 1628/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.78672790527344
Batch 1629/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.1346435546875
Batch 1630/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.96182250976562
Batch 1631/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.79673767089844
Batch 1632/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.53616333007812
Batch 1633/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.4211883544922
  0%|          | 1633/1415250 [00:09<2:10:14, 180.89it/s, epoch=0, test_loss=164, train_loss=197]Batch 1634/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.62286376953125
Batch 1635/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.23924255371094
Batch 1636/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.77239990234375
Batch 1637/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.7932891845703
Batch 1638/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.86265563964844
Batch 1639/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.69351196289062
Batch 1640/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.7062225341797
Batch 1641/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.14068603515625
Batch 1642/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.85169982910156
Batch 1643/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.2844696044922
Batch 1644/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.813232421875
Batch 1645/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.58213806152344
Batch 1646/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.1697540283203
Batch 1647/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.3226776123047
Batch 1648/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.62062072753906
Batch 1649/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.05361938476562
Batch 1650/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.25685119628906
Batch 1651/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.43714904785156
Batch 1652/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.4384002685547
  0%|          | 1652/1415250 [00:09<2:10:07, 181.06it/s, epoch=0, test_loss=164, train_loss=209]Batch 1653/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.30523681640625
Batch 1654/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.1688232421875
Batch 1655/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.58421325683594
Batch 1656/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.95816040039062
Batch 1657/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.5348663330078
Batch 1658/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.32284545898438
Batch 1659/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.5337677001953
Batch 1660/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.5878143310547
Batch 1661/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.04525756835938
Batch 1662/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.3626708984375
Batch 1663/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.6583251953125
Batch 1664/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.82838439941406
Batch 1665/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.61453247070312
Batch 1666/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.62998962402344
Batch 1667/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.47726440429688
Batch 1668/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.04098510742188
Batch 1669/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.10394287109375
Batch 1670/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.43795776367188
Batch 1671/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.4534912109375
  0%|          | 1671/1415250 [00:09<2:10:28, 180.58it/s, epoch=0, test_loss=164, train_loss=190]Batch 1672/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.90603637695312
Batch 1673/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.34788513183594
Batch 1674/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.36181640625
Batch 1675/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.1932830810547
Batch 1676/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.5044403076172
Batch 1677/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.89779663085938
Batch 1678/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.0764617919922
Batch 1679/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.47708129882812
Batch 1680/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.95799255371094
Batch 1681/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.62750244140625
Batch 1682/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.67771911621094
Batch 1683/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.38560485839844
Batch 1684/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.96954345703125
Batch 1685/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.04347229003906
Batch 1686/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.9890594482422
Batch 1687/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.91587829589844
Batch 1688/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.5379638671875
Batch 1689/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.94569396972656
Batch 1690/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.7166748046875
  0%|          | 1690/1415250 [00:09<2:09:10, 182.38it/s, epoch=0, test_loss=164, train_loss=172]Batch 1691/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.11595153808594
Batch 1692/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.81443786621094
Batch 1693/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 206.7646484375
Batch 1694/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.70797729492188
Batch 1695/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.9695587158203
Batch 1696/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.45404052734375
Batch 1697/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.64633178710938
Batch 1698/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.87542724609375
Batch 1699/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.56289672851562
Batch 1700/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.53082275390625
Batch 1701/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.89857482910156
Running validation...
Epoch 1, Step 1700: Train Loss = 188.0025177001953, Test Loss = 166.51983642578125
Batch 1702/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.05845642089844
Batch 1703/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.74180603027344
Batch 1704/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.088134765625
Batch 1705/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.47190856933594
Batch 1706/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.88502502441406
Batch 1707/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.29251098632812
Batch 1708/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0845184326172
Batch 1709/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.93251037597656
  0%|          | 1709/1415250 [00:09<2:09:49, 181.47it/s, epoch=0, test_loss=167, train_loss=186]Batch 1710/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.314697265625
Batch 1711/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.22076416015625
Batch 1712/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.20919799804688
Batch 1713/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.63821411132812
Batch 1714/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.37863159179688
Batch 1715/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.27532958984375
Batch 1716/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.8781280517578
Batch 1717/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.98121643066406
Batch 1718/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.36114501953125
Batch 1719/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.7909698486328
Batch 1720/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.16563415527344
Batch 1721/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.95533752441406
Batch 1722/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.51321411132812
Batch 1723/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.99087524414062
Batch 1724/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.30723571777344
Batch 1725/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.7699432373047
Batch 1726/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.9783477783203
Batch 1727/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.89683532714844
Batch 1728/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.4300079345703
  0%|          | 1728/1415250 [00:09<2:10:04, 181.13it/s, epoch=0, test_loss=167, train_loss=192]Batch 1729/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.87612915039062
Batch 1730/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.45816040039062
Batch 1731/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.24697875976562
Batch 1732/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.83277893066406
Batch 1733/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.66265869140625
Batch 1734/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.3914794921875
Batch 1735/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0958251953125
Batch 1736/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.20318603515625
Batch 1737/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.6438751220703
Batch 1738/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.20314025878906
Batch 1739/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.80995178222656
Batch 1740/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.46376037597656
Batch 1741/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.48736572265625
Batch 1742/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.47804260253906
Batch 1743/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.71202087402344
Batch 1744/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.77064514160156
Batch 1745/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.95745849609375
Batch 1746/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.03472900390625
Batch 1747/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.3367156982422
  0%|          | 1747/1415250 [00:09<2:08:59, 182.63it/s, epoch=0, test_loss=167, train_loss=177]Batch 1748/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.68150329589844
Batch 1749/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 215.2859649658203
Batch 1750/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.29483032226562
Batch 1751/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.00213623046875
Batch 1752/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.45155334472656
Batch 1753/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.562255859375
Batch 1754/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.98377990722656
Batch 1755/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.4240264892578
Batch 1756/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.37327575683594
Batch 1757/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.73277282714844
Batch 1758/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.850341796875
Batch 1759/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2034912109375
Batch 1760/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.27444458007812
Batch 1761/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.52395629882812
Batch 1762/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.87991333007812
Batch 1763/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.34046936035156
Batch 1764/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.644287109375
Batch 1765/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.573974609375
Batch 1766/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.3115692138672
  0%|          | 1766/1415250 [00:10<2:08:08, 183.85it/s, epoch=0, test_loss=167, train_loss=196]Batch 1767/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.00596618652344
Batch 1768/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.46644592285156
Batch 1769/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.3552703857422
Batch 1770/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.28524780273438
Batch 1771/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.15582275390625
Batch 1772/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.42501831054688
Batch 1773/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.06124877929688
Batch 1774/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.4032440185547
Batch 1775/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.90359497070312
Batch 1776/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.84100341796875
Batch 1777/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.00648498535156
Batch 1778/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1622314453125
Batch 1779/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.23214721679688
Batch 1780/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.05113220214844
Batch 1781/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.70376586914062
Batch 1782/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.7813262939453
Batch 1783/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.4647979736328
Batch 1784/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.73533630371094
Batch 1785/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.80615234375
  0%|          | 1785/1415250 [00:10<2:07:11, 185.21it/s, epoch=0, test_loss=167, train_loss=181]Batch 1786/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.28887939453125
Batch 1787/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.76417541503906
Batch 1788/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.40797424316406
Batch 1789/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.0635223388672
Batch 1790/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.85479736328125
Batch 1791/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.2335662841797
Batch 1792/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.63160705566406
Batch 1793/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.3289794921875
Batch 1794/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.36318969726562
Batch 1795/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.317626953125
Batch 1796/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.1072540283203
Batch 1797/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.6028594970703
Batch 1798/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.3083038330078
Batch 1799/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.07318115234375
Batch 1800/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.8678436279297
Batch 1801/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.33651733398438
Running validation...
Epoch 1, Step 1800: Train Loss = 198.2244415283203, Test Loss = 184.3199920654297
Batch 1802/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.78115844726562
Batch 1803/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.1069793701172
Batch 1804/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.4505615234375
  0%|          | 1804/1415250 [00:10<2:08:36, 183.18it/s, epoch=0, test_loss=184, train_loss=185]Batch 1805/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.95477294921875
Batch 1806/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.56163024902344
Batch 1807/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.048095703125
Batch 1808/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.0699920654297
Batch 1809/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.77777099609375
Batch 1810/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.5365753173828
Batch 1811/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 212.1917266845703
Batch 1812/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.66929626464844
Batch 1813/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.86465454101562
Batch 1814/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.7998504638672
Batch 1815/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.7613067626953
Batch 1816/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.1089630126953
Batch 1817/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.97012329101562
Batch 1818/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.52133178710938
Batch 1819/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.25914001464844
Batch 1820/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.19720458984375
Batch 1821/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.08990478515625
Batch 1822/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 205.9409637451172
Batch 1823/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.1018524169922
  0%|          | 1823/1415250 [00:10<2:09:32, 181.85it/s, epoch=0, test_loss=184, train_loss=194]Batch 1824/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.80113220214844
Batch 1825/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.6204376220703
Batch 1826/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.87429809570312
Batch 1827/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.04757690429688
Batch 1828/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.13818359375
Batch 1829/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.8319091796875
Batch 1830/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.33905029296875
Batch 1831/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.65138244628906
Batch 1832/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.56346130371094
Batch 1833/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.71461486816406
Batch 1834/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.109130859375
Batch 1835/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.14642333984375
Batch 1836/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.55230712890625
Batch 1837/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.919921875
Batch 1838/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0980224609375
Batch 1839/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.1854248046875
Batch 1840/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.9741668701172
Batch 1841/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.0693359375
Batch 1842/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.16061401367188
  0%|          | 1842/1415250 [00:10<2:09:02, 182.55it/s, epoch=0, test_loss=184, train_loss=195]Batch 1843/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.15223693847656
Batch 1844/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.01760864257812
Batch 1845/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.7753143310547
Batch 1846/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.69110107421875
Batch 1847/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.8656005859375
Batch 1848/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.57943725585938
Batch 1849/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.15542602539062
Batch 1850/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.72805786132812
Batch 1851/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.9351043701172
Batch 1852/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7372589111328
Batch 1853/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.67938232421875
Batch 1854/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.88340759277344
Batch 1855/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.1510467529297
Batch 1856/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 214.1356964111328
Batch 1857/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.63385009765625
Batch 1858/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.44921875
Batch 1859/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.78512573242188
Batch 1860/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.25082397460938
Batch 1861/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.24122619628906
  0%|          | 1861/1415250 [00:10<2:07:55, 184.15it/s, epoch=0, test_loss=184, train_loss=194]Batch 1862/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.3920440673828
Batch 1863/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.70535278320312
Batch 1864/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.4087677001953
Batch 1865/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.5996551513672
Batch 1866/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.98509216308594
Batch 1867/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.02639770507812
Batch 1868/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.1108856201172
Batch 1869/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.34780883789062
Batch 1870/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.58538818359375
Batch 1871/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.57066345214844
Batch 1872/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.28919982910156
Batch 1873/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.30819702148438
Batch 1874/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.45703125
Batch 1875/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.0673828125
Batch 1876/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.14198303222656
Batch 1877/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.55355834960938
Batch 1878/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.48777770996094
Batch 1879/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.15957641601562
Batch 1880/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.22210693359375
  0%|          | 1880/1415250 [00:10<2:07:49, 184.29it/s, epoch=0, test_loss=184, train_loss=186]Batch 1881/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.1260986328125
Batch 1882/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.77850341796875
Batch 1883/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.34031677246094
Batch 1884/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.17489624023438
Batch 1885/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.95187377929688
Batch 1886/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.32174682617188
Batch 1887/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.00697326660156
Batch 1888/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.18836975097656
Batch 1889/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.05091857910156
Batch 1890/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.48687744140625
Batch 1891/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.3760528564453
Batch 1892/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.6669921875
Batch 1893/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.8076171875
Batch 1894/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.6915283203125
Batch 1895/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.3934326171875
Batch 1896/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.01124572753906
Batch 1897/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.3112335205078
Batch 1898/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.30795288085938
Batch 1899/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.5407257080078
  0%|          | 1899/1415250 [00:10<2:07:39, 184.52it/s, epoch=0, test_loss=184, train_loss=194]Batch 1900/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.88877868652344
Batch 1901/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.35687255859375
Running validation...
Epoch 1, Step 1900: Train Loss = 177.5443878173828, Test Loss = 176.31192016601562
Batch 1902/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.883056640625
Batch 1903/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.05931091308594
Batch 1904/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.000732421875
Batch 1905/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.1980438232422
Batch 1906/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.18801879882812
Batch 1907/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2399444580078
Batch 1908/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.5244903564453
Batch 1909/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.5382537841797
Batch 1910/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.78524780273438
Batch 1911/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.09805297851562
Batch 1912/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.66830444335938
Batch 1913/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.6458282470703
Batch 1914/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.5673828125
Batch 1915/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.0160369873047
Batch 1916/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.30555725097656
Batch 1917/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.32113647460938
Batch 1918/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.9249267578125
  0%|          | 1918/1415250 [00:10<2:11:05, 179.69it/s, epoch=0, test_loss=176, train_loss=175]Batch 1919/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 209.82896423339844
Batch 1920/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.76950073242188
Batch 1921/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.96290588378906
Batch 1922/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.52496337890625
Batch 1923/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.1318359375
Batch 1924/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.33587646484375
Batch 1925/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.23861694335938
Batch 1926/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.54733276367188
Batch 1927/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.24700927734375
Batch 1928/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.65982055664062
Batch 1929/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.7967529296875
Batch 1930/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.85159301757812
Batch 1931/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.9174041748047
Batch 1932/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.4146728515625
Batch 1933/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.265380859375
Batch 1934/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.88055419921875
Batch 1935/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.3778533935547
Batch 1936/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.54579162597656
Batch 1937/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.85824584960938
  0%|          | 1937/1415250 [00:10<2:10:19, 180.74it/s, epoch=0, test_loss=176, train_loss=165]Batch 1938/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 202.92144775390625
Batch 1939/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.10488891601562
Batch 1940/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.60186767578125
Batch 1941/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.09144592285156
Batch 1942/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.1274871826172
Batch 1943/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.8009033203125
Batch 1944/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.9132080078125
Batch 1945/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.96136474609375
Batch 1946/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.0773468017578
Batch 1947/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.3228759765625
Batch 1948/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 207.11924743652344
Batch 1949/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.9246063232422
Batch 1950/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.13185119628906
Batch 1951/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.40481567382812
Batch 1952/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.82725524902344
Batch 1953/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.3295135498047
Batch 1954/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.91827392578125
Batch 1955/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.2411346435547
Batch 1956/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.89971923828125
  0%|          | 1956/1415250 [00:11<2:09:16, 182.20it/s, epoch=0, test_loss=176, train_loss=175]Batch 1957/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.4269561767578
Batch 1958/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.34617614746094
Batch 1959/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.77919006347656
Batch 1960/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.06919860839844
Batch 1961/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.5001983642578
Batch 1962/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.66351318359375
Batch 1963/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.9639434814453
Batch 1964/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.94203186035156
Batch 1965/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.01438903808594
Batch 1966/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.0977325439453
Batch 1967/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.49053955078125
Batch 1968/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.79945373535156
Batch 1969/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.55494689941406
Batch 1970/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.03240966796875
Batch 1971/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.78985595703125
Batch 1972/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.4674835205078
Batch 1973/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.41018676757812
Batch 1974/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.38455200195312
Batch 1975/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.720458984375
  0%|          | 1975/1415250 [00:11<2:08:44, 182.97it/s, epoch=0, test_loss=176, train_loss=194]Batch 1976/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.80731201171875
Batch 1977/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.90463256835938
Batch 1978/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.86097717285156
Batch 1979/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.39047241210938
Batch 1980/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.09967041015625
Batch 1981/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.5063018798828
Batch 1982/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.7941131591797
Batch 1983/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.7317352294922
Batch 1984/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.52793884277344
Batch 1985/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.5924072265625
Batch 1986/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.5302276611328
Batch 1987/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.15037536621094
Batch 1988/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.1366729736328
Batch 1989/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.62063598632812
Batch 1990/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.69271850585938
Batch 1991/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.6476593017578
Batch 1992/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.75570678710938
Batch 1993/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.33876037597656
Batch 1994/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.15115356445312
  0%|          | 1994/1415250 [00:11<2:09:14, 182.26it/s, epoch=0, test_loss=176, train_loss=168]Batch 1995/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 210.70083618164062
Batch 1996/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.72669982910156
Batch 1997/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.82481384277344
Batch 1998/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.63682556152344
Batch 1999/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.6844482421875
Batch 2000/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.54750061035156
Batch 2001/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.27133178710938
Running validation...
Epoch 1, Step 2000: Train Loss = 198.6105499267578, Test Loss = 178.00518798828125
Batch 2002/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.26419067382812
Batch 2003/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.53729248046875
Batch 2004/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.768798828125
Batch 2005/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.0094757080078
Batch 2006/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.12010192871094
Batch 2007/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.6815185546875
Batch 2008/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.31153869628906
Batch 2009/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.54177856445312
Batch 2010/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.20172119140625
Batch 2011/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.26904296875
Batch 2012/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.61935424804688
Batch 2013/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.4595489501953
  0%|          | 2013/1415250 [00:11<2:09:41, 181.61it/s, epoch=0, test_loss=178, train_loss=191]Batch 2014/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.99131774902344
Batch 2015/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.2089385986328
Batch 2016/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.32901000976562
Batch 2017/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.23545837402344
Batch 2018/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.1601104736328
Batch 2019/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.25086975097656
Batch 2020/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.5567626953125
Batch 2021/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.68063354492188
Batch 2022/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.13345336914062
Batch 2023/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.5371551513672
Batch 2024/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.4981689453125
Batch 2025/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.31494140625
Batch 2026/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.165771484375
Batch 2027/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.2073516845703
Batch 2028/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.3251190185547
Batch 2029/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.54673767089844
Batch 2030/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.0931854248047
Batch 2031/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.26068115234375
Batch 2032/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.7438507080078
  0%|          | 2032/1415250 [00:11<2:08:46, 182.90it/s, epoch=0, test_loss=178, train_loss=183]Batch 2033/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.44674682617188
Batch 2034/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.97222900390625
Batch 2035/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.80911254882812
Batch 2036/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.19276428222656
Batch 2037/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.3158721923828
Batch 2038/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.57925415039062
Batch 2039/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.90701293945312
Batch 2040/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0660858154297
Batch 2041/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.30032348632812
Batch 2042/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.4268035888672
Batch 2043/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.0216064453125
Batch 2044/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.15208435058594
Batch 2045/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.5234832763672
Batch 2046/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.50289916992188
Batch 2047/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.61276245117188
Batch 2048/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.78089904785156
Batch 2049/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.29403686523438
Batch 2050/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.826904296875
Batch 2051/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.48110961914062
  0%|          | 2051/1415250 [00:11<2:08:16, 183.61it/s, epoch=0, test_loss=178, train_loss=179]Batch 2052/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.04478454589844
Batch 2053/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.38722229003906
Batch 2054/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.865234375
Batch 2055/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.28550720214844
Batch 2056/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.14646911621094
Batch 2057/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.98826599121094
Batch 2058/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.72879028320312
Batch 2059/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.9121551513672
Batch 2060/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.70298767089844
Batch 2061/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.01217651367188
Batch 2062/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.19424438476562
Batch 2063/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.39768981933594
Batch 2064/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.3939666748047
Batch 2065/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.67236328125
Batch 2066/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.67300415039062
Batch 2067/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.0026397705078
Batch 2068/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.0651092529297
Batch 2069/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.95362854003906
Batch 2070/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.9572296142578
  0%|          | 2070/1415250 [00:11<2:09:48, 181.45it/s, epoch=0, test_loss=178, train_loss=181]Batch 2071/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.48670959472656
Batch 2072/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.52865600585938
Batch 2073/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.4297637939453
Batch 2074/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7277069091797
Batch 2075/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.74378967285156
Batch 2076/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.1631622314453
Batch 2077/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1748046875
Batch 2078/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.3466339111328
Batch 2079/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.76097106933594
Batch 2080/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.14842224121094
Batch 2081/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.53590393066406
Batch 2082/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.18711853027344
Batch 2083/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.8404541015625
Batch 2084/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.39599609375
Batch 2085/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.4646453857422
Batch 2086/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.34364318847656
Batch 2087/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.18429565429688
Batch 2088/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.62062072753906
Batch 2089/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.7091064453125
  0%|          | 2089/1415250 [00:11<2:10:02, 181.12it/s, epoch=0, test_loss=178, train_loss=166]Batch 2090/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.6153106689453
Batch 2091/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.41696166992188
Batch 2092/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.312255859375
Batch 2093/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.1899871826172
Batch 2094/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.36709594726562
Batch 2095/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.72164916992188
Batch 2096/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.9784698486328
Batch 2097/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 139.43133544921875
Batch 2098/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.7788543701172
Batch 2099/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.92308044433594
Batch 2100/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.36331176757812
Batch 2101/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.26449584960938
Running validation...
Epoch 1, Step 2100: Train Loss = 178.04844665527344, Test Loss = 168.35267639160156
Batch 2102/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.72344970703125
Batch 2103/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.7285614013672
Batch 2104/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.8212127685547
Batch 2105/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.63499450683594
Batch 2106/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.0996856689453
Batch 2107/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.86537170410156
Batch 2108/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.51519775390625
  0%|          | 2108/1415250 [00:11<2:10:17, 180.76it/s, epoch=0, test_loss=168, train_loss=198]Batch 2109/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.87106323242188
Batch 2110/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.2285614013672
Batch 2111/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.39645385742188
Batch 2112/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.8907928466797
Batch 2113/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.8523712158203
Batch 2114/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.34205627441406
Batch 2115/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.82131958007812
Batch 2116/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.37939453125
Batch 2117/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.28695678710938
Batch 2118/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.6239013671875
Batch 2119/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.4041290283203
Batch 2120/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.99163818359375
Batch 2121/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 140.456298828125
Batch 2122/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.72776794433594
Batch 2123/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.33529663085938
Batch 2124/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.58363342285156
Batch 2125/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.67294311523438
Batch 2126/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.3297882080078
Batch 2127/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.8046875
  0%|          | 2127/1415250 [00:12<2:09:07, 182.41it/s, epoch=0, test_loss=168, train_loss=197]Batch 2128/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.67984008789062
Batch 2129/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.0332794189453
Batch 2130/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.71212768554688
Batch 2131/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.48497009277344
Batch 2132/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.06588745117188
Batch 2133/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.2493133544922
Batch 2134/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.02320861816406
Batch 2135/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0486297607422
Batch 2136/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.16143798828125
Batch 2137/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.00830078125
Batch 2138/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.94143676757812
Batch 2139/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.69383239746094
Batch 2140/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0141143798828
Batch 2141/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.25030517578125
Batch 2142/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.5535430908203
Batch 2143/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.00331115722656
Batch 2144/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.4290008544922
Batch 2145/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.67892456054688
Batch 2146/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.66534423828125
  0%|          | 2146/1415250 [00:12<2:08:28, 183.31it/s, epoch=0, test_loss=168, train_loss=159]Batch 2147/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.39306640625
Batch 2148/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.51727294921875
Batch 2149/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.72726440429688
Batch 2150/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.92254638671875
Batch 2151/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.77261352539062
Batch 2152/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.7620391845703
Batch 2153/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.94361877441406
Batch 2154/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.15249633789062
Batch 2155/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.34149169921875
Batch 2156/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.02517700195312
Batch 2157/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.3884735107422
Batch 2158/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.55352783203125
Batch 2159/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.4517059326172
Batch 2160/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.80715942382812
Batch 2161/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.278076171875
Batch 2162/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.67691040039062
Batch 2163/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.8925018310547
Batch 2164/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.22857666015625
Batch 2165/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.72914123535156
  0%|          | 2165/1415250 [00:12<2:07:51, 184.19it/s, epoch=0, test_loss=168, train_loss=184]Batch 2166/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.01409912109375
Batch 2167/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.6532440185547
Batch 2168/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.9542694091797
Batch 2169/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.17236328125
Batch 2170/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.9212188720703
Batch 2171/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.2966766357422
Batch 2172/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.01377868652344
Batch 2173/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.0852813720703
Batch 2174/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.9066925048828
Batch 2175/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.09825134277344
Batch 2176/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.83143615722656
Batch 2177/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.30279541015625
Batch 2178/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.955078125
Batch 2179/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.32725524902344
Batch 2180/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.70506286621094
Batch 2181/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.8229217529297
Batch 2182/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.7202606201172
Batch 2183/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.11001586914062
Batch 2184/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.10389709472656
  0%|          | 2184/1415250 [00:12<2:08:01, 183.95it/s, epoch=0, test_loss=168, train_loss=190]Batch 2185/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.29722595214844
Batch 2186/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.5811004638672
Batch 2187/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.77365112304688
Batch 2188/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.21609497070312
Batch 2189/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.6193084716797
Batch 2190/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.91506958007812
Batch 2191/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.29864501953125
Batch 2192/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.71070861816406
Batch 2193/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.9592742919922
Batch 2194/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.0828857421875
Batch 2195/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.2458038330078
Batch 2196/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.5061798095703
Batch 2197/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.3339080810547
Batch 2198/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 216.80892944335938
Batch 2199/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.83074951171875
Batch 2200/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.3662872314453
Batch 2201/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.1758575439453
Running validation...
Epoch 1, Step 2200: Train Loss = 154.59439086914062, Test Loss = 182.90325927734375
Batch 2202/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.08493041992188
Batch 2203/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.42210388183594
  0%|          | 2203/1415250 [00:12<2:09:10, 182.31it/s, epoch=0, test_loss=183, train_loss=180]Batch 2204/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.88302612304688
Batch 2205/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.13267517089844
Batch 2206/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.75025939941406
Batch 2207/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.0568389892578
Batch 2208/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.26443481445312
Batch 2209/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.23565673828125
Batch 2210/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.12301635742188
Batch 2211/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.40206909179688
Batch 2212/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.96925354003906
Batch 2213/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.57591247558594
Batch 2214/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.37783813476562
Batch 2215/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.74578857421875
Batch 2216/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.22872924804688
Batch 2217/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.99147033691406
Batch 2218/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.5410614013672
Batch 2219/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.143310546875
Batch 2220/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.49960327148438
Batch 2221/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.87718200683594
Batch 2222/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.68060302734375
  0%|          | 2222/1415250 [00:12<2:09:22, 182.04it/s, epoch=0, test_loss=183, train_loss=179]Batch 2223/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.9389190673828
Batch 2224/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.89724731445312
Batch 2225/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.19740295410156
Batch 2226/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.33099365234375
Batch 2227/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.98533630371094
Batch 2228/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.81326293945312
Batch 2229/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.36668395996094
Batch 2230/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.65696716308594
Batch 2231/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.6219940185547
Batch 2232/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.6321563720703
Batch 2233/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.23330688476562
Batch 2234/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.30966186523438
Batch 2235/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.7325439453125
Batch 2236/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.05838012695312
Batch 2237/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.74928283691406
Batch 2238/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.4567413330078
Batch 2239/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.7310028076172
Batch 2240/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.55079650878906
Batch 2241/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.00161743164062
  0%|          | 2241/1415250 [00:12<2:09:46, 181.46it/s, epoch=0, test_loss=183, train_loss=173]Batch 2242/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.1941375732422
Batch 2243/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.14671325683594
Batch 2244/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.51736450195312
Batch 2245/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.55650329589844
Batch 2246/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.57192993164062
Batch 2247/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.01307678222656
Batch 2248/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.69818115234375
Batch 2249/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.25071716308594
Batch 2250/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.83921813964844
Batch 2251/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.86280822753906
Batch 2252/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.48963928222656
Batch 2253/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.47959899902344
Batch 2254/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.63987731933594
Batch 2255/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.87802124023438
Batch 2256/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.3538360595703
Batch 2257/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.95623779296875
Batch 2258/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1545867919922
Batch 2259/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.69712829589844
Batch 2260/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.00503540039062
  0%|          | 2260/1415250 [00:12<2:09:48, 181.42it/s, epoch=0, test_loss=183, train_loss=173]Batch 2261/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.6571502685547
Batch 2262/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.46006774902344
Batch 2263/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.3721923828125
Batch 2264/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.38328552246094
Batch 2265/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.4918212890625
Batch 2266/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.0765838623047
Batch 2267/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.49989318847656
Batch 2268/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.95701599121094
Batch 2269/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.687255859375
Batch 2270/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.5856475830078
Batch 2271/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.8515625
Batch 2272/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.12255859375
Batch 2273/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.11215209960938
Batch 2274/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.8643798828125
Batch 2275/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.09466552734375
Batch 2276/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.8317108154297
Batch 2277/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.79310607910156
Batch 2278/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.78977966308594
Batch 2279/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.80340576171875
  0%|          | 2279/1415250 [00:12<2:10:53, 179.92it/s, epoch=0, test_loss=183, train_loss=188]Batch 2280/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.85206604003906
Batch 2281/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.0288543701172
Batch 2282/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.82135009765625
Batch 2283/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.14256286621094
Batch 2284/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.54922485351562
Batch 2285/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.3145751953125
Batch 2286/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.8887176513672
Batch 2287/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.17860412597656
Batch 2288/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.39761352539062
Batch 2289/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.50462341308594
Batch 2290/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.9246368408203
Batch 2291/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.21127319335938
Batch 2292/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.73263549804688
Batch 2293/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.1631317138672
Batch 2294/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.9435577392578
Batch 2295/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.0985107421875
Batch 2296/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.56422424316406
Batch 2297/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 198.25079345703125
Batch 2298/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.01663208007812
  0%|          | 2298/1415250 [00:12<2:10:40, 180.21it/s, epoch=0, test_loss=183, train_loss=184]Batch 2299/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.38417053222656
Batch 2300/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.47756958007812
Batch 2301/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.561767578125
Running validation...
Epoch 1, Step 2300: Train Loss = 169.9093475341797, Test Loss = 188.00540161132812
Batch 2302/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.56350708007812
Batch 2303/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.7561492919922
Batch 2304/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.42098999023438
Batch 2305/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.67604064941406
Batch 2306/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.6418914794922
Batch 2307/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.8563995361328
Batch 2308/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.6935272216797
Batch 2309/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.79949951171875
Batch 2310/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.07192993164062
Batch 2311/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.3284454345703
Batch 2312/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.82142639160156
Batch 2313/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 137.3313446044922
Batch 2314/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.20924377441406
Batch 2315/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.329345703125
Batch 2316/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.05308532714844
Batch 2317/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.52041625976562
  0%|          | 2317/1415250 [00:13<2:10:45, 180.09it/s, epoch=0, test_loss=188, train_loss=172]Batch 2318/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.2024383544922
Batch 2319/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.8180694580078
Batch 2320/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.96250915527344
Batch 2321/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.6315460205078
Batch 2322/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.17910766601562
Batch 2323/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.91973876953125
Batch 2324/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.62083435058594
Batch 2325/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.1063995361328
Batch 2326/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.36497497558594
Batch 2327/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.93380737304688
Batch 2328/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.38369750976562
Batch 2329/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.16114807128906
Batch 2330/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.31057739257812
Batch 2331/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2295684814453
Batch 2332/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.27247619628906
Batch 2333/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.79652404785156
Batch 2334/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.462158203125
Batch 2335/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.34747314453125
Batch 2336/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.11074829101562
  0%|          | 2336/1415250 [00:13<2:10:48, 180.03it/s, epoch=0, test_loss=188, train_loss=178]Batch 2337/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.9237823486328
Batch 2338/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.93182373046875
Batch 2339/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.94017028808594
Batch 2340/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.40065002441406
Batch 2341/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.24818420410156
Batch 2342/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.09532165527344
Batch 2343/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.57681274414062
Batch 2344/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.54443359375
Batch 2345/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.2239532470703
Batch 2346/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.5255584716797
Batch 2347/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.95037841796875
Batch 2348/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.63719177246094
Batch 2349/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.29466247558594
Batch 2350/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.1328125
Batch 2351/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.4644317626953
Batch 2352/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.38018798828125
Batch 2353/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.114501953125
Batch 2354/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.40292358398438
Batch 2355/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.73350524902344
  0%|          | 2355/1415250 [00:13<2:09:46, 181.44it/s, epoch=0, test_loss=188, train_loss=174]Batch 2356/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.8481903076172
Batch 2357/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.2974853515625
Batch 2358/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.04409790039062
Batch 2359/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.85401916503906
Batch 2360/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.848876953125
Batch 2361/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.32398986816406
Batch 2362/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.29359436035156
Batch 2363/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.0498809814453
Batch 2364/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.7387237548828
Batch 2365/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.49496459960938
Batch 2366/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.02969360351562
Batch 2367/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.23260498046875
Batch 2368/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.22491455078125
Batch 2369/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.4888458251953
Batch 2370/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.88211059570312
Batch 2371/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.6710968017578
Batch 2372/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.60166931152344
Batch 2373/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.2368927001953
Batch 2374/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7434844970703
  0%|          | 2374/1415250 [00:13<2:08:58, 182.58it/s, epoch=0, test_loss=188, train_loss=180]Batch 2375/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.13748168945312
Batch 2376/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.85037231445312
Batch 2377/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.67034912109375
Batch 2378/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.82305908203125
Batch 2379/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.16912841796875
Batch 2380/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.48619079589844
Batch 2381/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.59437561035156
Batch 2382/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.07656860351562
Batch 2383/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.9377899169922
Batch 2384/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.9809112548828
Batch 2385/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.6894073486328
Batch 2386/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.76327514648438
Batch 2387/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.11431884765625
Batch 2388/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.73257446289062
Batch 2389/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.4364013671875
Batch 2390/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.255126953125
Batch 2391/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.81439208984375
Batch 2392/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.8758087158203
Batch 2393/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.36485290527344
  0%|          | 2393/1415250 [00:13<2:09:59, 181.15it/s, epoch=0, test_loss=188, train_loss=169]Batch 2394/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.14846801757812
Batch 2395/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.40171813964844
Batch 2396/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.3263397216797
Batch 2397/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.8909149169922
Batch 2398/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.15225219726562
Batch 2399/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.72544860839844
Batch 2400/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.6688232421875
Batch 2401/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.95364379882812
Running validation...
Epoch 1, Step 2400: Train Loss = 176.54843139648438, Test Loss = 162.03179931640625
Batch 2402/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7889862060547
Batch 2403/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.85504150390625
Batch 2404/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.10260009765625
Batch 2405/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.58160400390625
Batch 2406/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.62960815429688
Batch 2407/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 199.62017822265625
Batch 2408/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.20066833496094
Batch 2409/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.9185791015625
Batch 2410/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.8615264892578
Batch 2411/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.68316650390625
Batch 2412/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.40658569335938
  0%|          | 2412/1415250 [00:13<2:10:19, 180.68it/s, epoch=0, test_loss=162, train_loss=173]Batch 2413/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.0974578857422
Batch 2414/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.97694396972656
Batch 2415/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.72201538085938
Batch 2416/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.21067810058594
Batch 2417/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1815185546875
Batch 2418/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.71591186523438
Batch 2419/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.09405517578125
Batch 2420/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.1063232421875
Batch 2421/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.07948303222656
Batch 2422/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1266632080078
Batch 2423/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.284423828125
Batch 2424/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.3160400390625
Batch 2425/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.18475341796875
Batch 2426/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.85682678222656
Batch 2427/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0372772216797
Batch 2428/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.28944396972656
Batch 2429/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 197.4019775390625
Batch 2430/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.1026611328125
Batch 2431/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.51043701171875
  0%|          | 2431/1415250 [00:13<2:09:08, 182.34it/s, epoch=0, test_loss=162, train_loss=176]Batch 2432/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.94500732421875
Batch 2433/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.62120056152344
Batch 2434/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.8979949951172
Batch 2435/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.34237670898438
Batch 2436/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.10299682617188
Batch 2437/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.8999786376953
Batch 2438/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.77891540527344
Batch 2439/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.67730712890625
Batch 2440/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.2197265625
Batch 2441/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.99874877929688
Batch 2442/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.99560546875
Batch 2443/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.78500366210938
Batch 2444/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.69252014160156
Batch 2445/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.12559509277344
Batch 2446/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.348876953125
Batch 2447/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.33798217773438
Batch 2448/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.84600830078125
Batch 2449/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.786376953125
Batch 2450/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.3995361328125
  0%|          | 2450/1415250 [00:13<2:09:16, 182.14it/s, epoch=0, test_loss=162, train_loss=166]Batch 2451/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.1991424560547
Batch 2452/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.57528686523438
Batch 2453/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.62399291992188
Batch 2454/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.0500030517578
Batch 2455/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.29440307617188
Batch 2456/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.34608459472656
Batch 2457/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.31317138671875
Batch 2458/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2707977294922
Batch 2459/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.74822998046875
Batch 2460/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.28897094726562
Batch 2461/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.92616271972656
Batch 2462/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.1763153076172
Batch 2463/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.1004180908203
Batch 2464/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1021728515625
Batch 2465/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.3383026123047
Batch 2466/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.10682678222656
Batch 2467/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.68174743652344
Batch 2468/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.2684783935547
Batch 2469/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.28146362304688
  0%|          | 2469/1415250 [00:13<2:08:50, 182.75it/s, epoch=0, test_loss=162, train_loss=161]Batch 2470/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.42613220214844
Batch 2471/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.0681915283203
Batch 2472/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.98858642578125
Batch 2473/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.6110076904297
Batch 2474/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.10498046875
Batch 2475/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.5443572998047
Batch 2476/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.42758178710938
Batch 2477/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.3375244140625
Batch 2478/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.23231506347656
Batch 2479/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.666748046875
Batch 2480/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.49461364746094
Batch 2481/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.8350830078125
Batch 2482/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.56060791015625
Batch 2483/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.0412139892578
Batch 2484/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.63833618164062
Batch 2485/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.322265625
Batch 2486/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.0791778564453
Batch 2487/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.8946533203125
Batch 2488/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.3890380859375
  0%|          | 2488/1415250 [00:13<2:09:32, 181.76it/s, epoch=0, test_loss=162, train_loss=172]Batch 2489/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.5703887939453
Batch 2490/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.8056640625
Batch 2491/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.50401306152344
Batch 2492/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.95034790039062
Batch 2493/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.40228271484375
Batch 2494/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.82627868652344
Batch 2495/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.33511352539062
Batch 2496/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.67234802246094
Batch 2497/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.49700927734375
Batch 2498/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.700439453125
Batch 2499/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.78927612304688
Batch 2500/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.33551025390625
Batch 2501/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.30564880371094
Running validation...
Epoch 1, Step 2500: Train Loss = 181.5572509765625, Test Loss = 171.3212127685547
Batch 2502/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.2899627685547
Batch 2503/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.06105041503906
Batch 2504/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.88714599609375
Batch 2505/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.16537475585938
Batch 2506/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.0332794189453
Batch 2507/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.58697509765625
  0%|          | 2507/1415250 [00:14<2:10:39, 180.21it/s, epoch=0, test_loss=171, train_loss=173]Batch 2508/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 194.05296325683594
Batch 2509/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.92807006835938
Batch 2510/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.34384155273438
Batch 2511/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.2741241455078
Batch 2512/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.1226348876953
Batch 2513/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.40322875976562
Batch 2514/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.2921600341797
Batch 2515/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.79083251953125
Batch 2516/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.6283416748047
Batch 2517/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.6404571533203
Batch 2518/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.28257751464844
Batch 2519/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.5326690673828
Batch 2520/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.5609130859375
Batch 2521/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.4200897216797
Batch 2522/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.03701782226562
Batch 2523/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.56492614746094
Batch 2524/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.03964233398438
Batch 2525/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 203.24594116210938
Batch 2526/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.9070587158203
  0%|          | 2526/1415250 [00:14<2:09:15, 182.15it/s, epoch=0, test_loss=171, train_loss=176]Batch 2527/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.33990478515625
Batch 2528/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.5845947265625
Batch 2529/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.99151611328125
Batch 2530/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.47125244140625
Batch 2531/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.6741485595703
Batch 2532/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.4861602783203
Batch 2533/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.16738891601562
Batch 2534/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.7610626220703
Batch 2535/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.27374267578125
Batch 2536/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.1908416748047
Batch 2537/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.15626525878906
Batch 2538/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.92198181152344
Batch 2539/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.30079650878906
Batch 2540/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.0429229736328
Batch 2541/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.0278778076172
Batch 2542/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.14175415039062
Batch 2543/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0149383544922
Batch 2544/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.8750457763672
Batch 2545/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.48919677734375
  0%|          | 2545/1415250 [00:14<2:09:02, 182.47it/s, epoch=0, test_loss=171, train_loss=166]Batch 2546/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.6579132080078
Batch 2547/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.0962371826172
Batch 2548/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.50730895996094
Batch 2549/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.27828979492188
Batch 2550/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.1478271484375
Starting epoch 2/15
Batch 1/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.49728393554688
Running validation...
Epoch 2, Step 0: Train Loss = 182.8531951904297, Test Loss = 173.0584716796875
Batch 2/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.97962951660156
Batch 3/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.5537109375
Batch 4/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.17190551757812
Batch 5/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.39939880371094
Batch 6/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.47560119628906
Batch 7/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.61746215820312
Batch 8/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.30267333984375
Batch 9/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.98997497558594
Batch 10/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.10650634765625
Batch 11/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.16001892089844
Batch 12/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.8911590576172
Batch 13/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.1044921875
Batch 14/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.0934600830078
  0%|          | 2564/1415250 [00:14<2:09:18, 182.07it/s, epoch=1, test_loss=173, train_loss=176]Batch 15/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.57632446289062
Batch 16/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.2732391357422
Batch 17/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.09112548828125
Batch 18/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 146.91424560546875
Batch 19/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.5105438232422
Batch 20/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.98011779785156
Batch 21/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.65419006347656
Batch 22/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.11572265625
Batch 23/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.4412078857422
Batch 24/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2657470703125
Batch 25/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.75523376464844
Batch 26/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.2322235107422
Batch 27/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.25253295898438
Batch 28/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.90179443359375
Batch 29/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 141.03298950195312
Batch 30/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.80490112304688
Batch 31/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.81451416015625
Batch 32/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.716064453125
Batch 33/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.2496795654297
  0%|          | 2583/1415250 [00:14<2:09:33, 181.72it/s, epoch=1, test_loss=173, train_loss=181]Batch 34/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.9204559326172
Batch 35/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.54949951171875
Batch 36/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.16856384277344
Batch 37/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.99676513671875
Batch 38/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.02584838867188
Batch 39/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.6151123046875
Batch 40/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.55154418945312
Batch 41/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.68531799316406
Batch 42/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.86788940429688
Batch 43/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.9193572998047
Batch 44/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.75421142578125
Batch 45/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.4714813232422
Batch 46/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.97393798828125
Batch 47/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.90478515625
Batch 48/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.59925842285156
Batch 49/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.46189880371094
Batch 50/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 190.88232421875
Batch 51/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.31179809570312
Batch 52/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.40359497070312
  0%|          | 2602/1415250 [00:14<2:09:05, 182.38it/s, epoch=1, test_loss=173, train_loss=169]Batch 53/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.08267211914062
Batch 54/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.75836181640625
Batch 55/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.6224365234375
Batch 56/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.86538696289062
Batch 57/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.14125061035156
Batch 58/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.4364471435547
Batch 59/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.57354736328125
Batch 60/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.76040649414062
Batch 61/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.59197998046875
Batch 62/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.64776611328125
Batch 63/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.0828857421875
Batch 64/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.20083618164062
Batch 65/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.98916625976562
Batch 66/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.6208953857422
Batch 67/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.5847625732422
Batch 68/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 204.2915802001953
Batch 69/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.1016845703125
Batch 70/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.4710235595703
Batch 71/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.1070556640625
  0%|          | 2621/1415250 [00:14<2:07:44, 184.31it/s, epoch=1, test_loss=173, train_loss=161]Batch 72/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.72097778320312
Batch 73/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.9802703857422
Batch 74/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.30960083007812
Batch 75/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1965789794922
Batch 76/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.38221740722656
Batch 77/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.86370849609375
Batch 78/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.8673553466797
Batch 79/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.5133819580078
Batch 80/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.1409912109375
Batch 81/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.25225830078125
Batch 82/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.6395263671875
Batch 83/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.43138122558594
Batch 84/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.78102111816406
Batch 85/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.0658721923828
Batch 86/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.03147888183594
Batch 87/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 191.37547302246094
Batch 88/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.06092834472656
Batch 89/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.11190795898438
Batch 90/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.6356658935547
  0%|          | 2640/1415250 [00:14<2:07:56, 184.02it/s, epoch=1, test_loss=173, train_loss=168]Batch 91/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.6092071533203
Batch 92/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.0447235107422
Batch 93/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.52699279785156
Batch 94/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.85989379882812
Batch 95/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.55555725097656
Batch 96/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.29403686523438
Batch 97/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.86428833007812
Batch 98/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.33758544921875
Batch 99/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.10293579101562
Batch 100/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.2353515625
Batch 101/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.5670928955078
Running validation...
Epoch 2, Step 100: Train Loss = 179.47943115234375, Test Loss = 179.7845916748047
Batch 102/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.104248046875
Batch 103/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.96377563476562
Batch 104/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.9186553955078
Batch 105/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.76304626464844
Batch 106/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.31173706054688
Batch 107/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.4992218017578
Batch 108/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.66864013671875
Batch 109/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.9700469970703
  0%|          | 2659/1415250 [00:14<2:08:37, 183.05it/s, epoch=1, test_loss=180, train_loss=157]Batch 110/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.56539916992188
Batch 111/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.4525604248047
Batch 112/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.11077880859375
Batch 113/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.6592254638672
Batch 114/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.77101135253906
Batch 115/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.6713104248047
Batch 116/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.26177978515625
Batch 117/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.86126708984375
Batch 118/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.80184936523438
Batch 119/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.17599487304688
Batch 120/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.35400390625
Batch 121/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.4793243408203
Batch 122/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.9310760498047
Batch 123/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.53897094726562
Batch 124/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.44046020507812
Batch 125/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.27862548828125
Batch 126/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.50511169433594
Batch 127/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.77511596679688
Batch 128/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.7880401611328
  0%|          | 2678/1415250 [00:15<2:08:02, 183.88it/s, epoch=1, test_loss=180, train_loss=181]Batch 129/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.29373168945312
Batch 130/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.31895446777344
Batch 131/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.313232421875
Batch 132/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 187.4130096435547
Batch 133/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.6551055908203
Batch 134/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.49501037597656
Batch 135/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.6854705810547
Batch 136/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.8977813720703
Batch 137/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.986572265625
Batch 138/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.7103271484375
Batch 139/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.43849182128906
Batch 140/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.4389190673828
Batch 141/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.07618713378906
Batch 142/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.55075073242188
Batch 143/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.8790283203125
Batch 144/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.9935302734375
Batch 145/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1798858642578
Batch 146/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.5107421875
Batch 147/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.2755126953125
  0%|          | 2697/1415250 [00:15<2:09:23, 181.95it/s, epoch=1, test_loss=180, train_loss=158]Batch 148/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.22862243652344
Batch 149/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.54971313476562
Batch 150/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.82508850097656
Batch 151/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.6760711669922
Batch 152/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.89857482910156
Batch 153/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.3006591796875
Batch 154/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.343017578125
Batch 155/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.49215698242188
Batch 156/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.86135864257812
Batch 157/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.52847290039062
Batch 158/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.13766479492188
Batch 159/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.75985717773438
Batch 160/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.03956604003906
Batch 161/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.1045379638672
Batch 162/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.63821411132812
Batch 163/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.11351013183594
Batch 164/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.18878173828125
Batch 165/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.91134643554688
Batch 166/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.02752685546875
  0%|          | 2716/1415250 [00:15<2:09:18, 182.07it/s, epoch=1, test_loss=180, train_loss=174]Batch 167/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.4752960205078
Batch 168/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.88623046875
Batch 169/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.7376708984375
Batch 170/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.45993041992188
Batch 171/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.7388153076172
Batch 172/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.47117614746094
Batch 173/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.6841278076172
Batch 174/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.34658813476562
Batch 175/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.13937377929688
Batch 176/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.16226196289062
Batch 177/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.1052703857422
Batch 178/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.45745849609375
Batch 179/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.30844116210938
Batch 180/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.02484130859375
Batch 181/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.9578857421875
Batch 182/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.2541961669922
Batch 183/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.7982940673828
Batch 184/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.18458557128906
Batch 185/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.29541015625
  0%|          | 2735/1415250 [00:15<2:08:41, 182.93it/s, epoch=1, test_loss=180, train_loss=169]Batch 186/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.30789184570312
Batch 187/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.85206604003906
Batch 188/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.56854248046875
Batch 189/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.339599609375
Batch 190/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.79937744140625
Batch 191/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.97756958007812
Batch 192/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2156524658203
Batch 193/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.69076538085938
Batch 194/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.76719665527344
Batch 195/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.12017822265625
Batch 196/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.1258087158203
Batch 197/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.4599609375
Batch 198/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 201.37149047851562
Batch 199/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.03614807128906
Batch 200/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.03463745117188
Batch 201/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.6758270263672
Running validation...
Epoch 2, Step 200: Train Loss = 175.4747314453125, Test Loss = 166.72488403320312
Batch 202/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.52236938476562
Batch 203/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.49815368652344
Batch 204/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.96424865722656
  0%|          | 2754/1415250 [00:15<2:08:55, 182.59it/s, epoch=1, test_loss=167, train_loss=163]Batch 205/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.590087890625
Batch 206/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.13973999023438
Batch 207/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.56536865234375
Batch 208/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.9232635498047
Batch 209/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.0536346435547
Batch 210/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.94520568847656
Batch 211/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.96243286132812
Batch 212/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.56338500976562
Batch 213/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.91165161132812
Batch 214/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0428924560547
Batch 215/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.57908630371094
Batch 216/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.75161743164062
Batch 217/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1942901611328
Batch 218/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.729248046875
Batch 219/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.80740356445312
Batch 220/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.41819763183594
Batch 221/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.3582305908203
Batch 222/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.58187866210938
Batch 223/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1919708251953
  0%|          | 2773/1415250 [00:15<2:08:13, 183.58it/s, epoch=1, test_loss=167, train_loss=163]Batch 224/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.57278442382812
Batch 225/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.2386474609375
Batch 226/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.22731018066406
Batch 227/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.24610900878906
Batch 228/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.97047424316406
Batch 229/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.73770141601562
Batch 230/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.45993041992188
Batch 231/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.1021728515625
Batch 232/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.50595092773438
Batch 233/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.61489868164062
Batch 234/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.3650360107422
Batch 235/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.38729858398438
Batch 236/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.05035400390625
Batch 237/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.77450561523438
Batch 238/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.02764892578125
Batch 239/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.67596435546875
Batch 240/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.33851623535156
Batch 241/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.61973571777344
Batch 242/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.25381469726562
  0%|          | 2792/1415250 [00:15<2:07:29, 184.65it/s, epoch=1, test_loss=167, train_loss=166]Batch 243/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.35836791992188
Batch 244/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.581787109375
Batch 245/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.57757568359375
Batch 246/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.60958862304688
Batch 247/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.31300354003906
Batch 248/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.15335083007812
Batch 249/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.91912841796875
Batch 250/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.3489227294922
Batch 251/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.80758666992188
Batch 252/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.150634765625
Batch 253/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.30105590820312
Batch 254/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.3784637451172
Batch 255/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.29127502441406
Batch 256/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.94715881347656
Batch 257/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.8903350830078
Batch 258/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 195.02920532226562
Batch 259/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.83029174804688
Batch 260/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.07012939453125
Batch 261/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.78897094726562
  0%|          | 2811/1415250 [00:15<2:07:38, 184.42it/s, epoch=1, test_loss=167, train_loss=178]Batch 262/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.78684997558594
Batch 263/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.56504821777344
Batch 264/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.90672302246094
Batch 265/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.14950561523438
Batch 266/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.22036743164062
Batch 267/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.98294067382812
Batch 268/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.71609497070312
Batch 269/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.0406951904297
Batch 270/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.2328643798828
Batch 271/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.6419677734375
Batch 272/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.71751403808594
Batch 273/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.3563995361328
Batch 274/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.4726104736328
Batch 275/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.53036499023438
Batch 276/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.13722229003906
Batch 277/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.73989868164062
Batch 278/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.61329650878906
Batch 279/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.80006408691406
Batch 280/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.06590270996094
  0%|          | 2830/1415250 [00:15<2:07:30, 184.62it/s, epoch=1, test_loss=167, train_loss=176]Batch 281/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.5232391357422
Batch 282/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.4708709716797
Batch 283/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.2831268310547
Batch 284/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.68740844726562
Batch 285/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.99365234375
Batch 286/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.07736206054688
Batch 287/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.3522186279297
Batch 288/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.7506103515625
Batch 289/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.160400390625
Batch 290/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.95785522460938
Batch 291/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.7074737548828
Batch 292/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.49986267089844
Batch 293/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.0552520751953
Batch 294/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.28421020507812
Batch 295/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.67352294921875
Batch 296/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.50381469726562
Batch 297/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 186.92906188964844
Batch 298/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.07078552246094
Batch 299/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 146.11549377441406
  0%|          | 2849/1415250 [00:15<2:07:21, 184.83it/s, epoch=1, test_loss=167, train_loss=146]Batch 300/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.23301696777344
Batch 301/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.2046661376953
Running validation...
Epoch 2, Step 300: Train Loss = 156.03753662109375, Test Loss = 170.5791778564453
Batch 302/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.2913360595703
Batch 303/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.5312957763672
Batch 304/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.7545166015625
Batch 305/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.72183227539062
Batch 306/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.74400329589844
Batch 307/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 140.98806762695312
Batch 308/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.9447021484375
Batch 309/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.1698760986328
Batch 310/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.0141143798828
Batch 311/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.90940856933594
Batch 312/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.3566436767578
Batch 313/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.92071533203125
Batch 314/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.27708435058594
Batch 315/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.69674682617188
Batch 316/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 138.97293090820312
Batch 317/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.7060546875
Batch 318/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.10426330566406
  0%|          | 2868/1415250 [00:16<2:08:34, 183.09it/s, epoch=1, test_loss=171, train_loss=170]Batch 319/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.99671936035156
Batch 320/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.79745483398438
Batch 321/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.20887756347656
Batch 322/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.29632568359375
Batch 323/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.1993408203125
Batch 324/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.97940063476562
Batch 325/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.90011596679688
Batch 326/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.50869750976562
Batch 327/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2709503173828
Batch 328/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 192.3892822265625
Batch 329/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.79714965820312
Batch 330/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.8291473388672
Batch 331/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.61102294921875
Batch 332/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.80870056152344
Batch 333/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.69403076171875
Batch 334/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.81005859375
Batch 335/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.06912231445312
Batch 336/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.88528442382812
Batch 337/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.42633056640625
  0%|          | 2887/1415250 [00:16<2:08:16, 183.50it/s, epoch=1, test_loss=171, train_loss=182]Batch 338/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.5186309814453
Batch 339/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.89736938476562
Batch 340/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.16873168945312
Batch 341/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.15017700195312
Batch 342/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.13424682617188
Batch 343/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.22630310058594
Batch 344/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.94119262695312
Batch 345/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.98883056640625
Batch 346/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.877685546875
Batch 347/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.7134246826172
Batch 348/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.98892211914062
Batch 349/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.7588348388672
Batch 350/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.2040252685547
Batch 351/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.8037109375
Batch 352/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.99789428710938
Batch 353/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 196.8146209716797
Batch 354/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.64466857910156
Batch 355/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.69822692871094
Batch 356/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.97918701171875
  0%|          | 2906/1415250 [00:16<2:07:50, 184.13it/s, epoch=1, test_loss=171, train_loss=182]Batch 357/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.2987060546875
Batch 358/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.09890747070312
Batch 359/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.32455444335938
Batch 360/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.66299438476562
Batch 361/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.06277465820312
Batch 362/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.14810180664062
Batch 363/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.5107879638672
Batch 364/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.80477905273438
Batch 365/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.69847106933594
Batch 366/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.2735595703125
Batch 367/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.15553283691406
Batch 368/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.91868591308594
Batch 369/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.56045532226562
Batch 370/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.09205627441406
Batch 371/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.38160705566406
Batch 372/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.4442138671875
Batch 373/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.6886749267578
Batch 374/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.96359252929688
Batch 375/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.0685272216797
  0%|          | 2925/1415250 [00:16<2:08:11, 183.63it/s, epoch=1, test_loss=171, train_loss=172]Batch 376/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.61935424804688
Batch 377/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.28152465820312
Batch 378/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.45436096191406
Batch 379/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.03225708007812
Batch 380/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.99427795410156
Batch 381/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.22471618652344
Batch 382/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.51373291015625
Batch 383/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.24839782714844
Batch 384/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.73167419433594
Batch 385/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.2970733642578
Batch 386/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.11175537109375
Batch 387/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.62661743164062
Batch 388/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.6265869140625
Batch 389/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.91876220703125
Batch 390/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.87545776367188
Batch 391/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.45884704589844
Batch 392/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 200.48313903808594
Batch 393/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.98179626464844
Batch 394/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.26226806640625
  0%|          | 2944/1415250 [00:16<2:07:23, 184.77it/s, epoch=1, test_loss=171, train_loss=160]Batch 395/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.25914001464844
Batch 396/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.94866943359375
Batch 397/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.80062866210938
Batch 398/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.23658752441406
Batch 399/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.36524963378906
Batch 400/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.64471435546875
Batch 401/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.1818084716797
Running validation...
Epoch 2, Step 400: Train Loss = 166.78245544433594, Test Loss = 175.54441833496094
Batch 402/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.41171264648438
Batch 403/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.25343322753906
Batch 404/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.16104125976562
Batch 405/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.03488159179688
Batch 406/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.23057556152344
Batch 407/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.80299377441406
Batch 408/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.4982452392578
Batch 409/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.34205627441406
Batch 410/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.90869140625
Batch 411/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.60101318359375
Batch 412/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.25721740722656
Batch 413/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.67022705078125
  0%|          | 2963/1415250 [00:16<2:08:17, 183.48it/s, epoch=1, test_loss=176, train_loss=171]Batch 414/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.2695770263672
Batch 415/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.1912841796875
Batch 416/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.87039184570312
Batch 417/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 193.70314025878906
Batch 418/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.86781311035156
Batch 419/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.83056640625
Batch 420/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.72909545898438
Batch 421/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.95816040039062
Batch 422/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.30165100097656
Batch 423/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.8960723876953
Batch 424/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.16546630859375
Batch 425/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.5153350830078
Batch 426/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.33921813964844
Batch 427/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.46510314941406
Batch 428/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.60577392578125
Batch 429/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.6234130859375
Batch 430/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.06837463378906
Batch 431/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.08505249023438
Batch 432/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.00204467773438
  0%|          | 2982/1415250 [00:16<2:07:50, 184.12it/s, epoch=1, test_loss=176, train_loss=169]Batch 433/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.34475708007812
Batch 434/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.55380249023438
Batch 435/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.95205688476562
Batch 436/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.06007385253906
Batch 437/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.4193572998047
Batch 438/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.1604766845703
Batch 439/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.78550720214844
Batch 440/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.48171997070312
Batch 441/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.0321044921875
Batch 442/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.62930297851562
Batch 443/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.63385009765625
Batch 444/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.96841430664062
Batch 445/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.29217529296875
Batch 446/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.42965698242188
Batch 447/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.38668823242188
Batch 448/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.14532470703125
Batch 449/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.01165771484375
Batch 450/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.2821502685547
Batch 451/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.00668334960938
  0%|          | 3001/1415250 [00:16<2:07:39, 184.39it/s, epoch=1, test_loss=176, train_loss=177]Batch 452/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.9326171875
Batch 453/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.11239624023438
Batch 454/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.52830505371094
Batch 455/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.3334197998047
Batch 456/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.22288513183594
Batch 457/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.11717224121094
Batch 458/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.8826904296875
Batch 459/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.0753936767578
Batch 460/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1196746826172
Batch 461/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.84739685058594
Batch 462/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.4875946044922
Batch 463/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.7306365966797
Batch 464/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.36776733398438
Batch 465/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.47061157226562
Batch 466/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.59056091308594
Batch 467/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.96463012695312
Batch 468/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.51885986328125
Batch 469/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.67491149902344
Batch 470/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.0982666015625
  0%|          | 3020/1415250 [00:16<2:06:59, 185.34it/s, epoch=1, test_loss=176, train_loss=171]Batch 471/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.72750854492188
Batch 472/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.37875366210938
Batch 473/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.22747802734375
Batch 474/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.058837890625
Batch 475/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.688232421875
Batch 476/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.83294677734375
Batch 477/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.91851806640625
Batch 478/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 140.7648162841797
Batch 479/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 145.87081909179688
Batch 480/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.96559143066406
Batch 481/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.1314239501953
Batch 482/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.08531188964844
Batch 483/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.60218811035156
Batch 484/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.3351287841797
Batch 485/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.37718200683594
Batch 486/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.3048095703125
Batch 487/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.90928649902344
Batch 488/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.61900329589844
Batch 489/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.1494903564453
  0%|          | 3039/1415250 [00:16<2:06:50, 185.55it/s, epoch=1, test_loss=176, train_loss=167]Batch 490/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.13406372070312
Batch 491/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.31529235839844
Batch 492/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.74142456054688
Batch 493/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.71563720703125
Batch 494/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.0377960205078
Batch 495/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.061767578125
Batch 496/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.03677368164062
Batch 497/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.3164520263672
Batch 498/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.4040069580078
Batch 499/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.6294403076172
Batch 500/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.61282348632812
Batch 501/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2128143310547
Running validation...
Epoch 2, Step 500: Train Loss = 172.22418212890625, Test Loss = 170.58877563476562
Batch 502/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.39747619628906
Batch 503/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.74151611328125
Batch 504/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.47918701171875
Batch 505/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.19189453125
Batch 506/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.01425170898438
Batch 507/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.61114501953125
Batch 508/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.32089233398438
  0%|          | 3058/1415250 [00:17<2:08:50, 182.67it/s, epoch=1, test_loss=171, train_loss=171]Batch 509/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.93545532226562
Batch 510/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.2759552001953
Batch 511/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.33416748046875
Batch 512/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.9596710205078
Batch 513/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 142.86614990234375
Batch 514/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 141.159912109375
Batch 515/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.49319458007812
Batch 516/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.3289031982422
Batch 517/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.20755004882812
Batch 518/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.3674774169922
Batch 519/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.81275939941406
Batch 520/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.2728271484375
Batch 521/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 142.86978149414062
Batch 522/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.68051147460938
Batch 523/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.78350830078125
Batch 524/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.69667053222656
Batch 525/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.7835693359375
Batch 526/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.10760498046875
Batch 527/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.7130584716797
  0%|          | 3077/1415250 [00:17<2:11:09, 179.45it/s, epoch=1, test_loss=171, train_loss=160]Batch 528/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.49600219726562
Batch 529/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.78262329101562
Batch 530/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.4995880126953
Batch 531/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.28225708007812
Batch 532/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.41001892089844
Batch 533/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 145.02197265625
Batch 534/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1409912109375
Batch 535/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.8429412841797
Batch 536/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.3029022216797
Batch 537/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.8253173828125
Batch 538/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.39186096191406
Batch 539/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.8841094970703
Batch 540/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.8857879638672
Batch 541/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.39248657226562
Batch 542/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.34588623046875
Batch 543/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.0083770751953
Batch 544/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.68612670898438
Batch 545/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.73316955566406
  0%|          | 3095/1415250 [00:17<2:12:25, 177.73it/s, epoch=1, test_loss=171, train_loss=160]Batch 546/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.61997985839844
Batch 547/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.5339813232422
Batch 548/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.06640625
Batch 549/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.524169921875
Batch 550/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.89500427246094
Batch 551/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.5206298828125
Batch 552/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.91244506835938
Batch 553/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.87017822265625
Batch 554/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 184.63131713867188
Batch 555/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.0769805908203
Batch 556/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.63278198242188
Batch 557/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.11668395996094
Batch 558/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.81362915039062
Batch 559/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.97914123535156
Batch 560/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.665283203125
Batch 561/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.91928100585938
Batch 562/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.0729522705078
Batch 563/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.64820861816406
  0%|          | 3113/1415250 [00:17<2:14:35, 174.87it/s, epoch=1, test_loss=171, train_loss=170]Batch 564/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.865966796875
Batch 565/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.6539764404297
Batch 566/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.81236267089844
Batch 567/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.62806701660156
Batch 568/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.2850341796875
Batch 569/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.69927978515625
Batch 570/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 185.47634887695312
Batch 571/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.60630798339844
Batch 572/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.6241455078125
Batch 573/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.76687622070312
Batch 574/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.95875549316406
Batch 575/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.59869384765625
Batch 576/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.86477661132812
Batch 577/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.284423828125
Batch 578/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.29751586914062
Batch 579/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.62759399414062
Batch 580/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.83978271484375
Batch 581/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.15377807617188
  0%|          | 3131/1415250 [00:17<2:14:09, 175.43it/s, epoch=1, test_loss=171, train_loss=161]Batch 582/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 145.844482421875
Batch 583/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.8799285888672
Batch 584/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.1761932373047
Batch 585/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.08641052246094
Batch 586/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.57550048828125
Batch 587/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.7732391357422
Batch 588/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 144.46487426757812
Batch 589/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.61553955078125
Batch 590/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.89019775390625
Batch 591/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 146.5304412841797
Batch 592/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.0749053955078
Batch 593/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.2196502685547
Batch 594/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.5985870361328
Batch 595/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.61532592773438
Batch 596/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.8359375
Batch 597/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.556640625
Batch 598/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.19729614257812
Batch 599/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.03639221191406
  0%|          | 3149/1415250 [00:17<2:14:24, 175.11it/s, epoch=1, test_loss=171, train_loss=162]Batch 600/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.79612731933594
Batch 601/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.77099609375
Running validation...
Epoch 2, Step 600: Train Loss = 174.3000946044922, Test Loss = 170.07574462890625
Batch 602/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.1767578125
Batch 603/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.23681640625
Batch 604/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.89605712890625
Batch 605/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.06185913085938
Batch 606/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.29483032226562
Batch 607/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.5740509033203
Batch 608/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.5568084716797
Batch 609/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.15757751464844
Batch 610/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.03475952148438
Batch 611/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.46942138671875
Batch 612/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.39801025390625
Batch 613/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.4505615234375
Batch 614/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.93141174316406
Batch 615/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.32398986816406
Batch 616/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.694580078125
Batch 617/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.28118896484375
  0%|          | 3167/1415250 [00:17<2:15:51, 173.22it/s, epoch=1, test_loss=170, train_loss=167]Batch 618/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.25096130371094
Batch 619/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.37167358398438
Batch 620/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.6565704345703
Batch 621/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.06529235839844
Batch 622/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.48263549804688
Batch 623/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.6545867919922
Batch 624/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.72727966308594
Batch 625/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 145.05897521972656
Batch 626/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.06724548339844
Batch 627/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.3849334716797
Batch 628/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.5126953125
Batch 629/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.11593627929688
Batch 630/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.1950225830078
Batch 631/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.93777465820312
Batch 632/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.07334899902344
Batch 633/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.90115356445312
Batch 634/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.23794555664062
Batch 635/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.56753540039062
  0%|          | 3185/1415250 [00:17<2:16:36, 172.29it/s, epoch=1, test_loss=170, train_loss=161]Batch 636/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.0497283935547
Batch 637/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.72158813476562
Batch 638/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.5296173095703
Batch 639/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.10986328125
Batch 640/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.8034210205078
Batch 641/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.49948120117188
Batch 642/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.58309936523438
Batch 643/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.45213317871094
Batch 644/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.7825927734375
Batch 645/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.7987518310547
Batch 646/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.56622314453125
Batch 647/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.1829833984375
Batch 648/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 188.5585174560547
Batch 649/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.71463012695312
Batch 650/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.0252685546875
Batch 651/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.90269470214844
Batch 652/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.2530975341797
Batch 653/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.4358673095703
  0%|          | 3203/1415250 [00:17<2:14:57, 174.38it/s, epoch=1, test_loss=170, train_loss=157]Batch 654/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 173.44308471679688
Batch 655/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.98504638671875
Batch 656/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.28848266601562
Batch 657/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.3775177001953
Batch 658/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.7345733642578
Batch 659/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.01840209960938
Batch 660/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.8532257080078
Batch 661/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.32919311523438
Batch 662/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 189.72483825683594
Batch 663/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.23348999023438
Batch 664/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.4269561767578
Batch 665/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.72409057617188
Batch 666/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.5757598876953
Batch 667/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.1986541748047
Batch 668/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.7967529296875
Batch 669/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.79461669921875
Batch 670/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.8899383544922
Batch 671/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.9930877685547
  0%|          | 3221/1415250 [00:18<2:15:52, 173.19it/s, epoch=1, test_loss=170, train_loss=144]Batch 672/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.0064239501953
Batch 673/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.41888427734375
Batch 674/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.91664123535156
Batch 675/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.16001892089844
Batch 676/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.35311889648438
Batch 677/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 142.14431762695312
Batch 678/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 180.24034118652344
Batch 679/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.8019561767578
Batch 680/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.28187561035156
Batch 681/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.1494598388672
Batch 682/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.67727661132812
Batch 683/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.8994140625
Batch 684/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.91067504882812
Batch 685/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.93264770507812
Batch 686/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 182.96031188964844
Batch 687/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.82083129882812
Batch 688/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.83192443847656
Batch 689/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.46263122558594
  0%|          | 3239/1415250 [00:18<2:15:29, 173.70it/s, epoch=1, test_loss=170, train_loss=171]Batch 690/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.3539581298828
Batch 691/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.51339721679688
Batch 692/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.52255249023438
Batch 693/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.8961181640625
Batch 694/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.17538452148438
Batch 695/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.73098754882812
Batch 696/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.3647003173828
Batch 697/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 147.34341430664062
Batch 698/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.37936401367188
Batch 699/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.7480926513672
Batch 700/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.85484313964844
Batch 701/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.8072052001953
Running validation...
Epoch 2, Step 700: Train Loss = 163.62887573242188, Test Loss = 147.04080200195312
Batch 702/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.8986358642578
Batch 703/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.7712860107422
Batch 704/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.04779052734375
Batch 705/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.52447509765625
Batch 706/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.68470764160156
Batch 707/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.2672576904297
  0%|          | 3257/1415250 [00:18<2:22:27, 165.19it/s, epoch=1, test_loss=147, train_loss=158]Batch 708/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.3843536376953
Batch 709/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.5125274658203
Batch 710/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 141.79319763183594
Batch 711/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.5906982421875
Batch 712/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.97633361816406
Batch 713/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.83631896972656
Batch 714/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.7510986328125
Batch 715/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 183.36180114746094
Batch 716/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.18548583984375
Batch 717/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.5862274169922
Batch 718/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.1985321044922
Batch 719/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.43930053710938
Batch 720/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 181.90602111816406
Batch 721/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.76644897460938
Batch 722/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.98219299316406
Batch 723/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1288299560547
Batch 724/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.20338439941406
Batch 725/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.17523193359375
  0%|          | 3275/1415250 [00:18<2:20:25, 167.58it/s, epoch=1, test_loss=147, train_loss=169]Batch 726/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.9207763671875
Batch 727/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.94009399414062
Batch 728/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.44349670410156
Batch 729/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.01756286621094
Batch 730/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.00843811035156
Batch 731/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.77035522460938
Batch 732/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2023468017578
Batch 733/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.28176879882812
Batch 734/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.46868896484375
Batch 735/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 144.17074584960938
Batch 736/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.74122619628906
Batch 737/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.76754760742188
Batch 738/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.03897094726562
Batch 739/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.50338745117188
Batch 740/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.27452087402344
Batch 741/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.74253845214844
Batch 742/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.65696716308594
Batch 743/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 145.25701904296875
Batch 744/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.49261474609375
  0%|          | 3294/1415250 [00:18<2:17:07, 171.62it/s, epoch=1, test_loss=147, train_loss=159]Batch 745/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 142.64541625976562
Batch 746/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.19186401367188
Batch 747/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.914794921875
Batch 748/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.70867919921875
Batch 749/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.92230224609375
Batch 750/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.51400756835938
Batch 751/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.69430541992188
Batch 752/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.99200439453125
Batch 753/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.805419921875
Batch 754/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.08143615722656
Batch 755/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.7958221435547
Batch 756/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.0765380859375
Batch 757/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.90740966796875
Batch 758/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.78121948242188
Batch 759/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.1517333984375
Batch 760/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.0643310546875
Batch 761/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.50157165527344
Batch 762/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.80859375
  0%|          | 3312/1415250 [00:18<2:15:17, 173.93it/s, epoch=1, test_loss=147, train_loss=157]Batch 763/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.21112060546875
Batch 764/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.4104766845703
Batch 765/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.3306884765625
Batch 766/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.2752685546875
Batch 767/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.03244018554688
Batch 768/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.75347900390625
Batch 769/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 142.5972900390625
Batch 770/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.20310974121094
Batch 771/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.823486328125
Batch 772/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.4841766357422
Batch 773/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.9105987548828
Batch 774/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.12164306640625
Batch 775/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.1004180908203
Batch 776/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.1530303955078
Batch 777/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.56597900390625
Batch 778/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.42987060546875
Batch 779/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.5237274169922
Batch 780/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 161.02703857421875
  0%|          | 3330/1415250 [00:18<2:17:51, 170.70it/s, epoch=1, test_loss=147, train_loss=161]Batch 781/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.34799194335938
Batch 782/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.82733154296875
Batch 783/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.6133270263672
Batch 784/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.39488220214844
Batch 785/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.69322204589844
Batch 786/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.77410888671875
Batch 787/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.620849609375
Batch 788/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 178.4398193359375
Batch 789/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.7738800048828
Batch 790/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 163.2815399169922
Batch 791/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.56239318847656
Batch 792/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 144.9567108154297
Batch 793/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 158.9761199951172
Batch 794/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.457275390625
Batch 795/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 152.12718200683594
Batch 796/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.4036407470703
Batch 797/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.2809295654297
Batch 798/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.8139190673828
  0%|          | 3348/1415250 [00:18<2:17:30, 171.13it/s, epoch=1, test_loss=147, train_loss=173]Batch 799/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.2755889892578
Batch 800/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.9759521484375
Batch 801/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.3563690185547
Running validation...
Epoch 2, Step 800: Train Loss = 154.7760009765625, Test Loss = 149.9202880859375
Batch 802/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.1988525390625
Batch 803/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 177.387939453125
Batch 804/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.8498992919922
Batch 805/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.6309051513672
Batch 806/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.75828552246094
Batch 807/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 148.5152130126953
Batch 808/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.57235717773438
Batch 809/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 141.7359161376953
Batch 810/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.14564514160156
Batch 811/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 169.91192626953125
Batch 812/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.5155792236328
Batch 813/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 175.5652618408203
Batch 814/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 176.28961181640625
Batch 815/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 165.48748779296875
Batch 816/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.58921813964844
  0%|          | 3366/1415250 [00:18<2:16:56, 171.84it/s, epoch=1, test_loss=150, train_loss=163]Batch 817/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 166.656005859375
Batch 818/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.82545471191406
Batch 819/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.02243041992188
Batch 820/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 179.0631866455078
Batch 821/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.39279174804688
Batch 822/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 151.34902954101562
Batch 823/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 136.4056396484375
Batch 824/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.7068328857422
Batch 825/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 155.73410034179688
Batch 826/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.62184143066406
Batch 827/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 167.97792053222656
Batch 828/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.3167724609375
Batch 829/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 171.5611114501953
Batch 830/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.3626708984375
Batch 831/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.45484924316406
Batch 832/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.9847869873047
Batch 833/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.31675720214844
Batch 834/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 133.83428955078125
  0%|          | 3384/1415250 [00:18<2:15:16, 173.96it/s, epoch=1, test_loss=150, train_loss=134]Batch 835/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 164.69589233398438
Batch 836/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.7015838623047
Batch 837/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.35687255859375
Batch 838/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 159.006591796875
Batch 839/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 153.7703094482422
Batch 840/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 156.14198303222656
Batch 841/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 143.39968872070312
Batch 842/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.3179473876953
Batch 843/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 150.1910400390625
Batch 844/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 157.3727264404297
Batch 845/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 162.12196350097656
Batch 846/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 149.68988037109375
Batch 847/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 168.76971435546875
Batch 848/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 174.84715270996094
Batch 849/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 160.92794799804688
Batch 850/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 133.65243530273438
Batch 851/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 170.41859436035156
Batch 852/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 154.96156311035156
  0%|          | 3402/1415250 [00:19<2:15:34, 173.56it/s, epoch=1, test_loss=150, train_loss=155]Batch 853/2550
Batch data shape: (20, 28, 28, 1)
Train Loss: 172.629638671875
Batch 854/2550
Batch data shape: (20, 28, 28, 1)
