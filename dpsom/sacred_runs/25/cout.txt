INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "25"
[ 0.          0.         -1.          0.          0.          0.66666667
  0.5         0.          0.          0.33333333  0.5         0.
  0.          0.16666667  0.          0.         -1.          0.
  0.16666667  0.66666667  0.5        -1.          0.16666667 -1.
  0.5         0.         -1.         -1.          0.16666667  0.
 -1.          0.5        -1.          0.          0.16666667 -1.
  0.16666667  0.5         0.16666667  0.          0.66666667  0.83333333
 -1.          0.5         0.         -1.         -1.         -1.
  0.          0.16666667  0.         -1.          0.         -1.
  0.          0.5         0.66666667  0.16666667  0.          0.83333333
  0.16666667  0.16666667 -1.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.          0.          0.5
 -1.          0.5        -1.          0.          0.5         0.16666667
  0.          0.16666667  0.5         0.5         0.16666667  0.
  0.         -1.         -1.         -1.         -1.          0.66666667
  0.          0.         -1.          0.5        -1.          0.16666667
 -1.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.         -1.          0.5         0.5         0.
  0.16666667  0.          0.16666667 -1.         -1.          0.16666667
  0.16666667 -1.          0.          0.         -1.         -1.
  0.          0.          0.          0.5         0.          0.16666667
 -1.         -1.          0.          0.5         0.         -1.
  0.         -1.          0.          0.         -1.          0.
  0.16666667 -1.          0.5        -1.          0.          0.16666667
 -1.          0.16666667  0.66666667 -1.          0.          0.
  0.16666667 -1.         -1.          0.5         0.          0.16666667
  0.          0.         -1.         -1.          0.5         0.
 -1.          0.5         0.16666667  0.          0.         -1.
  0.          0.16666667  0.83333333  0.16666667  0.16666667 -1.
  0.66666667  0.          0.          0.          0.          0.16666667
  0.          0.5         0.          0.          0.          0.
  0.5         0.          0.66666667  0.5        -1.          0.
  0.          0.66666667  0.          0.         -1.         -1.
  0.          0.         -1.          0.         -1.          0.
 -1.          0.5        -1.          0.          0.16666667 -1.
 -1.          0.83333333  0.          0.          0.83333333  0.
  0.16666667  0.83333333 -1.          0.5         0.          0.16666667
  0.83333333  0.         -1.          0.66666667  0.16666667  0.
  0.          0.          0.          0.16666667  0.5         0.66666667
  0.         -1.          0.          0.          0.16666667  0.16666667
  0.          0.          0.         -1.          0.          0.5
  0.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.83333333 -1.          0.16666667  0.          0.         -1.
  0.          0.          0.16666667  0.          0.          0.66666667
 -1.          0.          0.         -1.         -1.         -1.
 -1.          0.          0.         -1.          0.          0.16666667
  0.          0.         -1.          0.66666667 -1.          0.16666667
  0.         -1.          0.16666667  0.          0.5         0.66666667
 -1.          0.          0.16666667 -1.          0.          0.16666667
  0.          0.16666667  0.66666667  0.          0.          0.
  0.16666667 -1.         -1.          0.33333333  0.         -1.
 -1.          0.16666667  0.66666667 -1.          0.66666667  0.
  0.         -1.          0.         -1.          0.          0.
  0.          0.          0.         -1.          0.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  0.          0.         -1.         -1.
  0.5         0.16666667 -1.          0.          0.          0.
  0.16666667  0.          0.          0.5        -1.          0.
 -1.          0.         -1.          0.16666667 -1.         -1.
  0.16666667  0.5         0.          0.          0.5         0.16666667
  0.5         0.16666667  0.66666667 -1.          0.         -1.
  0.16666667  0.66666667  0.66666667  0.16666667 -1.          0.66666667
  0.16666667  0.         -1.          0.         -1.         -1.
  0.          0.16666667  0.          0.          0.66666667  0.66666667
  0.          0.16666667 -1.          0.16666667  0.          0.
  0.5         0.          0.66666667  0.          0.16666667  0.5
  0.16666667  0.5         0.16666667  0.         -1.          0.
  0.16666667  0.16666667  0.16666667  0.16666667  0.          0.66666667
 -1.          0.5         0.66666667  0.          0.          0.66666667
 -1.         -1.          0.          0.5        -1.          0.
  0.          0.16666667  0.          0.          0.5         0.66666667
  0.5         0.          0.          0.16666667  0.          0.
  0.16666667  0.          0.16666667  0.16666667  0.         -1.
  0.66666667  0.16666667 -1.          0.66666667  0.          0.
  0.5         0.         -1.          0.16666667  0.16666667 -1.
  0.66666667  0.          0.16666667  0.          0.          0.
  0.         -1.          0.          0.16666667  0.66666667  0.
  0.          0.5         0.         -1.          0.          0.16666667
  0.          0.16666667 -1.         -1.          0.16666667  0.66666667
  0.         -1.         -1.          0.         -1.          0.16666667
 -1.          0.16666667  0.         -1.          0.16666667  0.5
 -1.         -1.          0.16666667 -1.          0.16666667 -1.
 -1.          0.          0.          0.16666667  0.          0.
  0.          0.16666667  0.          0.33333333  0.          0.
  0.33333333  0.5         0.          0.16666667  0.66666667 -1.
  0.16666667  0.          0.5         0.16666667 -1.         -1.
  0.         -1.          0.          0.16666667  0.          0.16666667
  0.          0.16666667  0.5         0.16666667 -1.          0.
  0.          0.16666667  0.16666667  0.         -1.          0.5
  0.          0.16666667  0.          0.5         0.16666667  0.
  0.5         0.          0.          0.66666667 -1.         -1.
 -1.          0.          0.5         0.16666667  0.          0.
  0.          0.5        -1.         -1.          0.          0.
 -1.          0.16666667  0.          0.66666667 -1.         -1.
  0.16666667  0.         -1.          0.5        -1.          0.16666667
  0.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          0.          0.5         0.66666667
  0.          0.5         0.         -1.          0.         -1.
  0.16666667  0.66666667 -1.         -1.          0.          0.5
 -1.          0.5         0.         -1.          0.         -1.
 -1.          0.          0.          0.          0.16666667 -1.
 -1.          0.          0.          0.66666667  0.         -1.
 -1.         -1.          0.         -1.          0.16666667  0.
 -1.          0.16666667  0.66666667  0.          0.66666667 -1.
  0.16666667  0.16666667  0.5         0.          0.66666667  0.
 -1.          0.16666667 -1.          0.         -1.          0.5
  0.5         0.16666667  0.16666667 -1.          0.66666667  0.16666667
  0.83333333  0.16666667  0.          0.83333333  0.          0.16666667
  0.          0.66666667 -1.          0.16666667  0.          0.
  0.66666667 -1.         -1.          0.66666667  0.16666667  0.83333333
 -1.          1.         -1.         -1.          0.         -1.
  0.          0.16666667  0.          0.         -1.          0.5
  0.         -1.          0.16666667 -1.          0.16666667  0.5
 -1.          0.16666667  0.          0.          0.5         0.5
 -1.          0.          0.          0.16666667  0.66666667  0.16666667
  0.          0.          0.         -1.         -1.         -1.
  0.16666667 -1.          0.          0.         -1.          0.
  0.66666667  0.          0.          0.         -1.          0.
  0.16666667  0.16666667 -1.         -1.         -1.          0.66666667
  0.5        -1.          0.          0.          0.66666667  0.16666667
  0.          0.         -1.          0.16666667 -1.          0.
 -1.          0.16666667 -1.          0.         -1.         -1.
  0.          0.16666667 -1.          0.         -1.          0.16666667
  0.          0.5         0.          0.33333333  0.16666667  0.5
 -1.         -1.          0.          0.5        -1.          0.33333333
  0.         -1.          0.          0.16666667 -1.          0.
  0.16666667  0.          0.5         0.16666667 -1.          0.
  0.16666667  0.          0.         -1.         -1.         -1.
  0.          0.5         0.16666667 -1.          0.          0.5
  0.16666667  0.          0.5         0.16666667  0.5        -1.
 -1.         -1.          0.          0.16666667 -1.          0.16666667
  0.          0.16666667  0.5         0.16666667  0.5         0.
 -1.         -1.          0.5         0.66666667  0.16666667  0.
  0.5         0.5        -1.          0.         -1.          0.
  0.5         0.5         0.16666667  0.16666667  0.          0.
 -1.          0.          0.5        -1.          0.66666667  0.
  0.16666667  0.16666667  0.          0.          0.16666667  0.66666667
  0.16666667  0.16666667  0.66666667  0.          0.33333333  0.16666667
  0.          0.16666667  0.5        -1.          0.5         0.
  0.16666667 -1.          0.5         0.66666667  0.         -1.
  0.66666667  0.5        -1.         -1.          0.83333333  0.16666667
  0.16666667  0.          0.          0.66666667  0.          0.16666667
  0.5         0.          0.66666667 -1.         -1.         -1.
  1.          0.          0.          0.          0.          0.
  0.         -1.          0.         -1.         -1.          0.
  0.16666667  0.          0.16666667  0.          0.66666667  0.5
  0.          0.          0.          0.         -1.          0.16666667
  0.          0.16666667 -1.         -1.          0.         -1.
  0.5         0.         -1.          0.         -1.          0.
  0.          0.          0.          0.          0.         -1.
  0.         -1.         -1.          0.16666667  0.         -1.
  0.          0.16666667 -1.         -1.          0.66666667  0.16666667
 -1.          0.          0.16666667  0.         -1.          0.
  0.          0.5         0.16666667  0.          0.16666667  0.66666667
 -1.          0.16666667 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-18 15:23:25.102913: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-18_34329********* 

  0%|          | 0/139870 [00:00<?, ?it/s]Number of batches: 394


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 548.3773193359375
Running validation...
Epoch 1, Step 0: Train Loss = 545.94580078125, Test Loss = 546.6102294921875
  0%|          | 1/139870 [00:00<8:23:05,  4.63it/s, epoch=0, test_loss=547, train_loss=546]Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 549.3345336914062
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.3042602539062
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 547.0756225585938
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.9313354492188
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.5775756835938
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.6971435546875
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.5215454101562
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.8601684570312
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.0032348632812
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.6666870117188
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.28076171875
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.935791015625
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.7933959960938
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.1084594726562
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.2420043945312
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.1719360351562
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.2096557617188
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.95947265625
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.2528076171875
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.0164184570312
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.4994506835938
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.7886962890625
  0%|          | 23/139870 [00:00<25:50, 90.18it/s, epoch=0, test_loss=547, train_loss=544] Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.5299072265625
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.3499755859375
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.3115844726562
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.1783447265625
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.1556396484375
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.6476440429688
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.7141723632812
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.7537231445312
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.411865234375
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.2926635742188
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.6236572265625
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.0552368164062
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.7182006835938
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 540.6989135742188
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.6654052734375
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.7941284179688
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.068603515625
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.8186645507812
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 540.33154296875
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.60888671875
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.8644409179688
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.3702392578125
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.394287109375
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 536.491943359375
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.1123657226562
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 531.3297119140625
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 537.6138305664062
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.884521484375
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 537.0780639648438
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 535.0346069335938
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 528.6997680664062
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 535.184326171875
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 530.8784790039062
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 533.186279296875
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 518.8384399414062
  0%|          | 58/139870 [00:00<12:42, 183.46it/s, epoch=0, test_loss=547, train_loss=519]Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 524.0350341796875
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 521.9143676757812
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 519.028564453125
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 525.4622802734375
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 516.419189453125
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 523.531494140625
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 525.3447875976562
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 522.9828491210938
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 512.2506713867188
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 502.2434997558594
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 515.5979614257812
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 504.6998596191406
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 465.6538391113281
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 460.63568115234375
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 479.0286560058594
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 430.41845703125
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 427.54443359375
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 197.2745361328125
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 369.62786865234375
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 300.7696533203125
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 163.0715789794922
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 54.76547622680664
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 265.0
  0%|          | 81/139870 [00:00<11:43, 198.57it/s, epoch=0, test_loss=547, train_loss=265]Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 409.7705078125
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 368.80853271484375
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 205.38607788085938
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -157.44265747070312
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 168.9558868408203
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 266.9803161621094
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 107.18649291992188
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 128.27420043945312
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 67.76248168945312
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 134.25521850585938
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 113.44985961914062
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 134.49130249023438
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -620.166259765625
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -110.83610534667969
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.7892684936523438
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 199.07794189453125
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 270.2626647949219
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 117.3218002319336
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 44.70140075683594
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 90.52153015136719
Running validation...
Epoch 1, Step 100: Train Loss = 86.06279754638672, Test Loss = -99.8203353881836
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 52.41606521606445
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 48.98881530761719
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 176.08953857421875
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.3592071533203125
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 75.63599395751953
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42.987789154052734
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1035.15576171875
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26.64508819580078
  0%|          | 109/139870 [00:00<10:26, 223.16it/s, epoch=0, test_loss=-99.8, train_loss=-26.6]Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2446.97021484375
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59.30890655517578
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 31.25236701965332
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98.20821380615234
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 164.0048828125
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61.034915924072266
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 280.555908203125
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65.20626831054688
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 266.8670959472656
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 315.22698974609375
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5175.53369140625
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -476.243896484375
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -111.32734680175781
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9117.15234375
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -349.52166748046875
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1977.4365234375
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -285.9757080078125
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -716.99462890625
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 151.7508087158203
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -77.04878234863281
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 411.96734619140625
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2886.58837890625
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 86.27731323242188
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -446.48779296875
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -367.2294921875
  0%|          | 134/139870 [00:00<10:57, 212.58it/s, epoch=0, test_loss=-99.8, train_loss=-367] Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1947.995361328125
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13036.0341796875
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1725.050537109375
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2398.444091796875
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1996.2705078125
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4741.1171875
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -976.7828979492188
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4683.45703125
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6900.65234375
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3158.48681640625
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40008.921875
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -969.531494140625
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2063.47021484375
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -113100.7578125
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 954.6761474609375
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3633.212158203125
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -839.0216064453125
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10235.6611328125
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16139.072265625
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -62345.41796875
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2848.2607421875
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1117.4979248046875
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3453.998046875
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1974.82568359375
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18221.634765625
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16205.5205078125
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14423.0390625
  0%|          | 161/139870 [00:00<10:10, 228.76it/s, epoch=0, test_loss=-99.8, train_loss=-1.44e+4]Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4688.2724609375
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9468.7578125
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1654.8232421875
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25015.3046875
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4890.57177734375
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5520.3603515625
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2170.816650390625
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1561.646484375
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14850.298828125
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38655.03125
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -49106.5078125
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2183.6240234375
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3825.366455078125
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23195.64453125
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33859.93359375
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13550.015625
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3305.6357421875
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41383.234375
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9026.6240234375
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19374.146484375
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12054.541015625
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21585.62890625
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -54745.54296875
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24707.15234375
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22498.8515625
  0%|          | 186/139870 [00:00<10:59, 211.73it/s, epoch=0, test_loss=-99.8, train_loss=-2.25e+4]Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57305.0703125
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2638.57470703125
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -118361.3515625
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -165769.21875
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33650.8828125
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -649598.125
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96662.0625
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65811.859375
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1275360.875
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -110226.84375
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -121946.859375
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -165530.125
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -63178.99609375
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -329558.90625
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -137072.1875
Running validation...
Epoch 1, Step 200: Train Loss = -165258.609375, Test Loss = -65934.125
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -625367.9375
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160705.328125
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 22732.19921875
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3231789.5
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2376845.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -211591.25
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -364385.09375
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -225403.671875
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -386023.1875
  0%|          | 210/139870 [00:01<10:44, 216.71it/s, epoch=0, test_loss=-6.59e+4, train_loss=-3.86e+5]Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16504.49609375
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -401892.5
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -235769.421875
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -304809.4375
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -138066.34375
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1283469.75
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -309408.8125
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -416566.84375
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -947876.3125
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 12408.609375
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -391733.5625
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1037772.5
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2812512.75
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 127349.71875
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -242287.46875
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 70328.6875
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1801835.375
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -291896.625
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -377624.9375
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1260873.75
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1132368.25
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -854321.375
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190158.6875
Batch 234/394
  0%|          | 233/139870 [00:01<11:32, 201.73it/s, epoch=0, test_loss=-6.59e+4, train_loss=-1.9e+5] Batch data shape: (1, 28, 28, 1)
Train Loss: -1352089.25
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1672014.75
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6628485.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1832276.625
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1801489.375
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2936702.5
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -940968.1875
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5238594.5
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9554912.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3759324.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4916546.5
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5047250.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4980468.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3722078.75
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26912696.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1759611.625
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6140468.5
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20119100.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1583593.5
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1576325.375
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 169662.0
  0%|          | 254/139870 [00:01<11:55, 195.05it/s, epoch=0, test_loss=-6.59e+4, train_loss=1.7e+5] Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4960769.5
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5751020.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8136237.5
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11680352.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18635810.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6259767.5
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9621807.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14888675.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6375571.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3484854.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11584787.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15814096.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -73918784.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -123321976.0
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -144978432.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1274965.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11913584.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26745976.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -48107740.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17763280.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17147352.0
  0%|          | 275/139870 [00:01<11:46, 197.67it/s, epoch=0, test_loss=-6.59e+4, train_loss=-1.71e+7]Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38043796.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9624482.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43575316.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -49700708.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41044100.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228105472.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -45022312.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26060814.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -294904928.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26461452.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43773264.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -79201520.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -99330688.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95896488.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -419405024.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -119770064.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18937358.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -144831248.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -77746816.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23476880.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -72217304.0
  0%|          | 296/139870 [00:01<12:03, 192.96it/s, epoch=0, test_loss=-6.59e+4, train_loss=-7.22e+7]Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47011552.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8823212.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88968568.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90524144.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6978679.0
Running validation...
Epoch 1, Step 300: Train Loss = -8942996.0, Test Loss = -16313196.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -69914416.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1162292608.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104922184.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -45319504.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -60062656.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15675964.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169842048.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -191832304.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148491520.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -111640264.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -976627968.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -146713920.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136083200.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -135346272.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124401856.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1893497.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65366952.0
  0%|          | 318/139870 [00:01<11:36, 200.30it/s, epoch=0, test_loss=-1.63e+7, train_loss=-6.54e+7]Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -234775472.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -118626920.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169520768.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4300160.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -787868160.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -419490848.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -328478976.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -202351584.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -142173280.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -275054080.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -277431136.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -266021328.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -331718528.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -485758912.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -345983232.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1609157632.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -283632448.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -442109184.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16122200.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -363383456.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179566384.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6238686.0
  0%|          | 340/139870 [00:01<11:18, 205.55it/s, epoch=0, test_loss=-1.63e+7, train_loss=6.24e+6] Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2995248640.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -508180800.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -807646720.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -536150976.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -700954624.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1115085824.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169089088.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1396325.5
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -650540928.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190114624.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -605048064.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -747866560.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -652552064.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92883912.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -810229696.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -781617408.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -631448448.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90168848.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -227648576.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -872696960.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -972177088.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -637914112.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1167144448.0
  0%|          | 363/139870 [00:01<11:00, 211.18it/s, epoch=0, test_loss=-1.63e+7, train_loss=-1.17e+9]Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1367425536.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -976778048.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3316233728.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1453562880.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1091548288.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6827945984.0
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2392793600.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -345919936.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11804248064.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2008041472.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1084518656.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2531830272.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1939956224.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9285104.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3252101376.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -814206656.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1224373504.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1414968832.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3138141440.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2063689088.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1216904960.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1685093120.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -491941056.0
  0%|          | 386/139870 [00:01<10:48, 215.06it/s, epoch=0, test_loss=-1.63e+7, train_loss=-4.92e+8]Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 444110816.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1266351744.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11856400384.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 105416288.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1565782912.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3255208448.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2743907072.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3878874624.0
Starting epoch 2/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3489461760.0
Running validation...
Epoch 2, Step 0: Train Loss = -2591242752.0, Test Loss = -329678784.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26294820864.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3320614400.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3041504512.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1612333824.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1724256000.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3648477440.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1465587968.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28237852672.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3809778688.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3477366272.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6137369600.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6909319680.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3571247360.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3645821184.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2006383744.0
  0%|          | 410/139870 [00:02<10:32, 220.48it/s, epoch=1, test_loss=-3.3e+8, train_loss=-2.01e+9] Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3096448768.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1553123584.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6297810944.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2032670336.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2869524992.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1808116992.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9487438848.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5215342592.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26081923072.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41570406400.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6963111936.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 439554048.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8109750272.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52233445376.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4155672832.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10890374144.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -525704640.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6000324096.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14262841344.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26413719552.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4279055872.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -275613312.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -58816282624.0
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7757690368.0
  0%|          | 434/139870 [00:02<10:22, 224.13it/s, epoch=1, test_loss=-3.3e+8, train_loss=-7.76e+9]Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12577214464.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8489820672.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8091229696.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17643098112.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14471601152.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13708343296.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11250409472.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12137872384.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -99898015744.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5019374592.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7331527680.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8382331392.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13476356096.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7429930496.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16867105792.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16351910912.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -106369286144.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -130111045632.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5571544064.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37974581248.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104996495360.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19593203712.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12124831744.0
  0%|          | 457/139870 [00:02<10:27, 222.24it/s, epoch=1, test_loss=-3.3e+8, train_loss=-1.21e+10]Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12964403200.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4221345792.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16238925824.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12326891520.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136316985344.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13176756224.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23913093120.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17180581888.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39732543488.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5988182016.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33910882304.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17103222784.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190183063552.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20143298560.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -170909761536.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -195031728128.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124003106816.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28864053248.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12047663104.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 69651032.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -202102833152.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -232658157568.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40166285312.0
  0%|          | 480/139870 [00:02<10:31, 220.72it/s, epoch=1, test_loss=-3.3e+8, train_loss=-4.02e+10]Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19694458880.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35894394880.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30060191744.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42281988096.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -60716670976.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30959529984.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23501721600.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -256347717632.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -197794299904.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47535972352.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10925729792.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9893885952.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27547734016.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52741414912.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28302563328.0
Running validation...
Epoch 2, Step 100: Train Loss = -25569599488.0, Test Loss = -415899058176.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46726332416.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40332828672.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13578774528.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27969785856.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38672101376.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53949919232.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -449776648192.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43350753280.0
  0%|          | 503/139870 [00:02<10:34, 219.51it/s, epoch=1, test_loss=-4.16e+11, train_loss=-4.34e+10]Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -398679670784.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53932011520.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44303581184.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57531445248.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17101570048.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -55247650816.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 247265104.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -48112640000.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14721345536.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 12859089920.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -673065795584.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -102019432448.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -54753267712.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -616568520704.0
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -81190010880.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -187733327872.0
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83528073216.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87288422400.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16615518208.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47780573184.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2430273536.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -478773837824.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -108674326528.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -120720531456.0
  0%|          | 527/139870 [00:02<10:22, 223.88it/s, epoch=1, test_loss=-4.16e+11, train_loss=-1.21e+11]Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47020969984.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -101062385664.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -414730027008.0
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -150444376064.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -123619958784.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -78741463040.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -133118246912.0
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105723125760.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -163643424768.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -170021781504.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -138878025728.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -624854171648.0
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67359076352.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105826000896.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1229353385984.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5488175104.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4704957440.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -127171543040.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -143937273856.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -377507938304.0
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -566106390528.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3929192448.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103887953920.0
  0%|          | 550/139870 [00:02<10:20, 224.53it/s, epoch=1, test_loss=-4.16e+11, train_loss=-1.04e+11]Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19967258624.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -100823277568.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -130016182272.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -165976227840.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -174207533056.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88589934592.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -213331460096.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65776361472.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -192090734592.0
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104987279360.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6026602496.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -56690167808.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23509524480.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -287549358080.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179982041088.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -270570176512.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -283215003648.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20879568896.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -150704979968.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -222748344320.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -117894381568.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 15277643776.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -180276723712.0
  0%|          | 573/139870 [00:02<10:28, 221.46it/s, epoch=1, test_loss=-4.16e+11, train_loss=-1.8e+11] Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -174824013824.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -267831754752.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83656466432.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105178652672.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228775804928.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -183697227776.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -272342024192.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -181509521408.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 10678748160.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -242386223104.0
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -449764065280.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136696127488.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1682996985856.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179279544320.0
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182834429952.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2188031164416.0
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -296041054208.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -349777231872.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -204634308608.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97200373760.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -275266764800.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -267913658368.0
Running validation...
Epoch 2, Step 200: Train Loss = -226818342912.0, Test Loss = -260434362368.0
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -633831096320.0
  0%|          | 596/139870 [00:02<10:26, 222.19it/s, epoch=1, test_loss=-2.6e+11, train_loss=-6.34e+11]Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -303500492800.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1418919168.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2509724450816.0
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2235057569792.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -216653824000.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -411455225856.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -461055393792.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -407498129408.0
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4312834048.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -321321304064.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -253252993024.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -392051064832.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -109392756736.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -350899732480.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -265906552832.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -178150096896.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -326838845440.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -134164135936.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -413904633856.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -487935868928.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -863173869568.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -417460191232.0
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85119475712.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10628356096.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -275522322432.0
  0%|          | 621/139870 [00:03<10:06, 229.45it/s, epoch=1, test_loss=-2.6e+11, train_loss=-2.76e+11]Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -164298801152.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -189404102656.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -527641968640.0
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -517799378944.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -520215298048.0
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90164125696.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -491475533824.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -563874234368.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -885364817920.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -694954622976.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -347512766464.0
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -555481038848.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -262830227456.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1001760882688.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1119621218304.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -755337658368.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -539239383040.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -817573789696.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -546632138752.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -700728868864.0
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5696080838656.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -530739036160.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -805870567424.0
  0%|          | 644/139870 [00:03<10:16, 225.79it/s, epoch=1, test_loss=-2.6e+11, train_loss=-8.06e+11]Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4788155580416.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148788871168.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -149015986176.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -620883607552.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -320636059648.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -782560722944.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -960611942400.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -776299413504.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1258024075264.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -787170197504.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2446261747712.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -751763062784.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -701936041984.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -396524716032.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -683316084736.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -993623932928.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6289574854656.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6454743400448.0
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3782543671296.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50338889728.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1075098877952.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1176355995648.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1905225105408.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -941316702208.0
  0%|          | 668/139870 [00:03<10:07, 229.11it/s, epoch=1, test_loss=-2.6e+11, train_loss=-9.41e+11]Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -854433792000.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -944611917824.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -647025393664.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1655936122880.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1650201591808.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1231557754880.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5021497819136.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1100754714624.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1241391562752.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10757548802048.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -953644220416.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1155144613888.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1899036672000.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2013850107904.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1807434645504.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10915012411392.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2319777792000.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -605689348096.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2619305099264.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1742430273536.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1809444896768.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1560522391552.0
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -389256380416.0
  0%|          | 691/139870 [00:03<10:14, 226.43it/s, epoch=1, test_loss=-2.6e+11, train_loss=-3.89e+11]Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 85210644480.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3149098647552.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1682043961344.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -325029920768.0
Running validation...
Epoch 2, Step 300: Train Loss = -317258268672.0, Test Loss = -271461875712.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1318911475712.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12957951983616.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1107877298176.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1797968232448.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -705570275328.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -266223583232.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2690236284928.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2255698264064.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2882133295104.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1985809874944.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17959906967552.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1367539712000.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2158823604224.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1329814044672.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1218787409920.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -74498523136.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -877449969664.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2009857654784.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1893768626176.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1667947036672.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -135422197760.0
  1%|          | 716/139870 [00:03<10:03, 230.74it/s, epoch=1, test_loss=-2.71e+11, train_loss=-1.35e+11]Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8020238008320.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3872830521344.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3194571718656.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2361701695488.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1319303643136.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2928623222784.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1927358316544.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2007363878912.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3502896054272.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2337580515328.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2030366490624.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17599804997632.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3023794864128.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2651991834624.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 43763003392.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2391769350144.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1971565101056.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 436445478912.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20161922859008.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3039290720256.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3825315348480.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2317332512768.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2757282496512.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5499061796864.0
  1%|          | 740/139870 [00:03<10:19, 224.62it/s, epoch=1, test_loss=-2.71e+11, train_loss=-5.5e+12] Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -872829747200.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 238111260672.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3791992389632.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2264850759680.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2972855828480.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2419696336896.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2486503473152.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -334233534464.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2787625402368.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2669054787584.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1942041133056.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -168961900544.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1397517582336.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3326224629760.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2412402180096.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3543308435456.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4995120889856.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3937595293696.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4191960432640.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6969399181312.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3989226651648.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5556110622720.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -29422597963776.0
  1%|          | 763/139870 [00:03<10:39, 217.53it/s, epoch=1, test_loss=-2.71e+11, train_loss=-2.94e+13]Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10213577981952.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -796435415040.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27255724048384.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4033725857792.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3315166347264.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5599179309056.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3678737006592.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 41520070656.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9778314084352.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2531264561152.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3671164977152.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8631272603648.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7191594532864.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4886928818176.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4355680894976.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2883298525184.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1919193055232.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 80871882752.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2266980417536.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20428026281984.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 278716153856.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2076898492416.0
  1%|          | 785/139870 [00:03<10:37, 218.17it/s, epoch=1, test_loss=-2.71e+11, train_loss=-2.08e+12]Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5406107631616.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5329852563456.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6761641672704.0
Starting epoch 3/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6664129871872.0
Running validation...
Epoch 3, Step 0: Train Loss = -6117808144384.0, Test Loss = -7646511890432.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -49724983869440.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5580551880704.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5376808321024.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2006372188160.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2234337198080.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5027804479488.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2611675922432.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44030658347008.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6113825128448.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5732162338816.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8537650495488.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10155930419200.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9773500071936.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9634502934528.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2458556301312.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4439637753856.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5432657051648.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7670267379712.0
  1%|          | 807/139870 [00:03<11:18, 205.00it/s, epoch=2, test_loss=-7.65e+12, train_loss=-7.67e+12]Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2030625619968.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7088109518848.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4843671388160.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7960951521280.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6147035103232.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52261048811520.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57295400599552.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9065936715776.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 561683103744.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11308927811584.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67390826086400.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3881192390656.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9794538700800.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -637425287168.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7723577507840.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9682299125760.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25049710985216.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6240793001984.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -328576794624.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88427538153472.0
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9278142283776.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9959048740864.0
  1%|          | 829/139870 [00:03<11:07, 208.37it/s, epoch=2, test_loss=-7.65e+12, train_loss=-9.96e+12]Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9310986829824.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9600228130816.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28021322940416.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12776638513152.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14024664154112.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8973581287424.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12233459367936.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88096641122304.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10317142687744.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9087500681216.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6912682229760.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12801078722560.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7031665197056.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14310279479296.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9998756216832.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82335303204864.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84712424996864.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5784452726784.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34057926737920.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -81817759645696.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9661002547200.0
  1%|          | 850/139870 [00:04<11:18, 204.79it/s, epoch=2, test_loss=-7.65e+12, train_loss=-9.66e+12]Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7547593424896.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5581595738112.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3389162782720.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15646513430528.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5128809086976.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112367954296832.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7488484147200.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17898892427264.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8251597914112.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26725371084800.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3006361239552.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19960961171456.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9606243811328.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -142733557628928.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10586123403264.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -132377376456704.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -130786275622912.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97282418540544.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18762422026240.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6987034132480.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 66933014528.0
  1%|          | 871/139870 [00:04<11:13, 206.23it/s, epoch=2, test_loss=-7.65e+12, train_loss=6.69e+10] Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -153032285224960.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -189087378046976.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24055646257152.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14271323832320.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20748580159488.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5928385511424.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27981938425856.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22256929472512.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13246654316544.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9552197058560.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -111721268117504.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82261961605120.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24274322587648.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3995098677248.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4802479128576.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12784977838080.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25237278162944.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10627013672960.0
Running validation...
Epoch 3, Step 100: Train Loss = -12882676809728.0, Test Loss = -25112849940480.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27719387578368.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16648824160256.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5157656985600.0
  1%|          | 892/139870 [00:04<11:31, 201.08it/s, epoch=2, test_loss=-2.51e+13, train_loss=-5.16e+12]Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10817788444672.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15457001144320.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22220457902080.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -197608123400192.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16691200262144.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -243840443219968.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22018724462592.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15302817480704.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9978999996416.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8443204730880.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22339062333440.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 166969163776.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23221623586816.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6606455570432.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1833633447936.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -202961682694144.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24528109436928.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27782849495040.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -168493697728512.0
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30628472946688.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -81427429326848.0
  1%|          | 913/139870 [00:04<11:26, 202.33it/s, epoch=2, test_loss=-2.51e+13, train_loss=-8.14e+13]Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26756893376512.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39574495559680.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8053822324736.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17549072793600.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 21663580160.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -214971032010752.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37308770287616.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -31843814473728.0
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16918327066624.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38243902947328.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112193186037760.0
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47502548008960.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40704713687040.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19854419558400.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41025527611392.0
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27366250250240.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51177379069952.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39254642130944.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33478053724160.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -203667131072512.0
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26518856138752.0
  1%|          | 934/139870 [00:04<11:44, 197.17it/s, epoch=2, test_loss=-2.51e+13, train_loss=-2.65e+13]Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38352027910144.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -314228934180864.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -790477930496.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1119246221312.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21869266731008.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37429020983296.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -119397096095744.0
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -141673388572672.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 464556916736.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28369785716736.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7190939172864.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -29781802352640.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52418314240000.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37466115407872.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43904124583936.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27132667363328.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43698129731584.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16440037998592.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61769506619392.0
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34632636563456.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1306537099264.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10119601455104.0
  1%|          | 956/139870 [00:04<11:26, 202.34it/s, epoch=2, test_loss=-2.51e+13, train_loss=-1.01e+13]Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9080703811584.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52651152637952.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46356278280192.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -54801157062656.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61075810680832.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3622427688960.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34647526342656.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44266504060928.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27527500267520.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3216499539968.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25504063160320.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33689366953984.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34222639153152.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23064469307392.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26251465064448.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -70383852060672.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40794933166080.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61094638911488.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40439306518528.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2395416035328.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -69390322106368.0
  1%|          | 977/139870 [00:04<11:50, 195.38it/s, epoch=2, test_loss=-2.51e+13, train_loss=-6.94e+13]Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84384145211392.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23633353244672.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -272321579319296.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39448867766272.0
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39045400887296.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -429819154137088.0
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44077458391040.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41551887597568.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42866369888256.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24919792418816.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -79516974186496.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47757221953536.0
Running validation...
Epoch 3, Step 200: Train Loss = -46178431401984.0, Test Loss = -37627621277696.0
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -110955648253952.0
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32113883611136.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 389346164736.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -409515031789568.0
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -273985224835072.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47469228457984.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83268795891712.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83972801429504.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67667473989632.0
  1%|          | 998/139870 [00:04<11:46, 196.60it/s, epoch=2, test_loss=-3.76e+13, train_loss=-6.77e+13]Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1249242382336.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34508871041024.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32361576136704.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50627921051648.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15230130192384.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -69122092171264.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28492643172352.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37590031925248.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65841647321088.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18464498515968.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -75322066206720.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87474290294784.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124548791926784.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -54623285018624.0
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11916864913408.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -430823178240.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -63649829879808.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16244723941376.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35099525513216.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -64918162243584.0
  1%|          | 1018/139870 [00:04<12:12, 189.67it/s, epoch=2, test_loss=-3.76e+13, train_loss=-6.49e+13]Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67428499324928.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51392496533504.0
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20786916098048.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -66330833190912.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42113072889856.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -204480272400384.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -63599238184960.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -58857774645248.0
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59529635037184.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32146565627904.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -132805044469760.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -120453247008768.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85264630284288.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87888821747712.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -107505816436736.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -70534922502144.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82372255023104.0
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -483985033527296.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -74833975050240.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96190288887808.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -507931489468416.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16649732227072.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17917968121856.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83903108874240.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51205283774464.0
  1%|          | 1043/139870 [00:05<11:17, 204.92it/s, epoch=2, test_loss=-3.76e+13, train_loss=-5.12e+13]Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -73964864929792.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103067580104704.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -125447689994240.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -114459964080128.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -75970673377280.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -219048684027904.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -121201695391744.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67544710905856.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -62779092369408.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92095205343232.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98946550595584.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -727704563351552.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -696437402763264.0
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -661079084498944.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2167074193408.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96408610799616.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -147790629961728.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -237628460892160.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84096483065856.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -110343967735808.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -114739849986048.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -68853606383616.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -165341258842112.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160082507595776.0
  1%|          | 1067/139870 [00:05<10:46, 214.66it/s, epoch=2, test_loss=-3.76e+13, train_loss=-1.6e+14] Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95501894549504.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -509842347261952.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95044035936256.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -151155535511552.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -933352362213376.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -71694941159424.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124404935688192.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -161486072709120.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -237112611831808.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -199828822818816.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1004756394835968.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -225160774811648.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59619770630144.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -253609950314496.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -152497813454848.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -132102615990272.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -122725779636224.0
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -72455150370816.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7981544505344.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -257362057232384.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96108810338304.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23298880569344.0
Running validation...
Epoch 3, Step 300: Train Loss = -19788266995712.0, Test Loss = -139575800365056.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90882464284672.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1457229790183424.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -145074012815360.0
  1%|          | 1092/139870 [00:05<10:19, 224.00it/s, epoch=2, test_loss=-1.4e+14, train_loss=-1.45e+14]Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182178101067776.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47244136939520.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21493851357184.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -217939491946496.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -186693017665536.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -203321369427968.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -161040788619264.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1257766945554432.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -102576183836672.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -176796758704128.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88431732457472.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95843621273600.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4683077779456.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82032885497856.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -170154222682112.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -121495808376832.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -146410536173568.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7987270778880.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -652178167431168.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -238512536289280.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228802806415360.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -156576757317632.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104547791929344.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182346426875904.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -155648675282944.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179302452690944.0
  1%|          | 1118/139870 [00:05<09:53, 233.89it/s, epoch=2, test_loss=-1.4e+14, train_loss=-1.79e+14]Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -229097363996672.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -240976119463936.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97894568820736.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1513168249552896.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -199993189203968.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -247692978552832.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2630131908608.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -134697313107968.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -106335672729600.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 19826911215616.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1274242171666432.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -185105205166080.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -249075068829696.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -144574035001344.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -249805985021952.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -339925220196352.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50911959318528.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 11906308898816.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -173527466508288.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -153310065590272.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -221357749043200.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182713613025280.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -167741357031424.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22894318977024.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -170892235636736.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179951361851392.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -114673303158784.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14862826602496.0
  1%|          | 1146/139870 [00:05<09:23, 246.38it/s, epoch=2, test_loss=-1.4e+14, train_loss=-1.49e+13]Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -78432486555648.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -234693505056768.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -168708764860416.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190291478839296.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -173887371345920.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169460249919488.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -232258208268288.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -549077007728640.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228616579317760.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -295509520547840.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1667741975052288.0
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -665691745157120.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -66993445142528.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1832653418397696.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -244050762399744.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -195406080245760.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -335352657084416.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -260671530860544.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3280222552064.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -539288374607872.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90517559836672.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -217641461481472.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -362728006877184.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -534649205948416.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179342214692864.0
  1%|          | 1171/139870 [00:05<09:29, 243.58it/s, epoch=2, test_loss=-1.4e+14, train_loss=-1.79e+14]Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -177993058091008.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -153108168572928.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -94298095747072.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16375295770624.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -94873025773568.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -965047274700800.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 17332887879680.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -141661879402496.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -267782503530496.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -274540198363136.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -346315158454272.0
Starting epoch 4/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -299558970064896.0
Running validation...
Epoch 4, Step 0: Train Loss = -290703317925888.0, Test Loss = -280124830253056.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2350061149224960.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -237441009057792.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -194714187857920.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -102639928868864.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -113465863700480.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -259875435184128.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -120283964899328.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2119181424132096.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -387341357154304.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -350990633009152.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -364181618425856.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -562412277202944.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -362556375957504.0
  1%|          | 1196/139870 [00:05<09:27, 244.53it/s, epoch=3, test_loss=-2.8e+14, train_loss=-3.63e+14]Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -347822188658688.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104809759768576.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -201603667722240.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -204653060947968.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -405996916703232.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124143353724928.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -343721803513856.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -197250097610752.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -436816595386368.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -295913717235712.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2227080666284032.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2101069144391680.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -396367029600256.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 24828669067264.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -583181531086848.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3147884073582592.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -180805238259712.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393428399554560.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21979205730304.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -392506726416384.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -475883282366464.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1062721609007104.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -244887509270528.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10201956614144.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2949496312954880.0
  1%|          | 1221/139870 [00:05<09:38, 239.86it/s, epoch=3, test_loss=-2.8e+14, train_loss=-2.95e+15]Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -424193787166720.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -424313777815552.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -374148761124864.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -433940812791808.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1086546866339840.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -527572676902912.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -440673073364992.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -400817488134144.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -572972729368576.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3224999674511360.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -234201714524160.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -358634030628864.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -229762295398400.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -439132891381760.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -288402490523648.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -724416832995328.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -421071043952640.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4322425827229696.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3807781639421952.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -181665188020224.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1132507177156608.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2849805290176512.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -515910498516992.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -336002136670208.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -292891134001152.0
  1%|          | 1246/139870 [00:05<09:49, 235.35it/s, epoch=3, test_loss=-2.8e+14, train_loss=-2.93e+14]Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -108474641940480.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -532697176866816.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -193296982867968.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3296882931531776.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -234853475811328.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -605890432466944.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -374813541531648.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -869468750217216.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -89395323469824.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -659071791267840.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -355496993226752.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4219090256265216.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -376959649447936.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4553198983446528.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4450669524156416.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3707023317270528.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -500672759857152.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -217677985480704.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1136347578368.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4260491593515008.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4201964611043328.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -892982756638720.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393091781492736.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -752422033031168.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -402790287409152.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -805835924045824.0
  1%|          | 1272/139870 [00:05<09:31, 242.31it/s, epoch=3, test_loss=-2.8e+14, train_loss=-8.06e+14]Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -844838052298752.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -361983635357696.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -422121633218560.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4371705476677632.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2120553934618624.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -789484782223360.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92161576009728.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -123603521634304.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -440788232175616.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -749148731080704.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -407240745943040.0
Running validation...
Epoch 4, Step 100: Train Loss = -437675320410112.0, Test Loss = -601321325461504.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -666749112418304.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -477955000107008.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -164410744111104.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -440443259060224.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -436158559420416.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -637260672270336.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5082677182988288.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -553486664073216.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5370735270821888.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -508298675617792.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -367728925868032.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -736674199896064.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -280284868116480.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -531033514573824.0
  1%|          | 1297/139870 [00:06<09:28, 243.58it/s, epoch=3, test_loss=-6.01e+14, train_loss=-5.31e+14]Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8343917690880.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -543294505353216.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -192632789663744.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8410020970496.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6402312327135232.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -946102777937920.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -636273366663168.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6524414489264128.0
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -801589744893952.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1994372761518080.0
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -874295353933824.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1160560292921344.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -192402757255168.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -637869282557952.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7849321168896.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4262103548428288.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1025687313973248.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1044317204381696.0
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -503423048876032.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1145138139103232.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3533905265164288.0
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1243409675190272.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1285989175656448.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -766228004077568.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -937118310334464.0
  1%|          | 1322/139870 [00:06<09:28, 243.54it/s, epoch=3, test_loss=-6.01e+14, train_loss=-9.37e+14]Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -779274906763264.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1405093450612736.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1317826426044416.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -913537396375552.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5902239084314624.0
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -495441590353920.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -890582876553216.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7174385311940608.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -31321904644096.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 24683606966272.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -928626623119360.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -881537004339200.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2526670741307392.0
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4284625283186688.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 12061165748224.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -790535841251328.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160520007057408.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -725038932164608.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -968854025011200.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1329122995339264.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -998826722721792.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -513214769004544.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1006615914348544.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -412123989540864.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1509112458248192.0
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -750142009376768.0
  1%|          | 1348/139870 [00:06<09:19, 247.54it/s, epoch=3, test_loss=-6.01e+14, train_loss=-7.5e+14] Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 32957221830656.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -286617394741248.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190178937274368.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1398479737847808.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1051417859063808.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1167214002569216.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1094660193779712.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -86958843887616.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -838101664530432.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1045376920453120.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -654813129867264.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 84479892783104.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -825689209044992.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -691080706129920.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1304562493292544.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -400600390959104.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -720933480300544.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1185459560513536.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -806825242918912.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1689221509152768.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1135466946494464.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 54753597849600.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1480952370954240.0
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2068683614584832.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -678415451553792.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7308152068374528.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -946876207595520.0
  1%|          | 1375/139870 [00:06<09:08, 252.58it/s, epoch=3, test_loss=-6.01e+14, train_loss=-9.47e+14]Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -775843362111488.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0099823902457856e+16
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1514286014791680.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1189889550843904.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1184790619357184.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -684015417819136.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1726689193230336.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -900033146781696.0
Running validation...
Epoch 4, Step 200: Train Loss = -1067629682884608.0, Test Loss = -1.021749097398272e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2965525130903552.0
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -973650899501056.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 15386079657984.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0362262510370816e+16
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9107757324042240.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1131426724446208.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1411603413073920.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1727799173840896.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1335387205140480.0
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 33380733288448.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -952582071648256.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -971756818923520.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1071129812795392.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -509188908253184.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1787900798697472.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -929578495246336.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1106343444348928.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1287481274138624.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -328806254510080.0
  1%|          | 1402/139870 [00:06<09:03, 254.76it/s, epoch=3, test_loss=-1.02e+16, train_loss=-3.29e+14]Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1416159970721792.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1666272022495232.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2608329343893504.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1366332444508160.0
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -201140197130240.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7608610062336.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1116718676049920.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -392919378821120.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -566687111839744.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1335765162262528.0
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1800681010757632.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1196586075947008.0
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -235079766900736.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1402576868212736.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1153017525043200.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3800432681943040.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1752047351234560.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1061803828183040.0
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1150688176373760.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -651413931687936.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2856061983784960.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2670857088401408.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1822822506692608.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1798151275020288.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2047421110550528.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1237361824366592.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2022857118842880.0
  1%|          | 1429/139870 [00:06<08:55, 258.76it/s, epoch=3, test_loss=-1.02e+16, train_loss=-2.02e+15]Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1701684568850432e+16
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1628053256011776.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2278848980844544.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2012806631063552e+16
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -315542388867072.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -241105673125888.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1802016745586688.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1382608223076352.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1581277035626496.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2212489991290880.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2062661667782656.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2579992223416320.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1446204609134592.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5315467631656960.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2091385872187392.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1656357627363328.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1139104246923264.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1532947916128256.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1900585976594432.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3956043109302272e+16
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2736345411682304e+16
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.265625930399744e+16
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34867635027968.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1509856024461312.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2383997833314304.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3236818887639040.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1756227696590848.0
  1%|          | 1456/139870 [00:06<08:53, 259.47it/s, epoch=3, test_loss=-1.02e+16, train_loss=-1.76e+15]Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1778532132847616.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2256979544244224.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1213225718775808.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2717655052058624.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3382321910644736.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2059526475874304.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4770538559832064e+16
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2258739138658304.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2731954608799744.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6911378967166976e+16
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1865767884685312.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2073758789533696.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2780206955757568.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4659463990542336.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3687745088126976.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7529299133267968e+16
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3868366448099328.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -942704989044736.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3886050707505152.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2716578625880064.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2313839005663232.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2025574759399424.0
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1379141379162112.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 148352163381248.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5066190951022592.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2206604543918080.0
  1%|          | 1482/139870 [00:06<09:05, 253.79it/s, epoch=3, test_loss=-1.02e+16, train_loss=-2.21e+15]Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -420132962697216.0
Running validation...
Epoch 4, Step 300: Train Loss = -444361544302592.0, Test Loss = -3466897869766656.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2588329828679680.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.322330478129971e+16
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2069018756251648.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2749030861897728.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1179563912593408.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -348638802870272.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3631926619406336.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3094609836113920.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3738202363920384.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2349907604144128.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.170722642296832e+16
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1665467252998144.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3316576430325760.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1383763032408064.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1916222073470976.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -68202503602176.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1381025259192320.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2402278287867904.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1770061249380352.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2772845381812224.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -145908360544256.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9480339294519296.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3040989048471552.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3079953528651776.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2221689677021184.0
  1%|          | 1508/139870 [00:06<09:16, 248.80it/s, epoch=3, test_loss=-3.47e+15, train_loss=-2.22e+15]Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1572673947697152.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3232800408862720.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2532980585136128.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2137786014498816.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4260110683602944.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4194713364070400.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2747769752125440.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.246404046271283e+16
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3122905080659968.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3486955098603520.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 53519272902656.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2167814815219712.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1679078407012352.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 166035198050304.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.401833758752768e+16
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2921125101174784.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3322277730975744.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2382588547170304.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3423500714901504.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4846208631701504.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1008686491238400.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 80322867757056.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3094114035826688.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2779954894864384.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3868429261996032.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2881529529237504.0
  1%|          | 1534/139870 [00:07<09:12, 250.56it/s, epoch=3, test_loss=-3.47e+15, train_loss=-2.88e+15]Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2393452666945536.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -315239325237248.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3095235022290944.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2861833346088960.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1655126045491200.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23104709459968.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1482471581417472.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4051513248841728.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2049284052615168.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2578535155761152.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3445883668529152.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3006349331922944.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3688672801062912.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7929223666728960.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3851320796643328.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4449842206081024.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.969689853316301e+16
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9333392357195776.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -788675986194432.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0922686494408704e+16
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3854628995203072.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2426644711079936.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5183002250313728.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3313064757690368.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 50866975408128.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8219906516451328.0
  1%|          | 1560/139870 [00:07<09:14, 249.56it/s, epoch=3, test_loss=-3.47e+15, train_loss=-8.22e+15]Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1583459952754688.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3189940393345024.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6217598131765248.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8155337051865088.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3302809717964800.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3366152700952576.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2298878023958528.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1301751034544128.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 187707502362624.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1824607065604096.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.406213094899712e+16
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 291217908695040.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1276895656148992.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3322640924147712.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4266706948063232.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4632457773056000.0
Starting epoch 5/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4243955432554496.0
Running validation...
Epoch 5, Step 0: Train Loss = -4552269123026944.0, Test Loss = -2.685759229054157e+16
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4494993330601984e+16
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3414257676845056.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4250757855444992.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1284565796651008.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1184469167898624.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3426361431556096.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1418146124660736.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2677803405082624e+16
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3872997496586240.0
  1%|          | 1586/139870 [00:07<09:12, 250.49it/s, epoch=4, test_loss=-2.69e+16, train_loss=-3.87e+15]Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4569086067474432.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5827621543739392.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8192064927825920.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6328165085478912.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5342072806572032.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1670217084174336.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2500477614817280.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2394873227378688.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4551717219729408.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1311759382085632.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3783418168999936.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3034387415302144.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5638974768939008.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4346747958591488.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.582695073592115e+16
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.100517563629568e+16
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5290594402304000.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 340927658852352.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8252373247983616.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.839245496090624e+16
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2438787925803008.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5071020641746944.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -203438541504512.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4232024818712576.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4564555950718976.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1727777568915456e+16
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3594304048070656.0
  1%|          | 1613/139870 [00:07<09:05, 253.55it/s, epoch=4, test_loss=-2.69e+16, train_loss=-3.59e+15]Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -94951392149504.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.20406154870784e+16
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4981347630186496.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4187009266483200.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4215565698727936.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4165736830337024.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2596557111099392e+16
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5245791450955776.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6721783268900864.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3612980981792768.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6397802074603520.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.473327838874829e+16
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4468713487073280.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5729387051745280.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2825166673412096.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6268301294436352.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3314697919004672.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9861133242466304.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5038907708145664.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.258147829861581e+16
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.52048572079145e+16
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2481163683758080.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3144617486647296e+16
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.511396683743232e+16
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5170018027307008.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2733509386960896.0
  1%|          | 1639/139870 [00:07<09:08, 251.96it/s, epoch=4, test_loss=-2.69e+16, train_loss=-2.73e+15]Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2574298170523648.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1477390366670848.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8210915539288064.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2231736276615168.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.238263232279347e+16
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2734238726094848.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6560487919583232.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5012446951505920.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9788756601077760.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1152037467193344.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7858616551866368.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4608817736187904.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.659998096982016e+16
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4261717538242560.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.553518126773043e+16
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.535212976157491e+16
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5589456224321536e+16
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6242543301820416.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2774663763591168.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 10104174804992.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.152474509023642e+16
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.455007468381798e+16
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9387868648636416.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4182623836438528.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7439341307559936.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4651605274132480.0
  1%|          | 1665/139870 [00:07<09:06, 252.81it/s, epoch=4, test_loss=-2.69e+16, train_loss=-4.65e+15]Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8311774793170944.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9534039270621184.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3898156878135296.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4536217219629056.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.360868227704422e+16
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1552264275492864e+16
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9149669762400256.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1062997426438144.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1642349054656512.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5878630722830336.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8015714644393984.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4724538113785856.0
Running validation...
Epoch 5, Step 100: Train Loss = -5308326174785536.0, Test Loss = -8090152333213696.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6599697212899328.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7240947641352192.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1759931569012736.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4865764154671104.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5862029063618560.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7289447485800448.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.555048665625395e+16
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6137317139939328.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.54843956995031e+16
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7662249573351424.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4445786683211776.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8546146326675456.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2736242059902976.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6974343988903936.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 79075775348736.0
  1%|          | 1692/139870 [00:07<08:57, 257.13it/s, epoch=4, test_loss=-8.09e+15, train_loss=7.91e+13] Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5100942605156352.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1831742885330944.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 130439759003648.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.300646773260288e+16
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9217740699074560.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5407682726985728.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.94479031894016e+16
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7747686136545280.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.662857174941696e+16
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6963222707961856.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1432876524437504e+16
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2069409195622400.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6292243052756992.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 178704730816512.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.635587837847142e+16
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2712198031802368e+16
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2588884152025088e+16
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5293265871962112.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3530540699287552e+16
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.305202203066368e+16
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4755421348691968e+16
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3756902655655936e+16
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8113738682990592.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1974405328470016e+16
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8401598665457664.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.27178813472768e+16
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3279889629118464e+16
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9719479852335104.0
  1%|          | 1720/139870 [00:07<08:44, 263.19it/s, epoch=4, test_loss=-8.09e+15, train_loss=-9.72e+15]Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.791143205378458e+16
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6131938230272000.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9506371091300352.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.402239367204045e+16
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -367622356992000.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 234151819083776.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8998535231963136.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0821565239263232e+16
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.048821806759936e+16
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.811489082448282e+16
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 270833792581632.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7319047863533568.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1784813925171200.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7183766057385984.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0781348239245312e+16
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.207043113353216e+16
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1578712138973184e+16
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7137142576775168.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1297752558338048e+16
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4789650891735040.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4429987381706752e+16
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8328992780189696.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 322073893273600.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2276666332151808.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1886558747623424.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5697127288078336e+16
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0476852237828096e+16
  1%|          | 1747/139870 [00:07<09:00, 255.72it/s, epoch=4, test_loss=-8.09e+15, train_loss=-1.05e+16]Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.296004234084352e+16
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2875529195618304e+16
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -742409524740096.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8895526883819520.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.143777708212224e+16
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6033495432364032.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 863056565370880.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9577536551911424.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7918651604729856.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2979581220814848e+16
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3747602705154048.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7370066169430016.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0538034046959616e+16
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7967418810892288.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7493435082604544e+16
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.134269724360704e+16
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 444502372253696.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.586647892230144e+16
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.056291613881139e+16
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6285685711437824.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.968222801513677e+16
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0184981292777472e+16
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8174736345399296.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.96431658327081e+16
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.375569576984576e+16
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1945245721755648e+16
  1%|▏         | 1773/139870 [00:07<09:44, 236.18it/s, epoch=4, test_loss=-8.09e+15, train_loss=-1.19e+16]Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2146593788592128e+16
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5907422036099072.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.547200765100032e+16
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9570638834434048.0
Running validation...
Epoch 5, Step 200: Train Loss = -8630607395422208.0, Test Loss = -1.5098865187291136e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.499521716158464e+16
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.046031768748032e+16
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 150072432001024.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.464811699254067e+16
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.415844105610854e+16
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0433826329198592e+16
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4640671633702912e+16
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5691538461884416e+16
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3959421101080576e+16
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 332150087876608.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9216078546731008.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0690487132356608e+16
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0575515152809984e+16
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4220375525228544.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7000192448397312e+16
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9466017726070784.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6695664499032064.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0983665894948864e+16
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2781264591454208.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4892554285744128e+16
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5422801284431872e+16
  1%|▏         | 1798/139870 [00:08<09:39, 238.08it/s, epoch=4, test_loss=-1.51e+16, train_loss=-1.54e+16]Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.412775187939328e+16
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3304744604860416e+16
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1944395582537728.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 75253397061632.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9432612544184320.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3207890940723200.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5399020851691520.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5746916696457216e+16
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4312142069039104e+16
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2110065091739648e+16
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3255331538862080.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3385214038376448e+16
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0278367840436224e+16
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.654131976136294e+16
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7238804725235712e+16
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.076967344439296e+16
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0860073916039168e+16
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5680774464405504.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7720358878511104e+16
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.555977772774195e+16
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4629444589191168e+16
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6808707773956096e+16
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.937084152335565e+16
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3667856172449792e+16
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7954905696239616e+16
  1%|▏         | 1823/139870 [00:08<09:37, 239.01it/s, epoch=4, test_loss=-1.51e+16, train_loss=-1.8e+16] Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1197938377372467e+17
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.342651337015296e+16
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.092499261928243e+16
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1124049477999002e+17
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2783991627251712.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1980455054213120.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5333471333384192e+16
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1114198541008896e+16
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2748083557302272e+16
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9983673226952704e+16
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8411895585243136e+16
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.175732936395981e+16
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2651319017865216e+16
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.438903675997389e+16
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9378510187462656e+16
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3892663350657024e+16
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9708073492938752.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3441698227027968e+16
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5581014994714624e+16
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1625328855010509e+17
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3282115066422886e+17
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3710293241063014e+17
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -226146536914944.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.609814945824768e+16
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1871760177627136e+16
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1009507110813696e+16
  1%|▏         | 1849/139870 [00:08<09:25, 243.93it/s, epoch=4, test_loss=-1.51e+16, train_loss=-3.1e+16]Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6763715844046848e+16
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.477268496973824e+16
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9379704188370944e+16
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2543012827561984e+16
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2494274884993024e+16
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.589628197293261e+16
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8326234610008064e+16
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2094518813366682e+17
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.586720369803264e+16
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3191732476706816e+16
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4598050955237786e+17
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4345307806498816e+16
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9371071304105984e+16
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5913453252182016e+16
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.613736520227226e+16
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.176481791200461e+16
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5493132447645696e+17
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.141264992108544e+16
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7558328007786496.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1145702671253504e+16
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.536157787193344e+16
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.102299733052621e+16
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.921363498290381e+16
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0286482107400192e+16
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1279390763712512.0
  1%|▏         | 1874/139870 [00:08<09:25, 244.18it/s, epoch=4, test_loss=-1.51e+16, train_loss=1.28e+15]Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.238031491537306e+16
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6921034053648384e+16
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3291336786575360.0
Running validation...
Epoch 5, Step 300: Train Loss = -3151706326040576.0, Test Loss = -841854660640768.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7257550109999104e+16
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8986830644903936e+17
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.627263250464768e+16
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.302399897141248e+16
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0169423847489536e+16
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2443467829542912.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.803030948839424e+16
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3431876983128064e+16
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.599053503024333e+16
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.231820484817715e+16
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8235233701933875e+17
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.47864718147584e+16
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.934724926799872e+16
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.152101354831872e+16
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4540016927637504e+16
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -315895918362624.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0339322016301056e+16
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.891153772072141e+16
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.575761223876608e+16
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7476386209923072e+16
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1232822312370176.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.772068693141094e+16
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.038278333549773e+16
  1%|▏         | 1900/139870 [00:08<09:18, 246.85it/s, epoch=4, test_loss=-8.42e+14, train_loss=-3.04e+16]Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9412423521796096e+16
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.184954231180493e+16
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4215861787164672e+16
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.335574224535552e+16
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1309731494690816e+16
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0615645452304384e+16
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8780730469318656e+16
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.545219765947597e+16
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.860667879456768e+16
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.927176392928133e+17
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.216220519353549e+16
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1194993590861824e+16
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 395989877784576.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6711989405417472e+16
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.177099138236416e+16
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 905211199619072.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9198689355707187e+17
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.513268832731136e+16
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3488259166306304e+16
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.756546597912576e+16
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3892630337224704e+16
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.656354621711974e+16
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6598570857725952.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 727553367080960.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5127487121915904e+16
  1%|▏         | 1925/139870 [00:08<09:58, 230.46it/s, epoch=4, test_loss=-8.42e+14, train_loss=-2.51e+16]Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.031448878795981e+16
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9158131393101824e+16
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.934753058835661e+16
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.75317612232704e+16
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2575428820664320.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2020754740609024e+16
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.840611670674637e+16
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.326084037541888e+16
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -828848694362112.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9645292882231296.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8635734520889344e+16
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.928877114078003e+16
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4104404437172224e+16
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.735921225845965e+16
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4484487568031744e+16
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.652449637446451e+16
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.453288005481267e+16
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6893167504654336e+16
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.378411396608819e+16
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9182983519299174e+17
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.908889975803085e+16
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7034689353154560.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3298933142428058e+17
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.702740026503987e+16
  1%|▏         | 1949/139870 [00:08<10:29, 219.22it/s, epoch=4, test_loss=-8.42e+14, train_loss=-2.7e+16] Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9314764282855424e+16
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.18850536465367e+16
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.386588557587251e+16
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 621145954975744.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.388344421987123e+16
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1718747400175616e+16
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.130128973647053e+16
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.211177638015795e+16
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.699336561439539e+16
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4585505198833664e+16
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.773736050655232e+16
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9859847172325376e+16
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.167429019369472e+16
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1567749499256832.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.189916931260416e+16
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0696653846400205e+17
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2113201755914240.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1501564862660608e+16
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3144734797070336e+16
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.222541020482765e+16
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1833002665312256e+16
Starting epoch 6/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2795086077034496e+16
Running validation...
Epoch 6, Step 0: Train Loss = -2.6324535309500416e+16, Test Loss = -1.0509315962883277e+17
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0687352351358976e+17
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7430448586031104e+16
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9544214593273856e+16
  1%|▏         | 1974/139870 [00:08<10:08, 226.66it/s, epoch=5, test_loss=-1.05e+17, train_loss=-2.95e+16]Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9326755558981632.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9357327136194560.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.521879383166157e+16
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2313543462354944e+16
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7891136227128115e+17
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5391599965896704e+16
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.830631482425344e+16
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.825617564860416e+16
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.609688138566861e+16
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.119032550653952e+16
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4213468961767424e+16
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1216476744712192e+16
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5500195521363968e+16
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4951247161327616e+16
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.303798607754035e+16
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0432026737901568e+16
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.023471219048448e+16
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2394317838548992e+16
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.524516118102016e+16
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0343145082322944e+16
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6142297699647488e+17
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4397156280015258e+17
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.01642835893289e+16
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2306379989647360.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.221562251948851e+16
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8537628448417382e+17
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6112149977890816e+16
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.708004180426752e+16
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1845793065533440.0
  1%|▏         | 2003/139870 [00:08<09:27, 242.80it/s, epoch=5, test_loss=-1.05e+17, train_loss=-1.85e+15]Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.559794522718208e+16
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.904105085717709e+16
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0139447123247104e+17
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.238919206764544e+16
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -514626739503104.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.221681769325527e+17
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.086980685706035e+16
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.818998563504128e+16
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.556545379958784e+16
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.970507829837824e+16
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.829915218162483e+16
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8240778190848e+16
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.624572964719821e+16
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.085996279201792e+16
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.104257863155712e+16
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8138924916342784e+17
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.910238486508339e+16
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.337291379716915e+16
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6757697521123328e+16
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9118562131968e+16
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2282713385926656e+16
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.306600886422733e+16
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.89119742050304e+16
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2610102694589235e+17
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.326512643891528e+17
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4976752824614912e+16
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.913973366115533e+16
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4723707797490893e+17
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.063513226405478e+16
  1%|▏         | 2032/139870 [00:09<09:01, 254.33it/s, epoch=5, test_loss=-1.05e+17, train_loss=-4.06e+16]Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1438335700434944e+16
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7047151473328128e+16
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6724464939106304.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.766533472269107e+16
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.652886488481792e+16
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.861676751581348e+17
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.894341711547597e+16
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.117369109820211e+16
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.610618346713907e+16
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.859147624572518e+16
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5737318245728256.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.618460179772211e+16
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.224819688125235e+16
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.818643740562555e+17
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8197903407251456e+16
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1364383351177216e+17
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.157467541484339e+17
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3462738041136742e+17
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.870469908332544e+16
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6550540511019008e+16
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16933977063424.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6385980243194675e+17
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.481686032724787e+17
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.452440178937037e+16
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.138737161850061e+16
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.560251077741773e+16
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.767825316662477e+16
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.822423639687168e+16
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.693617624992973e+16
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4133689944244224e+16
  1%|▏         | 2062/139870 [00:09<08:39, 265.33it/s, epoch=5, test_loss=-1.05e+17, train_loss=-3.41e+16]Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.485738951357235e+16
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1761599263512986e+17
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8157496511863194e+17
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.368244834035302e+16
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5832463582494720.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7720853328363520.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0304136041857024e+16
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.326584510257562e+16
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.022227826016256e+16
Running validation...
Epoch 6, Step 100: Train Loss = -3.1876703957549056e+16, Test Loss = -4.3702886528660275e+17
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.915850157798195e+16
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.892787417396019e+16
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1329300166868992e+16
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.908666957974733e+16
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.747355529786163e+16
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.622706613937766e+16
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.016094725873336e+17
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.315975699031654e+16
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9111167916271206e+17
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.525494913892352e+16
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.631110924173312e+16
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.687964964021862e+16
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5503855907241984e+16
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.660153949782016e+16
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 501949875093504.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.77067033025577e+16
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.128111278129152e+16
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1136800936493056.0
  1%|▏         | 2089/139870 [00:09<08:54, 257.81it/s, epoch=5, test_loss=-4.37e+17, train_loss=1.14e+15] Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4141303203273114e+17
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.930627609768755e+16
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.178940042988749e+16
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.24325588975616e+17
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.627151475592397e+16
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7086695881467494e+17
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.775894111485952e+16
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.880103629003162e+16
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0695824702963712e+16
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.711708589719552e+16
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 434055300513792.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.49402220959105e+17
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.65356963193815e+16
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.148878387419546e+16
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.839234087170867e+16
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.774023090752717e+16
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5529234067816448e+17
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.687246525018931e+16
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.871072171773133e+16
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.359970767031501e+16
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.905347728782131e+16
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.727400682232218e+16
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.982298939837645e+16
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.63916522513367e+16
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.583164138533683e+16
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4236221980514714e+17
  2%|▏         | 2115/139870 [00:09<09:02, 253.86it/s, epoch=5, test_loss=-4.37e+17, train_loss=-3.42e+17]Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.622261600813056e+16
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.558748537946112e+16
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.021387827875676e+17
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2518077585489920.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1357920348405760.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.64225835155456e+16
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.412422680150016e+16
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.450987184067707e+17
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.526604534386524e+17
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1933780033994752.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.964188671711642e+16
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.177399785947136e+16
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.528004490539827e+16
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.894318627749888e+16
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.021610291488358e+16
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.721367596184371e+16
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.316730297529139e+16
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.785821982903501e+16
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.096820885277901e+16
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.454982237100442e+16
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.718348851163955e+16
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1849450767056896.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3466293357248512e+16
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3460414620762112e+16
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.95397602000896e+16
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.75899046321193e+16
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.221102744961024e+16
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.723303579949466e+16
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3995503922511872.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.034093801426125e+16
  2%|▏         | 2145/139870 [00:09<08:41, 264.31it/s, epoch=5, test_loss=-4.37e+17, train_loss=-5.03e+16]Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.572177854195302e+16
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.594120760341299e+16
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4976804091658240.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.23761641020457e+16
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.28555855314944e+16
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.481164732720742e+16
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3568830605295616e+16
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.901647075934208e+16
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.869500213742797e+16
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.750324453185946e+16
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.965423638347776e+16
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.671272628138803e+16
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2390940916383744.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.767529771702682e+16
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.490356915887145e+17
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4417598019928064e+16
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7355557084358246e+17
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.521341762286387e+16
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.332553655805542e+16
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.703506974683955e+17
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.28295266351186e+16
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.325516648408678e+16
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.583987108780442e+16
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5817153915518976e+16
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.731519719407616e+16
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.728721626680525e+16
Running validation...
Epoch 6, Step 200: Train Loss = -5.757109455028224e+16, Test Loss = -7.648594583827251e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5611429590872883e+17
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.959443404351078e+16
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 764141925040128.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.686015806270341e+17
  2%|▏         | 2175/139870 [00:09<08:25, 272.26it/s, epoch=5, test_loss=-7.65e+16, train_loss=-5.69e+17]Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.6864326055899955e+17
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.695004899429581e+16
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.718369576032666e+16
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.02112697468846e+16
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.285259248441754e+16
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1992584176074752.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.787172266108518e+16
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.491288345621299e+16
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.63502973410345e+16
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2168297604644864e+16
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.054140857797837e+16
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.296801489040179e+16
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.432959683264512e+16
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.432478029938688e+16
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7111578130251776e+16
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.061087350410445e+16
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.5003845238784e+16
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.627006797967524e+17
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.482261479876198e+16
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7325301872787456.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 921641731227648.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.609628009024717e+16
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.691952974135296e+16
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.447530922292019e+16
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.252409245591142e+16
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.050530453775974e+16
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.577137682428723e+16
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5754685218553856e+16
  2%|▏         | 2203/139870 [00:09<08:32, 268.56it/s, epoch=5, test_loss=-7.65e+16, train_loss=-1.58e+16]Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.668029927830323e+16
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.522165295008973e+16
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.302400025990267e+17
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.06771706192855e+16
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.607519851583898e+16
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.158142764856115e+16
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.726334241852621e+16
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7621840216614502e+17
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5208422501567693e+17
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.088746268295168e+16
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.564376100614963e+16
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.179142257482138e+16
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.878965837653606e+16
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.179114769691443e+16
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.449249361833492e+17
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.868768351315558e+16
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.076542228576338e+17
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.746874102771876e+17
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2997224946466816e+16
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.448305385013248e+16
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.88077192591442e+16
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.195696912406938e+16
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.846362365925786e+16
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0331299876385587e+17
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.9903739623637e+16
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1719600810177331e+17
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.883014945328333e+16
  2%|▏         | 2230/139870 [00:09<08:48, 260.28it/s, epoch=5, test_loss=-7.65e+16, train_loss=-6.88e+16]Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5067411696359834e+17
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.324224050744525e+16
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.540871650082816e+16
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.774880069707366e+16
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.750070917136384e+16
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.975631245115392e+16
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.815870518999122e+17
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.824865211309097e+17
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.998968426494689e+17
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -927128820383744.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.749909613857997e+16
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1301780378668237e+17
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5923744150755738e+17
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.930602620492186e+16
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.798860699133542e+16
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1027837056607846e+17
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.503331863416013e+16
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4309308893862298e+17
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4086080545633075e+17
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0729995677519053e+17
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.641926077191291e+17
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.804831750160384e+16
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1140713092114022e+17
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.186163357806756e+17
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.531056790817997e+16
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1490786427482931e+17
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.215016269166674e+17
  2%|▏         | 2257/139870 [00:09<08:57, 255.87it/s, epoch=5, test_loss=-7.65e+16, train_loss=-1.22e+17]Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9085467145836954e+17
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5216804559742566e+17
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.31357404665217e+17
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4846590404735795e+17
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.890476295494042e+16
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5637999976552858e+17
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.235605225692201e+17
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0101729579447091e+17
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.280385803563827e+16
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.54928028254208e+16
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6622985599320064.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2123508826688717e+17
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.166130224562176e+16
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6195434762469376e+16
Running validation...
Epoch 6, Step 300: Train Loss = -1.6818221126516736e+16, Test Loss = -8.976802053200282e+16
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0382306907992883e+17
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.632389812631634e+17
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.09700873888727e+16
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.095959775721554e+17
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.293859436442419e+16
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1290364140847104e+16
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.755735385964544e+17
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.208551226694697e+17
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.441150506369024e+17
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0078181132756582e+17
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.18741127211778e+17
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.967820792574771e+16
Batch 314/394
  2%|▏         | 2283/139870 [00:10<08:56, 256.25it/s, epoch=5, test_loss=-8.98e+16, train_loss=-6.97e+16]Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2182586258777702e+17
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.562295134447206e+16
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.345400816494182e+16
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1876623146090496.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.13382766650327e+16
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1017255116183962e+17
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.570822659034317e+16
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0425758233133056e+17
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5713540233035776.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.266943836984443e+17
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4258063062073344e+17
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2511141807993651e+17
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.773459216059597e+16
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.368504921561498e+16
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.326193645107282e+17
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.05014097870848e+17
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.333010319854797e+16
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6483531290260275e+17
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.513020255717294e+17
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.02705574754386e+16
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.73887751378174e+17
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1333016816818586e+17
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2369364937552691e+17
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2007809164050432.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.330004701741056e+16
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.221003101719757e+16
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4122111505334272.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.202962489703137e+17
  2%|▏         | 2311/139870 [00:10<08:45, 261.77it/s, epoch=5, test_loss=-8.98e+16, train_loss=-9.2e+17] Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3661803060291174e+17
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2973842929759027e+17
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.813680241783603e+16
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2116523648811008e+17
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.928324179988316e+17
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.460061543890944e+16
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3716720011247616.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2889640954914406e+17
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.72994021792809e+16
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4920915672786534e+17
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1084513445045862e+17
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.429681443753165e+16
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7694491892842496.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2031544284885811e+17
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1625223198815027e+17
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.164048586892902e+16
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 318065950588928.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.659631736271667e+16
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5513995680782746e+17
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.861955674190643e+16
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.828532051201229e+16
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.540084047240233e+17
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1367021790887936e+17
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4562797863672218e+17
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4021631670511206e+17
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5389472552963277e+17
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6664275539984384e+17
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0734859813881119e+18
  2%|▏         | 2339/139870 [00:10<08:37, 265.93it/s, epoch=5, test_loss=-8.98e+16, train_loss=-1.07e+18]Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.630919997183754e+17
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1424798236082176e+16
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1455013129478472e+18
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5817306271226266e+17
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.685382509212467e+16
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0876341377236992e+17
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2212713736372224e+17
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3147108295114752.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5632919732302643e+17
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.495460476852634e+16
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.134463985731502e+17
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2726231895257907e+17
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6090393878868787e+17
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0410148603992474e+17
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2075565122689434e+17
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.49161758212096e+16
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.658794834637619e+16
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0756598490202112e+16
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.450419584316211e+16
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.245880255186207e+17
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9497403199586304.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.384676949413069e+16
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1598639928233165e+17
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3547635098622362e+17
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4528936341510554e+17
Starting epoch 7/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5794017240560435e+17
Running validation...
Epoch 7, Step 0: Train Loss = -1.5088749250818867e+17, Test Loss = -1.1334657064828928e+16
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.266505178924712e+18
  2%|▏         | 2366/139870 [00:10<08:39, 264.64it/s, epoch=6, test_loss=-1.13e+16, train_loss=-1.27e+18]Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3348163073500774e+17
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.402322885321687e+17
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.789136542144922e+16
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.33655331684352e+16
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1177724543290573e+17
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.681462813032448e+16
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.149159113255551e+18
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4361989244728115e+17
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.620960343008215e+17
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0191256297812787e+17
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6185810872316723e+17
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9939969787232256e+17
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8264396529873715e+17
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.358417089868595e+16
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.908001347069542e+16
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.623358698998989e+16
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5080561325165773e+17
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.345927941960499e+16
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3011617167127347e+17
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.562859977159475e+16
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.079129415282852e+17
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4883604432892723e+17
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1832273436032369e+18
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0649730813295329e+18
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.631346604722094e+17
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0821656507318272e+16
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6743928128536576e+17
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.437601282834563e+18
  2%|▏         | 2394/139870 [00:10<08:34, 267.03it/s, epoch=6, test_loss=-1.13e+16, train_loss=-1.44e+18]Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.796392139423744e+16
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7442621539273933e+17
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8037242094223360.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5246412346294272e+17
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7434981651447808e+17
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.53134923927126e+17
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1475614026013082e+17
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -803105197260800.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3471348404906885e+18
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.864675169843282e+17
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.704212786483036e+17
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.394363366029394e+17
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7291263455789056e+17
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.993400118681272e+17
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.868980788658176e+17
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.804704369690542e+17
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6179116034228224e+17
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.876760177022075e+17
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5554865215181947e+18
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.703761814916956e+17
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6963031747107226e+17
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.7469987815424e+16
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8093865712379494e+17
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1439864436228096e+17
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4593707727375565e+17
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.771174762601513e+17
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.552328449245315e+18
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5820636417068892e+18
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.959023169057587e+16
  2%|▏         | 2423/139870 [00:10<08:24, 272.20it/s, epoch=6, test_loss=-1.13e+16, train_loss=-5.96e+16]Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.7306384705847296e+17
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3155355635031736e+18
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7746808303045837e+17
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.436565417335194e+16
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.547820048174285e+16
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.191255861710029e+16
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.413725406503895e+17
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.47926157819904e+16
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4647446515115622e+18
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.04202561404928e+16
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.792785835444142e+17
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3242139228818637e+17
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.055696058022625e+17
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.714287046079283e+16
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6619482310128435e+17
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3016098536003994e+17
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5197055269323407e+18
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1174016268527206e+17
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5751324578443428e+18
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.709872522587013e+18
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.603810756646666e+17
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7478137482838016e+17
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.423205008054682e+16
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1072747270635520.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5733887698416435e+18
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5341263091253903e+18
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5661620421761434e+17
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0373694639570944e+17
  2%|▏         | 2451/139870 [00:10<08:53, 257.70it/s, epoch=6, test_loss=-1.13e+16, train_loss=-1.04e+17]Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9193420289828454e+17
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.455612863845499e+17
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2381930136941363e+17
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.265544380533965e+17
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0597570668868403e+17
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.261977441581138e+17
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4087413016286986e+18
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.482270085504041e+17
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.438404288586711e+17
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5891505969299456e+16
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.265996453098291e+16
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.662372417676247e+17
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2792786708476723e+17
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2423419677953229e+17
Running validation...
Epoch 7, Step 100: Train Loss = -1.4302923136486605e+17, Test Loss = -1.6840606281315123e+17
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.024422183450706e+17
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5395114422003302e+17
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.545492121387008e+16
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3377132627912294e+17
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7188390399115264e+17
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.368731500909691e+17
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.565782898156503e+18
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8369283067215872e+17
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7710993773355336e+18
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9558611051085824e+17
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.234295174767575e+17
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.937658346513367e+17
  2%|▏         | 2477/139870 [00:10<09:19, 245.42it/s, epoch=6, test_loss=-1.68e+17, train_loss=-1.94e+17]Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.068616803064218e+16
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4664856594546688e+17
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2516071298891776.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.86547609534464e+17
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.454498514757222e+16
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1947596406915072.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6702422752312689e+18
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6001821345303757e+17
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.645400940305449e+17
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9263578408809923e+18
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.253569168412508e+17
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.936238867255132e+17
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.975185537362821e+17
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.288234177362657e+17
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.507118064579379e+16
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7898666320723968e+17
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6084635071086592.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4606539185004216e+18
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.902616551645512e+17
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.694011879864402e+17
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0985200057266995e+17
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.122023409773445e+17
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.031983815486079e+18
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.251222554387415e+17
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.729222177862451e+17
  2%|▏         | 2502/139870 [00:10<09:55, 230.55it/s, epoch=6, test_loss=-1.68e+17, train_loss=-3.73e+17]Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.977302956239749e+17
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.884741928551711e+17
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4068766516536934e+17
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.784848876293325e+17
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.313955190310175e+17
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3521510373615206e+17
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4197919432436613e+18
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9759079508620083e+17
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.22790038226731e+17
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1963115849999974e+18
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0623958088941568e+16
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5193194744578048.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.197863442583388e+17
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.63744698877739e+17
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.065401606239683e+17
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2587610436523786e+18
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6702715224719360.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.843619178573988e+17
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.092174402165146e+16
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.895192802266972e+17
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.396812856084398e+17
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3829322089784934e+17
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7157225959482982e+17
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4169459604743782e+17
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3902851929892454e+17
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1956688158877286e+17
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9255705591690035e+17
  2%|▏         | 2529/139870 [00:10<09:30, 240.79it/s, epoch=6, test_loss=-1.68e+17, train_loss=-3.93e+17]Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.896590900021166e+17
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9446290941280256.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.034038208856064e+16
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.107914598311526e+16
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.747690880832635e+17
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7664107691835392e+17
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.018369699843932e+17
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.006885300891812e+17
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6565252921491456e+16
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.774081252870062e+17
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0079820637405184e+17
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.552535157431337e+17
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.10349244547072e+16
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.325074189138002e+17
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.05147618244821e+17
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0909340313033114e+17
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.207154221678592e+16
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.507918865361797e+17
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1246192880071475e+17
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7308393503352422e+17
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.559468921247498e+17
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.700641247785124e+17
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9904807691157504.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.279405770493133e+17
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.833877458472141e+17
  2%|▏         | 2554/139870 [00:11<09:24, 243.29it/s, epoch=6, test_loss=-1.68e+17, train_loss=-4.83e+17]Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3174802731551949e+17
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7751299120850534e+18
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8279186522321715e+17
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7987061901636403e+17
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2128955188817428e+18
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.152752010990715e+17
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.729188521012101e+17
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.304482554039501e+17
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4648879316205568e+17
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.752328414720164e+17
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.271770380819497e+17
Running validation...
Epoch 7, Step 200: Train Loss = -2.0171052771652403e+17, Test Loss = -1.8588406801099653e+18
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5120486713419366e+17
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.246298991371223e+17
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3573744308060160.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.009353101714129e+18
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0334915051124163e+18
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.676341353616507e+17
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.480447020559237e+17
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.301165808494838e+17
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.065871350942925e+17
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8751518240997376.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1796751150782874e+17
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.117037655630807e+17
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4920869130207232e+17
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.508330213376e+16
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.676285846945792e+17
  2%|▏         | 2580/139870 [00:11<09:21, 244.36it/s, epoch=6, test_loss=-1.86e+18, train_loss=-3.68e+17]Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.314319591028818e+17
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.959569036274565e+17
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.737720731243643e+17
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.545329826136064e+16
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.692879538993234e+17
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3591653904574054e+17
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.023350096840622e+17
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.316048729168937e+17
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.848454335469978e+16
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4681375839944704.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6054020659832422e+17
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.787744652269978e+16
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1143118273799782e+17
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.307451235434496e+17
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.375018286545633e+17
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.201011653611356e+17
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.703824989755802e+16
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9037098785203814e+17
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2853921272967987e+17
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.449435560228946e+17
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6185106340747674e+17
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0652109724647424e+17
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8253521515747738e+17
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4412334851371827e+17
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.053467781909709e+17
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.887047763513836e+17
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.86490569230123e+17
  2%|▏         | 2607/139870 [00:11<09:09, 249.83it/s, epoch=6, test_loss=-1.86e+18, train_loss=-3.86e+17]Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.515837551078277e+17
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0329330592579584e+17
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.56101804574507e+17
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7046051433086976e+17
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5224138145367327e+18
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3341831118848e+17
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.935321509320458e+17
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.68588727920912e+18
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.20568289886208e+16
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.478731766726656e+16
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.52536894250156e+17
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2448596619309875e+17
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3999534922766746e+17
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.570578783358157e+17
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.965002482114888e+17
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.9903197524433306e+17
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9362677922083635e+17
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1039410103684956e+18
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7134421244195635e+17
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7937575131519386e+17
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.736880479535104e+17
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1292719401795584e+17
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4388092024101274e+17
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6147387064094556e+18
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6867864048427336e+18
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.494673685923758e+18
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3893613339607040.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5992458316598477e+17
  2%|▏         | 2635/139870 [00:11<08:52, 257.57it/s, epoch=6, test_loss=-1.86e+18, train_loss=-2.6e+17] Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3420793076658995e+17
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.075164910203699e+17
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.612311450078413e+17
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.179847413472952e+17
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.308929375685837e+17
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3163947474275533e+17
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.195344998479954e+17
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.802323520646021e+17
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.500128622293811e+17
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.625163451030307e+18
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1426522938015744e+17
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.519765181384622e+17
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.984049269016625e+18
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2867694217160294e+17
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3264658989540966e+17
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.1451942115527885e+17
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.612269140520468e+17
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.434139323288781e+17
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.323602648929665e+18
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.02459081129984e+17
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.193281415366574e+17
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.664121421691617e+17
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.147486349699318e+17
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2601303008660685e+17
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.263234375323484e+17
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.411355615348654e+17
  2%|▏         | 2661/139870 [00:11<09:06, 251.27it/s, epoch=6, test_loss=-1.86e+18, train_loss=-2.41e+17]Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.7086765303005184e+16
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.187051541350318e+17
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.063687445972255e+17
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.005706644579942e+16
Running validation...
Epoch 7, Step 300: Train Loss = -6.576122352159949e+16, Test Loss = -1.1945339137294336e+17
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5249992317167206e+17
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6820896190730076e+18
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.61701976502698e+17
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.421851625040118e+17
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7865299578794803e+17
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.343663635202048e+16
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.141533852931195e+17
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2624269057785856e+17
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.559015684911923e+17
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.623414096644014e+17
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3650960187386757e+18
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1805390750063e+17
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.384335944703017e+17
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1836778527994675e+17
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.787171610300252e+17
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5180553045213184.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1131677182984192e+17
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.071511829900165e+17
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.615163995557724e+17
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7862902673178624e+17
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.926169137197875e+16
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5987773179607122e+18
  2%|▏         | 2687/139870 [00:11<09:04, 252.15it/s, epoch=6, test_loss=-1.19e+17, train_loss=-1.6e+18] Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.977551314375148e+17
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.444316693486633e+17
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.883288495925494e+17
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3720575235837133e+17
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3781883003142144e+17
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.13272026782892e+17
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.846586454192947e+17
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.476364005769216e+17
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.880392282191954e+17
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.69400894359339e+17
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1675731528552284e+18
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5610614794276045e+17
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.159327059138314e+17
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7980170031923200.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.318373509066916e+17
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3487162661155635e+17
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7457484323160064.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.175339828115931e+18
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.276709889727529e+17
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.679665111521034e+17
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.64626196396245e+17
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.560240281680609e+17
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.122282091523604e+17
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5488301468431155e+17
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0344746559995904e+16
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4940417789486694e+17
  2%|▏         | 2713/139870 [00:11<09:15, 246.79it/s, epoch=6, test_loss=-1.19e+17, train_loss=-4.49e+17]Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7012585047916544e+17
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4779844267239014e+17
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.450257507943711e+17
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6158443183774106e+17
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.728566765853082e+16
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.87507823644246e+17
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.167520841640182e+17
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.315153673677701e+17
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -550958404730880.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.730247847439237e+17
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.765461018935296e+17
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9869580270279066e+17
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.929137100011602e+17
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.3454857164344525e+17
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.351592144830464e+17
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.859428031424102e+17
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3764942748534702e+18
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.9787793471176704e+17
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.505175333585551e+17
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.429102217030533e+18
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.10777122784333e+18
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.086306836723794e+17
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.142483724925141e+18
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5511641410974515e+17
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9214040972945e+17
Batch 375/394
Batch data shape: (1, 28, 28, 1)
  2%|▏         | 2738/139870 [00:11<09:25, 242.64it/s, epoch=6, test_loss=-1.19e+17, train_loss=-3.92e+17]Train Loss: -7.451344259388539e+17
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.30427053876052e+17
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2397912726175744e+16
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1402044156810363e+18
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0021909173305344e+17
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.347713617563484e+17
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.402706316048466e+17
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0541342331418706e+18
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.809972029793239e+17
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9494419874001715e+17
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1962583117122765e+17
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.657901528519803e+17
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.779441699959603e+16
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.299666193407017e+17
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2018907818772398e+18
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.473521299895091e+16
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2130640190386995e+17
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1125198340449894e+17
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.188332519476429e+17
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.323128506179912e+17
Starting epoch 8/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.897084945979146e+17
Running validation...
Epoch 8, Step 0: Train Loss = -4.947328847797289e+17, Test Loss = -1.2969096990896947e+18
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.642305218237694e+18
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.745284652661473e+17
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.756810626897019e+17
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4544126781843046e+17
Batch 6/394
Batch data shape: (1, 28, 28, 1)
  2%|▏         | 2763/139870 [00:11<09:38, 236.82it/s, epoch=7, test_loss=-1.3e+18, train_loss=-1.45e+17] Train Loss: -1.7975322897022976e+17
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.139752659787448e+17
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8878537339502592e+17
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.28182153497831e+18
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.258810871219487e+17
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5782004605229466e+17
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.780887494977126e+17
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.538165102105068e+17
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.496954391197123e+17
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.078266579479429e+17
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5893435425541325e+17
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7561526692937728e+17
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.403489109354742e+17
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.297027474526044e+17
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9025207036687155e+17
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4676668844867584e+17
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8234939769238323e+17
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.78414650046808e+17
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.910384569909248e+17
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.297922783255462e+18
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1285116809151775e+18
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.334186157181501e+17
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.158219832262656e+16
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.487407881488957e+17
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.382505242149519e+18
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.925735845203804e+17
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.574495418548224e+17
  2%|▏         | 2790/139870 [00:12<09:21, 243.97it/s, epoch=7, test_loss=-1.3e+18, train_loss=-6.57e+17]Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3899671936106496e+16
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.334513261890765e+17
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.881193879194829e+17
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8161700082255135e+18
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.047380985644319e+17
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4017568243253248.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.985985065289777e+18
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.506316764094136e+17
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4595631401926656e+17
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.0207745065458074e+17
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5664463376246374e+17
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.409353454727463e+18
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.540730790848758e+17
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.930877701584323e+17
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.62517091006677e+17
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.558165937195581e+17
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.434399991957291e+18
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.8820748942730854e+17
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.012572493399982e+17
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.647743227976745e+17
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.757246948494541e+17
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.956938251519918e+17
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.692115971447194e+17
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.814075925557412e+17
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.238409845282963e+18
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.542224699246772e+18
  2%|▏         | 2816/139870 [00:12<09:15, 246.69it/s, epoch=7, test_loss=-1.3e+18, train_loss=-5.54e+18]Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1126528376189747e+17
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4979364336529572e+18
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.408633158812041e+18
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.222837240247091e+17
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.900031668528087e+17
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9496110530061926e+17
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3209694186871194e+17
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.110648539920466e+17
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4970862549532672e+17
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.581028060832203e+18
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7518894847557632e+17
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.62514854485033e+17
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4599582771838976e+17
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.498426690292941e+17
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0824736643114598e+17
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.896370903536108e+17
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.679208814195507e+17
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.906923910087049e+18
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.442331747094364e+17
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.793502360708055e+18
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.650010373873467e+18
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.023186713311445e+18
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.767748331025203e+17
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5620084652035277e+17
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1179784029667328.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.53520541701505e+18
  2%|▏         | 2842/139870 [00:12<09:07, 250.33it/s, epoch=7, test_loss=-1.3e+18, train_loss=-5.54e+18]Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.566781741697335e+18
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0487875829739028e+18
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.381152171345838e+17
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.028394434337178e+17
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.308204041608888e+17
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.691241891089613e+17
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.064139307918295e+18
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.557600079384412e+17
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.917752328607498e+17
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.198055569520329e+18
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4047385825740063e+18
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.842468720373924e+17
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.25691037131735e+16
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.578129898340352e+17
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.7415006146750054e+17
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.576139844240015e+17
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.835197231716106e+17
Running validation...
Epoch 8, Step 100: Train Loss = -4.1198092525397606e+17, Test Loss = -5.1634210219648614e+17
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.01969729736147e+17
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.418919349274542e+17
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.679387875512156e+17
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.9791689865507635e+17
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.456283846762824e+17
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.580874288282993e+17
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.570224862359716e+18
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.12923581537452e+17
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.607334257457103e+18
  2%|▏         | 2868/139870 [00:12<09:02, 252.74it/s, epoch=7, test_loss=-5.16e+17, train_loss=-6.61e+18]Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.165341028451615e+17
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.075264928428851e+17
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.894945661588603e+17
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2974042918302515e+17
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.273939807620301e+17
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7849824753811456.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.359107962614579e+17
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5287348538585907e+17
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.3709377768783872e+16
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.781528785134027e+18
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.837341888118784e+17
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.080071152938516e+17
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.256766519812817e+18
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.203086216147763e+17
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.395271787458855e+18
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.410398134159278e+17
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.227956163815932e+18
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6475369134410957e+17
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.7517703811825664e+17
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.5815004011823104e+16
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.705672822125494e+18
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.156313772856443e+18
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.686016430692106e+17
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.883898365588275e+17
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0284151443397018e+18
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0565090624586383e+18
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.161707976902312e+18
  2%|▏         | 2895/139870 [00:12<08:52, 257.32it/s, epoch=7, test_loss=-5.16e+17, train_loss=-1.16e+18]Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.368466877898031e+18
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.000904582058476e+17
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.329758170618266e+17
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.204322838824878e+17
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.199780903476265e+18
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.083324273994498e+18
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.945437984315146e+17
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.866441763071459e+18
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.809332907273093e+17
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.611500497479926e+17
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.32491787694139e+18
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0989073803902976e+16
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6359624919744512e+16
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.59250126445609e+17
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.009316008889942e+18
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.935650913158955e+18
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7089347452209725e+18
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.140444196601856e+16
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.121178456727224e+17
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5698971332286874e+17
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.998655393584906e+17
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.865614127333376e+17
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.939385500708372e+17
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.692689451173806e+17
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.995613900931072e+17
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.069904402873057e+17
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6369164587237376e+17
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1861853047598612e+18
  2%|▏         | 2923/139870 [00:12<08:45, 260.72it/s, epoch=7, test_loss=-5.16e+17, train_loss=-1.19e+18]Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.768229367362355e+17
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.088593660674048e+16
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3437533455056896e+17
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.558537460126843e+17
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2838916188557804e+18
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.287301216306463e+17
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1150622268860662e+18
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0133424891903345e+18
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.51180626338775e+16
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.445248513897923e+17
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.89717043895468e+17
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.899405259111137e+17
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.363779840547226e+16
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.504184726635151e+17
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.704089356556042e+17
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1794650896908943e+18
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5971766882153267e+17
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.89106029376045e+17
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.88821079357784e+17
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.651574619629158e+17
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5108312311456072e+18
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.263497148855747e+17
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.0762557228711936e+16
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2066742916266066e+18
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5601066693781094e+18
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.415538710309765e+17
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.03448782172586e+18
  2%|▏         | 2950/139870 [00:12<08:59, 253.71it/s, epoch=7, test_loss=-5.16e+17, train_loss=-5.03e+18]Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.255305427938181e+17
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.42263155971457e+17
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.378680697004753e+18
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.179211377382785e+18
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.124583310387839e+18
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1696711898664796e+18
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.420167997860086e+17
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4305349964807537e+18
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.418090217401221e+17
Running validation...
Epoch 8, Step 200: Train Loss = -7.502588373190574e+17, Test Loss = -1.0578386252548014e+18
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1029579625605366e+18
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.470693602453094e+17
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1871492543348736e+16
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.101037518316896e+18
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.230012103374143e+18
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.025825668910285e+17
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.209776426245423e+18
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0498223608545935e+18
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.00641652800802e+18
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.976886715265843e+16
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.41539641391317e+17
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.195546315160289e+17
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.512407012026614e+17
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.271694430104453e+17
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1371501785375048e+18
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.276999183031337e+17
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.467354882369126e+17
  2%|▏         | 2976/139870 [00:12<09:23, 242.81it/s, epoch=7, test_loss=-1.06e+18, train_loss=-6.47e+17]Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.240180630399222e+17
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.116058403087319e+17
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.176271558168019e+18
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.195495007151194e+18
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9843242313362964e+18
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.974012198550241e+17
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1648619603663258e+17
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6814801258807296e+16
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.419730519924408e+17
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.360509902712996e+17
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7074205802705715e+17
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.194698548415824e+18
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.966358910426153e+17
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.313201618474762e+17
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2711733960586035e+17
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1331248664682168e+18
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.403887963144192e+17
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7376091308134236e+18
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.181401054789501e+18
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.255295479307305e+17
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.027618919348961e+17
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.718177224270807e+17
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0278521099735532e+18
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9151685233619763e+18
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1541260195950756e+18
  2%|▏         | 3001/139870 [00:12<09:37, 237.02it/s, epoch=7, test_loss=-1.06e+18, train_loss=-1.15e+18]Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0945636880120545e+18
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.461074206820139e+18
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.899200053006828e+17
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3180857432848466e+18
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.156181103133065e+18
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0076114222695055e+18
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3935509987351593e+18
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.820215979417272e+18
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5555998742950707e+17
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3588097985517978e+17
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0834348436325663e+18
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.107534532525425e+17
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.20716056253694e+17
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2846876652742902e+18
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.481223856910762e+18
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3644609447211827e+18
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0219215661050102e+18
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0578963182287585e+18
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.212084576030032e+18
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.646903366118277e+17
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.858550483706184e+17
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.061750275309568e+18
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2556808992661176e+18
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.94440119787638e+18
  2%|▏         | 3025/139870 [00:12<09:37, 237.00it/s, epoch=7, test_loss=-1.06e+18, train_loss=-8.94e+18]Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.868979648013271e+18
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.15129652272667e+18
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3694427788804096.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0498013326947123e+18
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6273967809980006e+18
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.119747779994583e+18
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1450677617691197e+18
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0047083679747932e+18
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5570533255877755e+18
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.995497374338253e+17
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8843039950197228e+18
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9378562586063667e+18
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0729092875393434e+18
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.113319940859101e+18
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2572511393095352e+18
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5817095989627453e+18
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1035852084057473e+19
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.929299183817196e+17
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2543555754377871e+18
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7721785479981957e+18
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.361760872067498e+18
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9487137984917012e+18
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.179482846803683e+19
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0191951051722588e+18
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.248205066470687e+17
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.291223215416017e+18
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5551152989048668e+18
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2481580407088742e+18
  2%|▏         | 3053/139870 [00:13<09:12, 247.62it/s, epoch=7, test_loss=-1.06e+18, train_loss=-1.25e+18]Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0197658361198019e+18
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.839013864374272e+17
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.801919574841754e+16
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5234314125482394e+18
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1200228110337311e+18
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.907806262221865e+17
Running validation...
Epoch 8, Step 300: Train Loss = -1.866405869864878e+17, Test Loss = -1.3831711966520934e+18
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1129959696595681e+18
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2102558682907148e+19
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2139383526344622e+18
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5397558086596035e+18
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.808238893203456e+17
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.531626054043566e+17
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9031746382093353e+18
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3747438523420508e+18
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.857003808496812e+18
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3676970823196344e+18
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0797417490016109e+19
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.353603829045658e+17
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.459427550718591e+18
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.765507388888842e+17
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.442929528075715e+17
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4290318266466304e+16
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.896658869530132e+17
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1737501031276216e+18
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1635874545910415e+18
  2%|▏         | 3078/139870 [00:13<09:16, 245.62it/s, epoch=7, test_loss=-1.38e+18, train_loss=-1.16e+18]Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.34375315504051e+18
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.141100764122317e+16
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.05574138149077e+18
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6599537326133084e+18
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.687417196612944e+18
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.316281719581573e+18
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.823333094882017e+17
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.483520736701186e+18
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1454875003330232e+18
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3495540409497027e+18
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9709593926227722e+18
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.950254076843262e+18
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1690373214130668e+18
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.280083747103834e+18
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3775613508882268e+18
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6684723363884564e+18
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.4601256991391744e+16
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1580048217399624e+18
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.295856135351828e+17
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.624433565774643e+16
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.863407650987835e+18
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.466026407191642e+18
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3460982759036027e+18
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.932873970997002e+17
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3294236323125658e+18
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0625054179461693e+18
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3097371731348685e+17
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.341515722057318e+16
  2%|▏         | 3106/139870 [00:13<08:59, 253.35it/s, epoch=7, test_loss=-1.38e+18, train_loss=4.34e+16] Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3339866055678362e+18
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1297386455325737e+18
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6861821701770445e+18
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4148771262675026e+18
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.173228384860242e+18
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.475941006003405e+16
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.399551033687933e+18
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2726959791449047e+18
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.416471873724088e+17
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.623950139949056e+16
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.344889574973768e+17
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9823045659150254e+18
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.242527728433889e+17
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.22487519479595e+18
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5670039058191483e+18
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.443453158595494e+18
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6319341906079252e+18
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4136477034421944e+18
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4012137701470372e+18
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7209063966496522e+18
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2652420049911415e+19
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4928914306013594e+18
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.541961947354235e+17
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2087109445025268e+19
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3866713544801649e+18
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0373745835581112e+18
  2%|▏         | 3132/139870 [00:13<09:00, 252.77it/s, epoch=7, test_loss=-1.38e+18, train_loss=-1.04e+18]Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4682782600315535e+18
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.268649501476782e+18
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.200421321919693e+16
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1670679272622653e+18
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.687909715049185e+17
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2712213966131036e+18
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0658974643440845e+18
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.001367126909911e+18
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3968606661737185e+18
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.224811972877353e+18
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.592031910429983e+17
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.207001196123914e+17
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1273056496386048e+17
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.960723288912036e+17
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.517078646467854e+18
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1872548890555187e+17
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.556614969992151e+17
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.448685184676266e+18
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4049378160303145e+18
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7010151317915566e+18
Starting epoch 9/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3921936516306698e+18
Running validation...
Epoch 9, Step 0: Train Loss = -1.5252103693357548e+18, Test Loss = -3.2091333179956265e+18
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5366704141053198e+19
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3757023516035645e+18
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4332810267711242e+18
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3402871037126246e+17
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.034577844240384e+17
  2%|▏         | 3158/139870 [00:13<09:04, 251.14it/s, epoch=8, test_loss=-3.21e+18, train_loss=-5.03e+17]Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.533549065520808e+18
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.692098854351012e+17
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3617106263354638e+19
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6869684584298578e+18
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8374032270253097e+18
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.135490037725266e+18
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.87521301102959e+18
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1373120659314442e+18
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0876902815361925e+18
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.1425110595836314e+17
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.903337324796969e+17
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0083131168464568e+18
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8215512930097562e+18
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.752748618627154e+17
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5401821442932736e+18
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.511595075715727e+17
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.225016397627392e+18
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4651476225231421e+18
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.209980440627957e+19
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2854361853026894e+19
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8736280119919247e+18
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2338235014591283e+17
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.709745856898138e+18
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3933089412503044e+19
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.629475091719127e+17
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9458585042333204e+18
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.395114578935808e+16
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6155795049005711e+18
  2%|▏         | 3186/139870 [00:13<08:50, 257.82it/s, epoch=8, test_loss=-3.21e+18, train_loss=-1.62e+18]Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0583770266617774e+18
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.005981883264139e+18
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0592332183156818e+18
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9199785588293632.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5622199857022763e+19
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6932690723738747e+18
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.756848057372115e+18
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4177358564997202e+18
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7151395956009206e+18
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.675773252675568e+18
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8748171338173645e+18
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2325546493474243e+18
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.470311478882992e+18
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.199361355377541e+18
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.448116187408892e+19
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5370177498285343e+18
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5829311563812045e+18
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.902834985422029e+17
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.927728657002922e+18
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3997849547867423e+18
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7242781020824535e+18
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.538999756976554e+18
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6144945068262818e+19
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6255668088203117e+19
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.908434279772979e+17
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.234666779797881e+18
  2%|▏         | 3212/139870 [00:13<09:01, 252.23it/s, epoch=8, test_loss=-3.21e+18, train_loss=-4.23e+18]Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.249287981320949e+19
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8531615651135488e+18
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0055950553831178e+18
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.010595371281285e+17
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.175046655134925e+17
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2846918414691205e+18
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.244270485750415e+17
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4411538598794887e+19
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.840851751086326e+17
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0751300104562934e+18
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4000313828303176e+18
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9341322657493156e+18
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8733702295414374e+17
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.685827630703313e+18
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4599878893318963e+18
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8334691744211272e+19
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1982430990258668e+18
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6732858333681156e+19
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4283540052158972e+19
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1689605205258666e+19
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9547366483107512e+18
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.441753082020495e+17
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9165008164356096.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6870606249920561e+19
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6648565374249337e+19
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0052319102815437e+18
Batch 87/394
Batch data shape: (1, 28, 28, 1)
  2%|▏         | 3238/139870 [00:13<09:04, 250.71it/s, epoch=8, test_loss=-3.21e+18, train_loss=-3.01e+18]Train Loss: -1.3240450963073925e+18
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0330556861909565e+18
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.300543722458448e+18
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.397593406260904e+18
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.200570596316414e+18
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2955893230636892e+18
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2257372119121265e+18
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4380893010705514e+19
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.321682014220845e+18
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3294346804550697e+18
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0290274034909184e+17
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.277476814781153e+17
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3564800021320172e+18
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2643720419540992e+18
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4403129533865656e+18
Running validation...
Epoch 9, Step 100: Train Loss = -1.4443373033831793e+18, Test Loss = -2.406591259666809e+18
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.004159558540329e+18
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4806947169398948e+18
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.490474863122186e+17
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2949929754445742e+18
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.56143872771516e+18
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1452314358694543e+18
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6927047480309187e+19
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5027342900797112e+18
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8261299343057224e+19
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8639097035919196e+18
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1858445935942042e+18
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0528387865926697e+18
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.647435645545349e+17
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6807301043317637e+18
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.5541554181505024e+16
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7277489323872092e+18
  2%|▏         | 3269/139870 [00:13<08:34, 265.71it/s, epoch=8, test_loss=-2.41e+18, train_loss=-1.73e+18]Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4247433406211686e+17
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.446733561613517e+16
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7472972595686015e+19
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.520925075792724e+18
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.675702312535851e+18
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8031556388433428e+19
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.328899218292343e+18
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.403764955280835e+18
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0456101848348099e+18
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.962084325228544e+18
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.1135509541001626e+17
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5612571708576236e+18
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.660592707947725e+16
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3710925391529509e+19
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.898553443864019e+18
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.807833838967849e+18
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.321061159188562e+18
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2737837770751345e+18
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.814979661342441e+18
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.178035005993517e+18
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2673777474538045e+18
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9706502924164137e+18
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0382868032252805e+18
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0738123832093573e+18
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.70941798058138e+18
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3113090094195016e+18
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1786568642317517e+18
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5656794890879107e+19
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6311858355062702e+18
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.125599380877607e+18
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9836773061823037e+19
  2%|▏         | 3300/139870 [00:14<08:14, 275.96it/s, epoch=8, test_loss=-2.41e+18, train_loss=-1.98e+19]Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.255931493757747e+16
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.294397166347878e+16
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1986119007642583e+18
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6188123969903657e+18
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.788490564856971e+18
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2631883371727815e+19
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.644257713343693e+16
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7438398727428506e+18
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.84255502609023e+17
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6480519315594936e+18
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4281507585421476e+18
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7017980370967593e+18
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4501649054755717e+18
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.356940285187195e+18
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.587970546075435e+18
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.085232820021887e+18
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.645613320821539e+18
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.811494609906303e+18
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.724391979175117e+16
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5654942135812096e+17
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.974541760584745e+17
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4266114952894874e+18
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.634207758802485e+18
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1151737272103076e+18
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.671735190170108e+18
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.418739367018496e+17
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8320057244445573e+18
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7206307471352136e+18
  2%|▏         | 3328/139870 [00:14<08:26, 269.57it/s, epoch=8, test_loss=-2.41e+18, train_loss=-2.72e+18]Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4199418891418993e+18
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.040497043448791e+17
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.968845306640466e+18
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.989684487960658e+18
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.74834641161447e+18
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.125299532360581e+17
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5640851147642634e+18
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.757217271427367e+18
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9563167839588188e+18
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.797262362570916e+18
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.637275671121887e+18
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.78542002847744e+16
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.37639295095826e+18
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.890544157463937e+18
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2852034736666706e+18
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6973062041931612e+19
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.389738495192072e+18
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8946218121347727e+18
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.036012740250488e+19
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.238227770056114e+18
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9278163960814633e+18
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.237571911370146e+18
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3803222245855724e+18
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6097087686165135e+18
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.900241141186429e+18
Running validation...
Epoch 9, Step 200: Train Loss = -2.1566695178942546e+18, Test Loss = -1.755875814215254e+18
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.450077156650254e+18
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9899999103588762e+18
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.441895308210995e+16
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1999582802402607e+19
  2%|▏         | 3357/139870 [00:14<08:18, 273.62it/s, epoch=8, test_loss=-1.76e+18, train_loss=-2.2e+19] Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9240010926824358e+19
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4554771959051715e+18
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3403133015265116e+18
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2819328075043963e+18
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.047482843602092e+18
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.679041419496653e+16
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1292222716911288e+18
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9268820330495345e+18
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.123215502229635e+18
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.020648702823629e+17
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1257807158834627e+18
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9063603357118628e+18
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8434216787978486e+18
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.342523541750022e+18
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.911690895775498e+17
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3154816560469115e+18
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1989677832410235e+18
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.938571153269326e+18
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.584673110703735e+18
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4055702787076915e+17
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.15768620983255e+16
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2684792676396564e+18
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.42613383215317e+17
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.90483230132011e+17
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.435836947602342e+18
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9588286712986993e+18
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.187618158876033e+18
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.804592997058478e+17
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6439497067024876e+18
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0385084392310047e+18
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.598882089722708e+18
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4962529125253775e+18
  2%|▏         | 3389/139870 [00:14<07:58, 285.29it/s, epoch=8, test_loss=-1.76e+18, train_loss=-3.5e+18]Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0081128525979976e+18
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.021103032724357e+18
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.278393098644226e+18
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.903627246322844e+18
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.594069093371347e+18
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.279721689620939e+18
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.496200410845151e+18
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6257355249808835e+18
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2765968245485732e+18
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5941330868974387e+18
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4398220194954084e+19
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.734781186906784e+18
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0826851357811016e+18
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4540400242565054e+19
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.565162657799209e+17
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.721885342928732e+17
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8117442520720343e+18
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.122356783648342e+18
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.820217363553583e+18
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.470854468801659e+18
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.865160504120967e+18
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2165649701139907e+18
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.686122299819557e+18
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.13645949096978e+18
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.98202864152268e+18
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7535383055208284e+18
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.578731296841007e+18
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.643752069487395e+18
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.535744896293929e+18
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5342573139864846e+19
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5776491005720134e+19
  2%|▏         | 3420/139870 [00:14<07:50, 290.10it/s, epoch=8, test_loss=-1.76e+18, train_loss=-2.58e+19]Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3915857846778986e+19
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4824399324643328.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7598591229910057e+18
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.449145489371169e+18
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.804611054587609e+18
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.305779290565509e+18
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6812536623317647e+18
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.622247049463857e+18
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1403006759746929e+18
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.002206160334356e+18
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.882480117334147e+18
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.461336821273723e+18
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1307633746764104e+19
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2572559182864056e+18
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2931006001339433e+18
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.762822471754606e+19
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.68135481740152e+18
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.259315028687323e+18
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5928052047552184e+18
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.304826173062906e+18
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.558449092826235e+18
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8392607400199193e+19
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.790675844217176e+18
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1594031256525865e+18
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.81627797246994e+18
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.316516074636968e+18
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.442797405961978e+18
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9135746968447877e+18
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.180574962266407e+18
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.4698513519332557e+17
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.084126479725887e+18
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.094628252933685e+18
  2%|▏         | 3452/139870 [00:14<07:39, 297.07it/s, epoch=8, test_loss=-1.76e+18, train_loss=-3.09e+18]Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.7744536653542195e+17
Running validation...
Epoch 9, Step 300: Train Loss = -4.897297976357028e+17, Test Loss = -2.1743853989967954e+18
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.905974597595693e+18
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1692811342178157e+19
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9002626346893967e+18
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.154542893580681e+18
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.808984287421137e+18
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.562082666545152e+17
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.002871364869161e+18
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8513226005295923e+18
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.982311596941378e+18
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2799476392604467e+18
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.853738009622846e+19
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5070596844106547e+18
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7583745604418273e+18
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9228312947738542e+18
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.680652779227185e+18
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.380701043674317e+16
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.668897022754685e+18
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6083450993201644e+18
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7489838534806733e+18
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.346525817101353e+18
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6061360056880333e+17
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3044802764515836e+19
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4480415796968817e+18
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.516165670886441e+18
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2789715478128886e+18
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8927637749227848e+18
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.183641193931866e+18
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4560968237336494e+18
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0718614902910484e+18
  2%|▏         | 3482/139870 [00:14<07:42, 294.97it/s, epoch=8, test_loss=-2.17e+18, train_loss=-3.07e+18]Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.287157243056226e+18
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.997583813451186e+18
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.880499462935937e+18
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.881596995442783e+19
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5798201942828646e+18
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4454758693134664e+18
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.600350692173414e+16
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.887672127039734e+18
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1159980330270065e+18
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.664174093683917e+16
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.795406938648399e+19
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9803670576014295e+18
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7718600705565e+18
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6284724312740987e+18
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.745893729077035e+18
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.496399253624324e+18
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0998842247788626e+18
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0473699517084467e+17
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.701299736477696e+18
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.427527113597518e+18
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.920936804852695e+18
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5929123541127004e+18
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.815451255525081e+18
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8098827101667328e+17
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.446566256944087e+18
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.078616889732104e+18
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.711538969897861e+18
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.759759395337011e+16
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3054654113822802e+18
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.842072187353432e+18
  3%|▎         | 3512/139870 [00:14<07:53, 288.10it/s, epoch=8, test_loss=-2.17e+18, train_loss=-4.84e+18]Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.523456701315678e+18
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1295267519992955e+18
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1370026594606776e+18
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6774084483177513e+18
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.532854883006546e+18
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.674272959312691e+18
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.361860209044357e+18
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.940554894097514e+18
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4857803941242995e+19
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.849110701291864e+18
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.495049815206134e+17
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2807702938603487e+19
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2519700691620987e+18
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7960160131146056e+18
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.467166865880777e+18
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8513902204947005e+18
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2682990321454285e+17
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.011210822914933e+18
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6947043473649828e+18
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1162435520241336e+18
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.596725069760692e+18
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.096986917480169e+18
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.418209027430023e+18
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.388582961497506e+18
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2643346585587548e+18
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6214364659027804e+18
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.946969132723077e+17
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.706945210317013e+18
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.542629657176703e+19
  3%|▎         | 3541/139870 [00:14<08:10, 277.81it/s, epoch=8, test_loss=-2.17e+18, train_loss=-1.54e+19]Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.110667859840205e+17
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5409513901158564e+18
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3042894523098726e+18
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.93464249217083e+18
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.78855125984818e+18
Starting epoch 10/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.545872276045693e+18
Running validation...
Epoch 10, Step 0: Train Loss = -4.0440757849717473e+18, Test Loss = -2.4222625988975e+18
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6889472730954465e+19
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.664805571162276e+18
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9468806063437906e+18
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0740367679941509e+18
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.386995160654545e+18
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.607360486657491e+18
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4592859885965148e+18
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3811161230576976e+19
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.888647499906023e+18
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.246279821610451e+18
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.680241995834982e+18
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.743293596182249e+18
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.071882540090655e+18
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.263285196349768e+18
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4416249456364093e+18
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1121854764576932e+18
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7431322525976494e+18
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.915223795460997e+18
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3193529304358584e+18
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.975092700322988e+18
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.604950029265273e+18
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.859449197022937e+18
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.206157542801277e+18
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2091054453758624e+19
  3%|▎         | 3571/139870 [00:14<08:00, 283.50it/s, epoch=9, test_loss=-2.42e+18, train_loss=-3.21e+19]Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.022214417128358e+19
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.682667518485856e+18
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.2139374278947635e+17
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.679302019445686e+18
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6886530437838537e+19
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2945203759098102e+18
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.400691714228748e+18
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6446553339828634e+17
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0825438485369324e+18
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.148909049026183e+18
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2909533147485438e+19
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8805962199591813e+18
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7366889239937024e+16
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0495708142338834e+19
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4333716206811873e+18
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3917177221745213e+18
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4506430242081997e+18
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.64953285792288e+18
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0877171665448468e+19
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.027387175633682e+18
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.676073747254084e+18
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8080535194417234e+18
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.264872891140276e+18
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.702571981382196e+19
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.835478912851247e+18
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.467675009078264e+18
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.335464951977607e+18
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.092394151358497e+18
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0620109656178033e+18
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.142404444288713e+18
  3%|▎         | 3601/139870 [00:15<07:57, 285.35it/s, epoch=9, test_loss=-2.42e+18, train_loss=-7.14e+18]Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.632713078796976e+18
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.040056080411761e+19
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.092615814852012e+19
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7255974630095585e+18
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1429628979405914e+19
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4299885353030386e+19
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3189820043401626e+18
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7245922875300905e+18
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.152834146458665e+18
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0634040782366966e+18
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.221317165421691e+18
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8527308314333676e+18
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.782423793447102e+19
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1086965886238065e+18
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.242266932073202e+18
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5521419131989524e+18
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.617163119801926e+18
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.227011589168169e+17
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.187370843425735e+18
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.840225504548356e+18
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.666156264272062e+19
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.885492620115575e+18
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.395842210192635e+19
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.847566778760272e+19
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9159668493177586e+19
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.226728633749471e+18
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9331073304470487e+18
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6932059499003904.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.556181791847953e+19
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.2850136371360694e+19
  3%|▎         | 3631/139870 [00:15<07:52, 288.30it/s, epoch=9, test_loss=-2.42e+18, train_loss=-4.29e+19]Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.879249858224194e+18
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4180358543486484e+18
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.168974586477281e+18
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3294586478592e+18
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.41218743620128e+18
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.424934180554539e+18
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2329830995916227e+18
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.184215085975339e+18
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.754967668687582e+19
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9464276114518573e+19
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.10463809099373e+18
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.339035331364127e+17
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1153657608148091e+18
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4880024521484534e+18
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.260108707257123e+18
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4445079711768904e+18
Running validation...
Epoch 10, Step 100: Train Loss = -3.635338934415786e+18, Test Loss = -6.970012016169714e+18
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.663247394360263e+18
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.420528500235043e+18
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3932214201247334e+18
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.528745680149414e+18
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.129299480996479e+18
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.188868050358632e+18
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.561992490898424e+19
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3973868041273344e+18
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.680047494177384e+19
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.089510682114654e+18
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0858497520975217e+18
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.275383672546001e+18
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6096551988111606e+18
  3%|▎         | 3660/139870 [00:15<07:52, 288.16it/s, epoch=9, test_loss=-6.97e+18, train_loss=-1.61e+18]Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9596077283132047e+18
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.177857336246272e+16
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.145540917006172e+18
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1309685180077179e+18
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.147475760953426e+17
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.734848033315042e+19
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.979488378985382e+18
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4416707344476406e+18
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.602546877777314e+19
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.9782778166832e+18
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.853497328476881e+19
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.273206089767191e+18
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.216561883684012e+18
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.164406728192688e+18
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.056575582912119e+18
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.5951171811907994e+17
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.29245898207291e+19
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.551192422135366e+18
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.013325627478508e+18
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2607031621173903e+18
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.774546664445968e+18
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9562467980943557e+19
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.560671090026742e+18
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.98872251159203e+18
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.958413161955852e+18
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.624224183475503e+18
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.886833855476007e+18
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.737553373877764e+18
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.110977431336124e+18
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.953145179895497e+18
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8170359796848394e+19
  3%|▎         | 3691/139870 [00:15<07:42, 294.56it/s, epoch=9, test_loss=-6.97e+18, train_loss=-3.82e+19]Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9064391942846546e+18
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.451395143187825e+18
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.089619973570455e+19
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.683165385148334e+17
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2049894103161242e+17
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.001511268985602e+18
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.037461229071499e+18
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.495521191435803e+19
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0422633519538766e+19
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.978365015752704e+17
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.376352047176352e+18
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.297049761276232e+17
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.523208042862346e+18
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.477690513521902e+18
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.76838632264722e+18
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.576960248739791e+18
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.541287259531641e+18
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.063366050926035e+18
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6234677292223693e+18
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.940401174574531e+18
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.743528457714008e+18
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.29645458866176e+17
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4550875034458522e+18
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2107563659836785e+18
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.978171598011892e+18
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.180684163461415e+18
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.852827716148789e+18
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.31189801000108e+18
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.418587809185792e+17
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.901510686439375e+18
  3%|▎         | 3721/139870 [00:15<07:40, 295.35it/s, epoch=9, test_loss=-6.97e+18, train_loss=-4.9e+18] Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.58480911249467e+18
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5604522969595904e+18
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.0000799797241446e+17
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.371721781860106e+18
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.977406675569869e+18
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.920161258477978e+18
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0385223205653053e+18
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.052339164610298e+18
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.002483893072822e+18
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.759260270084227e+18
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.498234550635987e+18
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.984156355601105e+18
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.124868411803566e+17
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.345538773816967e+18
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1868284640865288e+19
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2326702885335204e+18
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.185794587454865e+19
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.319505203049529e+18
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3448499395289416e+18
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5216976967654965e+19
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.744355724414681e+18
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.870330841751355e+18
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.108352019420676e+18
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2733819055751823e+18
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.756952057526616e+18
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3288723863099146e+18
Running validation...
Epoch 10, Step 200: Train Loss = -5.279550271859458e+18, Test Loss = -6.129288262079165e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.472788458678046e+19
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.243371941259117e+18
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.121720450213478e+16
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.9186516329117516e+19
  3%|▎         | 3751/139870 [00:15<07:43, 293.72it/s, epoch=9, test_loss=-6.13e+19, train_loss=-4.92e+19]Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.593987399657403e+19
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.748988738509996e+18
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.243094748529689e+18
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.564538622177444e+18
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.732017004927779e+18
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.1820004103723418e+17
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.126319032877711e+18
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.976465493616493e+18
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.075823411616285e+18
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0811924426939433e+18
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.575420710609224e+18
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.871138876745318e+18
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.63563063290128e+18
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.735709058921398e+18
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5544498194921554e+18
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.095645841198416e+18
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.917961463614931e+18
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3323012191691997e+19
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.484224689274094e+18
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.201710780127642e+17
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.3728835473873306e+17
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.545932802211447e+18
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.769975539012993e+18
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.522181267827458e+18
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.833440355520348e+18
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.943355784170045e+18
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.86123557551394e+18
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0623965132687933e+18
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.534374514128585e+18
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.127361369900843e+18
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.923114006701146e+19
  3%|▎         | 3782/139870 [00:15<07:39, 296.34it/s, epoch=9, test_loss=-6.13e+19, train_loss=-1.92e+19]Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.303768327077757e+18
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.864146310696796e+18
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.32365828031932e+18
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.24440482638096e+18
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3232734590470193e+19
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.212930870129931e+19
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.365611130189513e+18
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.073493980433613e+18
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.945633750411117e+18
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.000546225680548e+18
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.670092837955568e+18
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.60179008197456e+19
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.552326240475283e+18
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0115080365027623e+19
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.341974604721789e+19
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.779613045201306e+17
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.679516255387648e+17
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.7965145788648e+18
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.848060677434114e+18
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.717266728537031e+18
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.587297191509098e+18
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.049229735980171e+18
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0179555726880408e+19
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.266328422683771e+18
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2636107675908178e+19
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.457992097155252e+18
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.499616202795516e+18
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9161786682834944e+18
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.758244977148428e+18
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.281513883827438e+18
  3%|▎         | 3812/139870 [00:15<07:37, 297.08it/s, epoch=9, test_loss=-6.13e+19, train_loss=-8.28e+18]Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.179704388649904e+19
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.944676741338366e+19
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.642629902072021e+19
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3863858623873024e+16
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.620756545653178e+18
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0036852311734616e+19
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.373318170589895e+19
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.212125026361803e+18
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.642180529720394e+18
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.777104456396308e+18
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.356311026885198e+18
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1538303609183666e+19
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2666085879933043e+19
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.874499418236387e+18
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.185020639270719e+19
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.626138982975275e+18
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0891469714656068e+19
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.703946694922338e+19
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.192570433913487e+18
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.050129358343373e+18
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1488236247701258e+19
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5424589030209094e+19
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4392097034192552e+19
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.3200804655838265e+19
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.401523392621419e+19
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.002397919060951e+18
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3665361629674471e+19
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0696372371423494e+19
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.381334696222523e+18
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.062397381181964e+18
  3%|▎         | 3842/139870 [00:15<08:03, 281.61it/s, epoch=9, test_loss=-6.13e+19, train_loss=-7.06e+18]Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.571508764036825e+18
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.918741186184479e+17
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7205818757927141e+19
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.987467862772285e+18
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.246777191543341e+18
Running validation...
Epoch 10, Step 300: Train Loss = -1.3108275921519903e+18, Test Loss = -7.557245233646273e+18
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.660975359633916e+18
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.820045115310316e+19
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.537504052125368e+18
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.240095608713249e+18
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.467439438712013e+18
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.54186636450857e+17
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2390708794710032e+19
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.810332797299327e+18
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.173411783399755e+19
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.823440075414045e+18
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.0768495819197776e+19
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.144661413756404e+18
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.000306416346464e+18
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.101708885964882e+18
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.591421247519982e+18
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.976127446817178e+16
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5975899514551665e+18
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.408816439602971e+18
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.552656093963616e+18
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.344658286854799e+18
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2183021454596506e+17
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2391185943769383e+19
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1219396858617004e+19
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.085876474128787e+19
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.943384371472368e+18
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.685722733395313e+18
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.48544283235844e+18
  3%|▎         | 3874/139870 [00:16<07:48, 290.31it/s, epoch=9, test_loss=-7.56e+18, train_loss=-9.49e+18]Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.573069404993225e+18
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.745265020530852e+18
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.156465780338983e+19
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2140287324902654e+19
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.639139058706285e+18
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.927880269909616e+19
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.952764633073058e+18
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.848975133457514e+18
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.569815013453988e+17
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.687997729005634e+18
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.872256530314953e+18
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.1090818300103885e+17
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.808978202872617e+19
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.183442722336276e+18
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0087128580426301e+19
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.721344267408638e+18
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.605587789288833e+18
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4422510625328464e+19
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.790515156440842e+18
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.1184290375427686e+17
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.900192034346762e+18
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.835934047892144e+18
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.075298182709117e+19
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.785940903444611e+18
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.713603155793281e+18
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4724995816882176e+17
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.43172443646355e+18
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.682904569294094e+18
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.533665223076217e+18
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3858601897767731e+17
  3%|▎         | 3904/139870 [00:16<07:45, 292.23it/s, epoch=9, test_loss=-7.56e+18, train_loss=-1.39e+17]Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.725080798596956e+18
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1415686072454087e+19
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.922329167503819e+18
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.09417029659394e+18
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.105762131477581e+19
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.03501492175215e+18
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0196181442204008e+19
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2055978952814494e+19
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0423099751456047e+19
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.243354356870493e+19
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.668141534669754e+19
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2610928859632108e+19
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9828996765835592e+18
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.961972715051549e+19
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0420707214154007e+19
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.912257968897524e+18
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4716598099921338e+19
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.373719478688547e+18
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.7973671754661888e+17
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.070870337483545e+19
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.045122245163483e+18
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.435911926497935e+18
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5771244155725939e+19
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8814970517851734e+19
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.817866323068715e+18
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.810395141557977e+18
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.554173092105814e+18
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3367121260676383e+18
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.136733078899917e+17
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9772603874971484e+18
  3%|▎         | 3934/139870 [00:16<07:51, 288.26it/s, epoch=9, test_loss=-7.56e+18, train_loss=-3.98e+18]Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.949488867827201e+19
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.558724626541445e+17
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6835684622123663e+18
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.99673982247764e+18
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.68028069084686e+18
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.150341830425759e+19
Starting epoch 11/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0591621898645275e+19
Running validation...
Epoch 11, Step 0: Train Loss = -9.95604810269773e+18, Test Loss = -9.858752318755832e+18
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.951783868701082e+19
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.26684694846872e+18
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.677330923001217e+18
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.683386440011743e+18
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8963030184398684e+18
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.290042023916601e+18
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.609216462285177e+18
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.523873294575311e+19
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.082529010978023e+19
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0137851250838864e+19
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3772605794824487e+19
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9237728340685095e+19
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2920726175856198e+19
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2492873216139723e+19
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4026421418039706e+18
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5784433838732083e+18
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.241961045988999e+18
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1540999611694973e+19
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0668295753265316e+18
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.14248866298069e+18
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.257397861338841e+18
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5201689435956576e+19
  3%|▎         | 3963/139870 [00:16<08:03, 281.29it/s, epoch=10, test_loss=-9.86e+18, train_loss=-1.52e+19]Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.5314112144925e+18
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.128049108575951e+19
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.96011585981456e+19
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2013743432150286e+19
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.604050619006976e+17
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7052047658736157e+19
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.868479258322115e+19
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.997744342148841e+18
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1598313854316052e+19
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.477896153582797e+17
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.019057943046049e+19
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2187569623431905e+19
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1797902663560987e+19
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.801164413538664e+18
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.59188667858944e+16
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.593849401201341e+19
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.91249754663315e+18
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0447623258801963e+19
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.863311115816272e+18
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.657032616989164e+18
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.455015730874994e+19
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0706542853980422e+19
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.464454050588341e+19
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.373610627037397e+18
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1995559708850127e+19
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.630150877103994e+19
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.185546637836026e+18
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.925324449282785e+18
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.932338021851464e+18
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1339182053393433e+19
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.854784075238736e+18
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5086534284644712e+19
  3%|▎         | 3995/139870 [00:16<07:47, 290.38it/s, epoch=10, test_loss=-9.86e+18, train_loss=-1.51e+19]Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0171280802369765e+19
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.952878650479503e+19
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.51382815634395e+19
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4556414040680694e+18
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8476181279026446e+19
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.718731715880524e+19
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1747850734228472e+19
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.065702185230926e+18
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.336529163434066e+18
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4189277801304556e+18
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4390529130611343e+19
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.882892053264204e+18
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.555000908562301e+19
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.331066789667275e+18
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1535734050509554e+19
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.075421752221237e+18
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7831294639085191e+19
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.688948541432529e+18
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3962820206918107e+19
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.672759703408738e+18
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0931674456837325e+20
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.248441895427244e+18
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.803063594125256e+19
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0314594226566647e+20
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.644324137589906e+19
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1799073682431672e+19
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.769728720292282e+18
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.58842772998062e+16
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.77299063169163e+19
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.37568518362733e+19
  3%|▎         | 4025/139870 [00:16<07:44, 292.59it/s, epoch=10, test_loss=-9.86e+18, train_loss=-9.38e+19]Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7852191957082702e+19
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.702391213873168e+18
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1783905919526502e+19
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.598492312850661e+18
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.504661871402156e+19
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7965335002115736e+19
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.145793139125518e+18
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.161944415181734e+18
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.388380036879824e+19
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.563797449186581e+19
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3668410575418294e+19
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7217013435565343e+18
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.416873617531863e+18
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.84030108622599e+18
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3665626611976765e+19
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.898390706396332e+18
Running validation...
Epoch 11, Step 100: Train Loss = -8.023132499590775e+18, Test Loss = -2.734285746967308e+19
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1564924984715379e+19
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0335402704024633e+19
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1570095948913705e+18
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.489470787154346e+18
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.330444678190858e+18
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2985577570685682e+19
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.799173961790836e+19
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.868868925242999e+18
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.907013182633804e+19
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0440256530895864e+19
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.816028161478754e+18
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2398057930430087e+19
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.625413917829759e+18
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.742708762048725e+18
  3%|▎         | 4055/139870 [00:16<07:53, 287.01it/s, epoch=10, test_loss=-2.73e+19, train_loss=-9.74e+18]Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.4347432741568512e+17
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.454162826059842e+18
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.576223638722183e+18
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.70450637475414e+17
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0419492033902988e+20
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3812567544936006e+19
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.924138076236415e+18
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0382777141628292e+20
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3512095206320636e+19
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.002818260406895e+19
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0586226595087778e+19
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8021843302713655e+19
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6704174254842184e+18
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.047646439236174e+18
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.9324995440082944e+17
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.31499236557513e+19
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6185657784816108e+19
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6677379175966835e+19
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.859278550868951e+18
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8017463947900223e+19
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.066488555947111e+19
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8025191315620233e+19
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.992605340599845e+19
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1177906787342877e+19
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8033449747456459e+19
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1368299320320197e+19
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0686583399907852e+19
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.879520129878432e+19
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3360156993013154e+19
  3%|▎         | 4084/139870 [00:16<08:04, 280.37it/s, epoch=10, test_loss=-2.73e+19, train_loss=-1.34e+19]Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.663667841758659e+19
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.953276455735788e+18
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3418159529913221e+19
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1646061466166179e+20
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.571759399661732e+17
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.8742989732695245e+17
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1628364606614798e+19
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.433978227094297e+19
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5446772737824e+19
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.98552282670275e+19
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.376001302967091e+17
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.758849592744477e+18
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0916265331636306e+18
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.683943164078981e+18
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3180219716104356e+19
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5393258446375617e+19
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4100392201297068e+19
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.748270535565378e+18
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5419089273046958e+19
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.714390627970449e+18
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.989443145158361e+19
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0113787339353358e+19
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.217868494174945e+17
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1330597577372467e+18
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6797423835993866e+18
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.825431524519759e+19
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4344201208175002e+19
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6469574776852054e+19
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6647842995109888e+19
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.933407952717742e+17
  3%|▎         | 4114/139870 [00:16<07:56, 284.78it/s, epoch=10, test_loss=-2.73e+19, train_loss=-7.93e+17]Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.076023860383449e+19
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.611387726770838e+19
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.432387991730913e+18
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1413901015326392e+18
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1264752812774392e+19
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.930304137444983e+18
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.583587454871824e+19
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.38484137645441e+18
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.155901605327929e+18
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5345105334637167e+19
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1142876346881933e+19
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0600722536894824e+19
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.424387187165207e+19
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.57805511882965e+17
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9036799187778798e+19
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7468500862402298e+19
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.285865422946042e+18
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.575231590710536e+19
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4043881701675893e+19
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0549280805371249e+19
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1818208923094509e+20
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7264498593989919e+19
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5888985358386332e+19
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6023496312393564e+19
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.497926581327757e+18
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.048091974993235e+19
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1295976743979975e+19
Running validation...
Epoch 11, Step 200: Train Loss = -1.1044512937149465e+19, Test Loss = -1.3878396405602583e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.441789813506284e+19
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0863879669380284e+19
  3%|▎         | 4143/139870 [00:16<07:59, 283.29it/s, epoch=10, test_loss=-1.39e+19, train_loss=-1.09e+19]Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.7259458363968717e+17
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1819816848898969e+20
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0167984466509693e+20
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3798340963984212e+19
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7662950612779663e+19
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9195133260225053e+19
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6533952282169967e+19
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.9326995019923456e+17
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2130447795345687e+19
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1208910816222904e+19
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3059139296630407e+19
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5730519286064087e+18
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7514560523495014e+19
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0430521454943535e+19
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0373885610996793e+19
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2614103169195049e+19
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.753580414866817e+18
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8154962275000123e+19
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.812345686881783e+19
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1481823856900964e+19
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6603252301045432e+19
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8050960019884605e+18
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.211822585998213e+17
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.311176522167065e+19
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7441438563214295e+18
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.438376925514957e+18
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.789917408893757e+19
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7109663167543247e+19
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0983143695645147e+19
  3%|▎         | 4172/139870 [00:17<07:59, 282.76it/s, epoch=10, test_loss=-1.39e+19, train_loss=-1.1e+19] Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1358879234955674e+18
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.44510924300925e+19
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.110942810365336e+19
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1430455753669345e+19
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8100036171634573e+19
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1509858143861473e+19
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1925448250393362e+19
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.281246924353569e+18
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.011630078394735e+19
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6900277651214172e+19
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.783337161655006e+19
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7499738007240966e+19
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9151319920881435e+19
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.304719860035276e+19
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8188486384531014e+19
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.329281058639792e+20
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4994944966050972e+19
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.119645772803316e+19
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3171735884384436e+20
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1809263936704348e+18
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1291885991475282e+18
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4601737617725719e+19
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2022796810893394e+19
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4853034298788807e+19
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9796935556525457e+19
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.043219819068234e+19
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.26781002239962e+19
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3977641623660528e+19
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.827217125555202e+19
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8207499139598516e+19
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4468211826136973e+19
  3%|▎         | 4203/139870 [00:17<07:49, 289.21it/s, epoch=10, test_loss=-1.39e+19, train_loss=-1.45e+19]Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.626883680251412e+18
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4747610924894388e+19
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.831683017781805e+19
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3133091129300668e+20
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4282334107805627e+20
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3403855103538181e+20
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6854894778515456e+16
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6072807209876062e+19
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0897298206301356e+19
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.025883267527921e+19
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6836867735622255e+19
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5017470660769219e+19
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.929206840435304e+19
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.126755656742522e+19
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.717426275471642e+19
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.875316365122601e+19
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.575790488065776e+19
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1841752545677751e+20
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7243322000038953e+19
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.356087392068174e+19
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4546755219364826e+20
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.306731966314106e+19
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7233051461923897e+19
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4807440830626267e+19
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2279449174108275e+19
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.702950105380343e+19
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.57648821615453e+20
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9561841259315724e+19
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.655837013893186e+18
  3%|▎         | 4232/139870 [00:17<07:55, 285.27it/s, epoch=10, test_loss=-1.39e+19, train_loss=-6.66e+18]Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.072687058596764e+19
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3116449121711423e+19
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.886729407719434e+19
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.50776667233667e+19
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.63464326171276e+18
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.3113919164949463e+18
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.937376207930969e+19
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6784596952837784e+19
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.613986090700243e+18
Running validation...
Epoch 11, Step 300: Train Loss = -2.4481088187316306e+18, Test Loss = -1.6651975179709396e+20
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6402200002836824e+19
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5508827893668826e+20
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7210612628624245e+19
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1022811856658498e+19
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.580096489858793e+18
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7142988815225324e+18
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4697445687383556e+19
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.838303836999621e+19
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6386031664856826e+19
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.746514407289625e+19
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5848112552940038e+20
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3489990624555827e+19
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.98390160655437e+19
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.840373653993423e+18
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2874526896280306e+19
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3110068125473178e+17
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.688716915662651e+18
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7828560153666912e+19
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5316888567733551e+19
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8070534075638088e+19
  3%|▎         | 4261/139870 [00:17<08:22, 270.02it/s, epoch=10, test_loss=-1.67e+20, train_loss=-1.81e+19]Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.503617103757312e+17
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.188417026790208e+19
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.569390668735788e+19
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3545654480730063e+19
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6924458130425774e+19
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.84407493803665e+18
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.980058813415293e+19
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7043787027876676e+19
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6849094304923124e+19
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6744274543418802e+19
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5818987130133676e+19
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5547335209799123e+19
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4817122489807038e+20
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.91717642400883e+19
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0199528336105406e+19
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.326656954792673e+17
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5660113216971735e+19
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.10911058418881e+19
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.3329966544257024e+17
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4734074177537861e+20
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0450148817517412e+19
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.234918571859352e+19
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.362555930874898e+19
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0111435464487993e+19
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8457742469028643e+19
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.589163400392344e+18
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.026200908639764e+17
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.900113762764351e+19
  3%|▎         | 4289/139870 [00:17<08:23, 269.43it/s, epoch=10, test_loss=-1.67e+20, train_loss=-1.9e+19] Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.563111029925426e+19
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.205724559020969e+19
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6576405525630026e+19
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4314468214736683e+19
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.229185830132646e+18
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7475465188546183e+19
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5187006557679256e+19
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.670114606336442e+18
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.362713377041285e+17
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.510319176884421e+18
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.320993399835145e+19
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1986832885060469e+19
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.723206629950541e+19
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2606708934004703e+19
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.930440272579343e+19
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2513991516480864e+19
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.0401472279763485e+19
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2096691669367783e+19
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.561431084159992e+19
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8628235620255374e+20
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.313658222064697e+19
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3387773368087347e+18
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7306029786998925e+20
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2827391912815624e+19
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.445409409683633e+19
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.086556078367205e+19
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8904936957282877e+19
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.843163833574359e+17
  3%|▎         | 4317/139870 [00:17<08:40, 260.48it/s, epoch=10, test_loss=-1.67e+20, train_loss=6.84e+17]Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.866223839866834e+19
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.967473571725312e+18
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7573369002418242e+19
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5153068992789217e+19
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.167363693940782e+19
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7438252217504104e+19
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8150170603326276e+19
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1420413972453523e+19
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.58160216498058e+18
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.7164405927344865e+18
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.598860099490152e+18
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.883765772381795e+19
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6316151947969167e+18
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.223412062976737e+18
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6436326644739736e+19
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0941626117086773e+19
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3254336675927556e+19
Starting epoch 12/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.121826983970498e+19
Running validation...
Epoch 12, Step 0: Train Loss = -2.0843404544354288e+19, Test Loss = -2.0872801087234507e+19
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9896338444550826e+20
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8647644639313527e+19
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.970452820128065e+19
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.908399454691525e+18
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.829786350477115e+18
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9994319883943805e+19
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.213748455280214e+18
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7655398287310796e+20
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.352622171222075e+19
  3%|▎         | 4344/139870 [00:17<08:37, 262.08it/s, epoch=11, test_loss=-2.09e+19, train_loss=-2.35e+19]Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2154163142151635e+19
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6421750399596757e+19
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.012671643810372e+19
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.746767842770472e+19
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.633075701630527e+19
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.396650015537496e+18
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0595995755900568e+19
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3301963141089853e+19
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4305373035058168e+19
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.263774257172447e+18
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9535080265300836e+19
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2097773608303067e+19
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0201616289193e+19
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9876870051864773e+19
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6473002834205134e+20
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7842923953669852e+20
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.634569278225698e+19
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.637889420461867e+18
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6771121299340657e+19
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0050340441183643e+20
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.818610248737358e+18
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6713595969980596e+19
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.774134072707645e+17
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.052349723820635e+19
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5939889428723925e+19
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.165308263005107e+19
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4570710499101508e+19
  3%|▎         | 4371/139870 [00:17<08:47, 256.75it/s, epoch=11, test_loss=-2.09e+19, train_loss=-1.46e+19]Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.867801203146752e+16
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.04555500584377e+20
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1166166182687932e+19
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2346790982268223e+19
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1856958552094015e+19
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9601930572269617e+19
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.33054408183943e+19
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4563023793818173e+19
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9884743635137724e+19
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8667688736287883e+19
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.933652713752861e+19
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0839225080754785e+20
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0489381591419716e+19
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.034212619813493e+19
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.174984854685614e+19
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.432465846900936e+19
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5569393612075565e+19
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.497281065848511e+19
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2264307818975724e+19
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0591189331277355e+20
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1431332323326113e+20
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.91524489828683e+18
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.117311501820127e+19
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6127735072460217e+20
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.445935304145843e+19
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.257931242226896e+19
  3%|▎         | 4397/139870 [00:17<08:48, 256.40it/s, epoch=11, test_loss=-2.09e+19, train_loss=-1.26e+19]Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0077845403752989e+19
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.287499191172465e+18
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2947866481889116e+19
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.197802566046712e+18
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8628351728683267e+20
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0804625888247808e+19
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4420302786486338e+19
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9374718693412962e+19
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.657152695355048e+19
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4427396816015196e+18
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.065450732769719e+19
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8660717832567783e+19
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1829763671297255e+20
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.504076491411528e+19
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.162386824427062e+20
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.023903422869605e+20
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4931666972360835e+20
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.576213138386138e+19
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.688312623287763e+18
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6501465355701453e+17
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1562911319626718e+20
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.201263444522896e+20
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6211938272756105e+19
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4292308657390486e+19
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.543174793092399e+19
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6229896635159675e+19
  3%|▎         | 4423/139870 [00:18<08:59, 250.95it/s, epoch=11, test_loss=-2.09e+19, train_loss=-1.62e+19]Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1208883289445106e+19
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.519293948343563e+19
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6960243935375e+19
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4019388980655555e+19
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.703140344831536e+20
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.618232171058902e+19
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8719874837183463e+19
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8899665859552215e+18
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4906044994418115e+18
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8182863482066567e+19
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8161545031622066e+19
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7718688155726512e+19
Running validation...
Epoch 12, Step 100: Train Loss = -1.774886645137408e+19, Test Loss = -1.787807885884706e+19
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.737687635943647e+19
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0459433093702353e+19
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.376463203903209e+18
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8247127737686819e+19
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.039213418598944e+19
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.740626190720041e+19
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1164158034650962e+20
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1777155998150033e+19
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.278867327055047e+20
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.491274325824163e+19
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.625169115464545e+19
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4210040978883478e+19
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.59973366147842e+18
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.028830730297855e+19
  3%|▎         | 4449/139870 [00:18<09:09, 246.58it/s, epoch=11, test_loss=-1.79e+19, train_loss=-2.03e+19]Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.7890211950166016e+17
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0113788419371434e+19
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.067557283198665e+18
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.865692498923356e+17
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.250691154120729e+20
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7509332326211387e+19
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0085953183002657e+19
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.130866552847561e+20
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7498851781375427e+19
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.853939648359249e+19
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.462096805660197e+19
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.901263848028294e+19
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.598872088065606e+18
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6839670390761456e+19
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.45742177852457e+17
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6046617502609415e+20
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4367740613626954e+19
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1615654213210604e+19
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.421216635435352e+19
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.74537377002826e+19
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.30248956666903e+20
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.04495110617862e+19
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.045427414615772e+19
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3351087102102077e+19
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.47365475999086e+19
  3%|▎         | 4474/139870 [00:18<09:09, 246.24it/s, epoch=11, test_loss=-1.79e+19, train_loss=-3.47e+19]Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6192286720926417e+19
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.156840488053663e+19
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.028829626887517e+19
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7908760711326597e+19
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8426117235572474e+20
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.746609295143102e+19
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6847316375127458e+19
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4896401876624972e+20
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.49269928119763e+17
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.001220363747328e+17
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4205148152139874e+19
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0705704987132953e+19
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.043106009867983e+19
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3728104601006532e+20
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.799240374242509e+17
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.060262249298762e+19
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.644314025981641e+18
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0824723841798373e+19
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.669807526384291e+19
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0484590999763943e+19
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.996163688131461e+19
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.629971122547694e+19
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9110883162253165e+19
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2489543894930817e+19
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.994462411840448e+19
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9839706558845944e+19
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1229457939766968e+18
  3%|▎         | 4501/139870 [00:18<09:00, 250.26it/s, epoch=11, test_loss=-1.79e+19, train_loss=1.12e+18] Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.706962105561514e+18
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.72032106578654e+18
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.911125587720143e+19
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7632242333033955e+19
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.464582909452406e+19
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2490229951199445e+19
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4225224429322895e+18
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2810701326305985e+19
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.05723848042186e+19
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6060614725435654e+19
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.377472068594696e+18
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2621884393491268e+19
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0263550699167547e+19
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3768950979186655e+19
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.498276004173709e+18
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.819184979060038e+19
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2337989372194324e+19
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.166687937993061e+19
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.728980599464578e+19
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9312201542275695e+19
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.305603274545562e+17
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.90039875227956e+19
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.220637779136243e+19
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.422713180711918e+19
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9459835846760962e+20
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.825143230621601e+19
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9892357772653625e+19
  3%|▎         | 4528/139870 [00:18<08:52, 254.07it/s, epoch=11, test_loss=-1.79e+19, train_loss=-1.99e+19]Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4422186909611694e+20
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6097056899839558e+19
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4482314123287724e+19
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2903371445336277e+19
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6241271082949018e+19
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.916169267459077e+19
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3641584671230263e+19
Running validation...
Epoch 12, Step 200: Train Loss = -2.3429304160278807e+19, Test Loss = -2.7374288109064684e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.718885979311256e+19
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.333894409568492e+19
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.672251670064005e+17
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3554821768877808e+20
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.966849676543378e+20
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8436205235263767e+19
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.774911490201485e+19
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.652914737736948e+19
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.41203482983541e+19
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0184385881461228e+18
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2727298971292664e+19
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1422405168517874e+19
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.54538327214795e+19
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.8026681017421e+18
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.763671402733057e+19
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.168823849281179e+19
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1320154985181217e+19
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6535969866513383e+19
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.503843603152634e+18
  3%|▎         | 4554/139870 [00:18<09:13, 244.29it/s, epoch=11, test_loss=-2.74e+19, train_loss=-7.5e+18] Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.585556896200786e+19
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6060082522820706e+19
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.359455188386631e+19
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.137518662216948e+19
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6993244638383964e+18
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.74923118082392e+17
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5488282619800977e+19
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.371208415982387e+18
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.161494616670942e+19
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6190625339363295e+19
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.306863244103752e+19
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.410940267959314e+19
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.836084247028564e+18
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0725771074339865e+19
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.475414530300471e+19
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.83023065922719e+19
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6468322394120913e+19
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5457720594595316e+19
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.551707882933243e+19
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2532972405204713e+19
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.539721439174459e+19
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.7635739821041254e+19
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.347670738559356e+19
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5291358968261116e+19
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.147713222129169e+19
  3%|▎         | 4579/139870 [00:18<09:33, 236.02it/s, epoch=11, test_loss=-2.74e+19, train_loss=-4.15e+19]Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9041576546307932e+19
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.793625617910884e+19
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.569895388553402e+20
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1263553206601384e+19
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.503487916989114e+19
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5383589321627402e+20
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.250023933580935e+18
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0376590351120466e+18
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1768042126773846e+19
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2055099343512273e+19
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.171977796436099e+19
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0747320383239094e+19
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.137199252139724e+19
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.479516364480342e+19
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8484047185211556e+19
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0202222378989335e+20
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8443570844164686e+19
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.997659243847562e+19
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.85535066527666e+19
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.150995156336273e+19
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.718342496561712e+19
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7368023091809616e+20
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.935769757421446e+20
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.705614881761421e+20
  3%|▎         | 4603/139870 [00:18<09:53, 228.00it/s, epoch=11, test_loss=-2.74e+19, train_loss=-2.71e+20]Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8470500141432832.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0822398355212075e+19
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.35957152061556e+19
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.806234701458991e+19
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3062519156387086e+19
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1203750769166647e+19
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.981374265228054e+19
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3122069825152614e+19
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.232102606781389e+19
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.734889043149403e+19
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4125377464539546e+19
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.351253719050145e+20
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.470421536098222e+19
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.6596366001240015e+19
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0579075552375944e+20
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.841926176107974e+19
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.751768529655405e+19
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.905088937080809e+19
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.878328799282961e+19
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.3372626456916e+19
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.070327286741231e+20
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.273883716834034e+19
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2251590886982418e+19
  3%|▎         | 4626/139870 [00:18<10:15, 219.77it/s, epoch=11, test_loss=-2.74e+19, train_loss=-1.23e+19]Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.28757879386496e+19
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.653420840989858e+19
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.834801888566444e+19
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.904928412281864e+19
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0848246793563013e+19
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.6707596484783636e+18
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.551634095956243e+19
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.27782800074442e+19
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.202567965240721e+18
Running validation...
Epoch 12, Step 300: Train Loss = -5.200339255171219e+18, Test Loss = -6.6369411369117155e+19
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4404207016273773e+19
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3536124993958576e+20
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.329105704626684e+19
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.222467698287706e+19
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8042492131083289e+19
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.610379470709457e+18
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.062616847601579e+19
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.940520811186409e+19
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.094208895300141e+19
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5142029896046346e+19
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2072989917518915e+20
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5720200408424514e+19
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.012009737810451e+19
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.849336996379702e+19
  3%|▎         | 4649/139870 [00:19<10:15, 219.52it/s, epoch=11, test_loss=-6.64e+19, train_loss=-1.85e+19]Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.653741022674577e+19
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2037563252054426e+17
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.62759111967821e+19
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.861307155670263e+19
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1754179484170846e+19
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6093947480956207e+19
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3921521450667213e+18
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4971713824672344e+20
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.797940649344736e+19
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4872362555212825e+19
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.472481141279372e+19
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.004214424170555e+19
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1889501858265825e+19
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4570552130440004e+19
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3902282157217284e+19
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.354131684887783e+19
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.937477471002532e+19
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8008389658942636e+19
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0095353768024264e+20
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.793059149520254e+19
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3200150504900395e+19
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.757437660735734e+17
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.002079500493547e+19
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1034134627401335e+19
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.647504333635584e+17
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.877450429231184e+20
  3%|▎         | 4675/139870 [00:19<09:49, 229.26it/s, epoch=11, test_loss=-6.64e+19, train_loss=-2.88e+20]Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.024308435074102e+19
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.166742689773414e+19
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6535613624745984e+19
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1528417841657676e+19
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.727303292527051e+19
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.063233131666368e+19
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.4253608321993933e+18
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.014822288554302e+19
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3850895381781545e+19
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.672951246131718e+19
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.464369824098943e+19
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8899148009069085e+19
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.185180953914114e+18
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.277139926367758e+19
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1966242291820003e+19
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9756570285646545e+19
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.918923979992596e+17
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4705450151527318e+19
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.718804839251837e+19
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4022325756719792e+19
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3961396300373033e+19
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3191028956436365e+19
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6686454505955394e+19
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.6078680742510985e+19
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0097087956750696e+20
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.140138026818444e+19
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.226888282837824e+19
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.705366500135351e+20
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0529774809387526e+20
  3%|▎         | 4704/139870 [00:19<09:11, 245.30it/s, epoch=11, test_loss=-6.64e+19, train_loss=-1.05e+20]Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.72897553391367e+18
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4697708890973694e+20
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4825233088799834e+19
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.87251568910433e+19
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.943917638510549e+19
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9722346847733285e+19
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.4365963292067758e+18
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.026513715799156e+19
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5450258228671152e+19
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5603591683247178e+19
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.617093632616405e+19
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.288799467775407e+19
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.468462866084502e+19
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3303032926916575e+19
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3204423246073037e+19
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4914820255200051e+19
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.4726670137200476e+18
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6137215501519553e+19
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.615762771498689e+20
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.2207752219884257e+18
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5686178239619793e+19
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3732629712074703e+19
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.23641918143023e+19
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.70797728854615e+19
Starting epoch 13/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.30684554001719e+19
Running validation...
Epoch 13, Step 0: Train Loss = -4.2926948253677126e+19, Test Loss = -2.5544892075468653e+19
  3%|▎         | 4729/139870 [00:19<09:52, 228.04it/s, epoch=12, test_loss=-2.55e+19, train_loss=-4.29e+19]Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.846651809164102e+20
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9555784030264754e+19
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.936757842591508e+19
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0999622076410626e+19
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.227032216707321e+19
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6805995609150456e+19
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5506591706920255e+19
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.522041144116259e+20
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.516546596689884e+19
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.304768342649995e+19
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.371713315620572e+19
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.264027030996963e+19
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.780009481916121e+19
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.853465986547424e+19
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4402532499051774e+19
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.113052221473869e+19
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.676450555736988e+19
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.7420243258072105e+19
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2623907514379928e+19
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.999334567765449e+19
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4584237771164484e+19
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.054257588599731e+19
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.930230701764379e+19
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.271017889604766e+20
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.323347606212486e+20
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.171074433784008e+19
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.2280124824003543e+18
  3%|▎         | 4756/139870 [00:19<09:25, 238.78it/s, epoch=12, test_loss=-2.55e+19, train_loss=3.23e+18] Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.2298448657062035e+19
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.005652801383301e+20
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.10359422245174e+19
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.994464718865511e+19
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3552291702164685e+18
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.388449973616771e+19
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.329615654220936e+19
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3155196590674779e+20
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.788826801360811e+19
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.583803175154483e+16
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.133943466268484e+20
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.535582221599244e+19
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.420302385668791e+19
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.306814313886961e+19
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1059968713620455e+19
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.069065535076371e+20
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.568279498386047e+19
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.76003487407664e+19
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.842786102202702e+19
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5695224943318925e+19
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.069981796248196e+20
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.751845055664698e+19
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.350117479835291e+19
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.40437178549498e+19
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.054557867174632e+19
  3%|▎         | 4781/139870 [00:19<09:28, 237.78it/s, epoch=12, test_loss=-2.55e+19, train_loss=-5.05e+19]Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9505442909880582e+19
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.6381281696650625e+19
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.562538728275103e+19
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1155680761013705e+20
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.064292835125153e+20
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7234857959528333e+19
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2355157625542174e+20
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.346468664486941e+20
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.51681047948055e+19
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5161039372072518e+19
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9806039512803443e+19
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0151753475860464e+19
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.9831183066733216e+19
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.764111541136366e+19
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5656511177892037e+20
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.064201579558758e+19
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.921319927730038e+19
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.578543111527203e+19
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.5175149307324e+19
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.19806689254074e+18
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.633050185163342e+19
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.751032296669446e+19
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.318936209680866e+20
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.93156694019497e+19
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1212007422290716e+20
  3%|▎         | 4806/139870 [00:19<09:20, 240.77it/s, epoch=12, test_loss=-2.55e+19, train_loss=-4.12e+20]Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.80481336862583e+20
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.699791340415138e+20
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.865901023252919e+19
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8951573842486624e+19
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.05981407266603e+17
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.357301600850249e+20
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.260526281732475e+20
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.056344569671057e+19
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9647779088142696e+19
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.106784229689341e+19
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1220586491211153e+19
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.8739038166863315e+19
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.387301088068842e+19
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9353171544549884e+19
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8810890210707505e+19
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3381602268618844e+20
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9217580889617727e+20
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.3275463733374616e+19
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.054640326648005e+18
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.519003225773048e+18
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3010193398021226e+19
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.35672477311073e+19
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4734034116327506e+19
Running validation...
Epoch 13, Step 100: Train Loss = -3.2889491210547233e+19, Test Loss = 1.7808350030436762e+18
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.872859612442788e+19
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.875452152859276e+19
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2049965743215739e+19
  3%|▎         | 4832/139870 [00:19<09:09, 245.79it/s, epoch=12, test_loss=1.78e+18, train_loss=-1.2e+19]  Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2969995252909736e+19
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6347514653530653e+19
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.295034254308776e+19
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9678394530119916e+20
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9601400568677925e+19
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.1721255468592844e+20
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.711581047857349e+19
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9484765494208627e+19
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.522474723582201e+19
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5205602597839831e+19
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6640041921123713e+19
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.574372426378117e+17
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.905121374623184e+19
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0922370389443084e+19
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2486295937582367e+18
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.241799047844473e+20
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.737574930153734e+19
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3567374194438046e+19
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.5467595929244244e+20
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.47091785174829e+19
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5779353493785438e+20
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.513251140439114e+19
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.49603926961868e+19
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0460930648032936e+19
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2524679849520923e+19
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.53006883634099e+18
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.32612576423262e+20
  3%|▎         | 4859/139870 [00:19<08:58, 250.93it/s, epoch=12, test_loss=1.78e+18, train_loss=-3.33e+20]Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.7362393514301194e+19
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.850749969025781e+19
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8894776350837047e+19
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.1101212435809305e+19
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5853267263859825e+20
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.67912994587594e+19
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.462113286638482e+19
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.745857223341638e+19
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.091473966178501e+19
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.847476726808602e+19
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.002720615976927e+19
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.830786904109036e+19
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.483260089672401e+19
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5857772823114573e+20
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5597891814968787e+19
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.833271476382387e+19
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.846734512479388e+20
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.473340633417515e+18
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.1405684227493069e+18
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.869586586229224e+19
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.315200724977949e+19
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3540494492154462e+20
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6442471795733823e+20
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.8511955008841318e+18
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0870738364433695e+19
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.541319685377884e+18
  3%|▎         | 4885/139870 [00:19<08:57, 251.22it/s, epoch=12, test_loss=1.78e+18, train_loss=-8.54e+18]Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0224854447952495e+19
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.197910873790113e+19
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.946414409514903e+19
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.662382848367342e+19
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.376913569714012e+19
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.280446042033252e+19
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.369210283248006e+19
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.777196707371234e+19
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.069140361989692e+19
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.198384714174169e+18
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2527666162089066e+19
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.606278060739396e+18
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.445674600192121e+19
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.652059313791828e+19
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.230442012420604e+19
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.585712251345725e+19
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9498060788811694e+18
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.239783687011225e+19
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.207017576897813e+19
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2580717559142154e+19
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.653658775306109e+18
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.688497021134517e+19
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.953159037640717e+19
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.610855003640404e+19
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.849725123984307e+19
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4310629780657013e+19
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.421897333337332e+19
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.284048265926882e+19
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.35880757213916e+19
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.354816900534213e+19
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.8493040660064502e+18
  4%|▎         | 4916/139870 [00:20<08:26, 266.53it/s, epoch=12, test_loss=1.78e+18, train_loss=1.85e+18] Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.722505239787051e+19
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.987580117061416e+19
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7935705343276876e+19
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6507550769976705e+20
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.338661996089626e+19
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.875078318905832e+19
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.877641872297102e+20
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.0232242008098865e+19
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.2729033922667086e+19
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.342010336900337e+19
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0363361046708617e+19
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.084522521864857e+19
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.484120679372816e+19
Running validation...
Epoch 13, Step 200: Train Loss = -4.411680455288422e+19, Test Loss = -6.996295401827048e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4063812768854914e+20
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.534630484334241e+19
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.882256969499935e+17
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.758050895254604e+20
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.067247970536894e+20
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.039646730283385e+19
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.3549974364447834e+19
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.3404249491348914e+19
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.3740901279569805e+19
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.0034139522177434e+18
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.492600992655527e+19
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.904142369469812e+19
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.699628916658772e+19
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.87123135152128e+19
  4%|▎         | 4943/139870 [00:20<08:52, 253.23it/s, epoch=12, test_loss=-7e+19, train_loss=-1.87e+19] Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.206391403076439e+19
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.202501006931945e+19
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.074953260063418e+19
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.298448018010695e+19
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.398124472326475e+19
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.468743233752164e+19
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.510410658201207e+19
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2346607823124588e+20
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.05786926439465e+19
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.784510110912741e+18
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.3276322529930117e+18
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.103339239857193e+19
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5041888614998868e+19
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0815476949008777e+19
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.667387161691292e+19
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.42857840579235e+19
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.615558058575764e+19
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.3291637471445e+18
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.507756329129948e+19
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.288767809637948e+19
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.814105873056457e+20
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.491586687330838e+19
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.958585895232576e+19
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.697353807198578e+19
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.711067799728213e+19
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1860594656540623e+20
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1257745625170575e+20
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.214023772991809e+19
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.686544504487202e+19
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.099679069751424e+19
  4%|▎         | 4973/139870 [00:20<08:30, 264.32it/s, epoch=12, test_loss=-7e+19, train_loss=-8.1e+19] Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.9699429707382e+19
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.156353068701005e+19
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.8191612237608124e+20
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.85055854600074e+19
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.25610439001186e+19
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.1853327246511466e+20
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.594861503604064e+18
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.400864443606761e+18
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.876056220648866e+19
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.297942134660111e+19
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.768528821303535e+19
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.67160840673265e+19
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.918732001363676e+19
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.506734655127998e+19
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.642510275206919e+19
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9197765051257297e+20
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.393392382486721e+19
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.553908549608171e+19
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.595631501343772e+19
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.8810308510575755e+19
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.002314568282145e+19
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.04392251110148e+20
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4795891282105126e+20
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.300733594821586e+20
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.4052060968674918e+17
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.854032562939861e+19
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.2445551198737e+19
  4%|▎         | 5000/139870 [00:20<08:44, 257.30it/s, epoch=12, test_loss=-7e+19, train_loss=-8.24e+19]Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2368089641503425e+20
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.775444857444054e+19
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.849729954038048e+19
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.984297199141913e+19
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.433987786997393e+19
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0150204044074602e+20
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1300334548365502e+20
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.336865062286996e+19
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.651599170124959e+20
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.386357159285752e+19
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.001580310518406e+19
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.743115149343772e+20
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.244998118956597e+19
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.080457299472836e+19
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.388508128035104e+19
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2679330597001232e+20
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1566162155198952e+20
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.429077056305779e+20
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1192637824229494e+20
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5602260194159493e+19
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1934074578820242e+20
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.64757187113732e+19
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.5786293054414e+19
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.756932052263056e+19
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0898586794942005e+19
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.152749643142005e+18
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4984257053322012e+20
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.093584920502023e+19
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.516472150005907e+18
Running validation...
Epoch 13, Step 300: Train Loss = -1.001818260429498e+19, Test Loss = -5.878717830436456e+20
  4%|▎         | 5029/139870 [00:20<08:33, 262.60it/s, epoch=12, test_loss=-5.88e+20, train_loss=-1e+19]Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.3743650058639245e+19
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.355952672106118e+20
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.0941052094042866e+19
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.807211615590914e+19
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3117633276240986e+19
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.063072153417286e+18
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0162997081766101e+20
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.122254134491113e+19
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0125615445640321e+20
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.675333004321096e+19
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.88852652968738e+20
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.8348574119542915e+19
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.621226145120046e+19
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7753781229363134e+19
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.8116563971942646e+19
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.117813982532731e+17
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0851229749115617e+19
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.78398278513606e+19
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.843184781220223e+19
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.7285471680821985e+19
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.56972525012412e+18
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.749204448498554e+20
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.796946243231154e+19
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.94213103621851e+19
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.451219549231514e+19
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.894634232717456e+19
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.413913335704689e+19
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.760340206506318e+19
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.388390815992486e+19
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.029448283948067e+20
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.810444395780191e+19
  4%|▎         | 5060/139870 [00:20<08:12, 273.82it/s, epoch=12, test_loss=-5.88e+20, train_loss=-9.81e+19]Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.107194675430634e+19
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.5640284546302214e+20
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.225467050209051e+19
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.898267011338207e+19
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2839971719720468e+18
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.385020045144569e+19
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.933201142377978e+19
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.56978085987405e+18
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.3945928331806376e+20
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.915732533643103e+19
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.262805253676178e+19
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.145553889294025e+19
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.266043427320496e+19
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1149474516160217e+20
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1042266615400366e+19
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.781489265488429e+18
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.335785449870328e+19
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.174757905738865e+19
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.537102618480712e+19
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.625771418187465e+19
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.53821148180539e+19
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.173838498014429e+18
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.558862617200086e+19
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.778339103851204e+19
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6872907488770392e+19
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8130843348946125e+17
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9491980489510093e+19
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.7894783602553e+19
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.425629739407691e+19
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.455319847993816e+19
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.147665275624777e+19
  4%|▎         | 5091/139870 [00:20<07:55, 283.66it/s, epoch=12, test_loss=-5.88e+20, train_loss=-8.15e+19]Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.157650932226431e+19
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.745465349600077e+19
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8112983360856287e+20
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.43519108472541e+19
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.007020357803762e+20
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.297625095629292e+20
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9681124436576462e+20
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7704512152309596e+19
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.716993587858459e+20
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.354455265311772e+19
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.2098744399096185e+19
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1955621488287443e+20
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.114199112306026e+19
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.7177700927576474e+18
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7145508126218047e+20
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2209518035558466e+19
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.472832429396381e+19
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3150870672126457e+20
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5707725149086994e+20
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.810822303774723e+19
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.331118134910937e+19
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.307214536119471e+19
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7351035637160477e+19
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.774078494344217e+18
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1280980465901634e+19
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.241809934958943e+20
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.175369124252746e+18
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8385852000758137e+19
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.264128409867754e+19
  4%|▎         | 5120/139870 [00:20<08:08, 276.12it/s, epoch=12, test_loss=-5.88e+20, train_loss=-6.26e+19]Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.608127883000676e+19
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.807976543881003e+19
Starting epoch 14/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.337647690764937e+19
Running validation...
Epoch 14, Step 0: Train Loss = -8.210709513142849e+19, Test Loss = -8.584166993805338e+19
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.426087460250268e+20
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.008932748672054e+19
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.269278190529413e+19
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0733226882181366e+19
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3449332864090374e+19
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.141333300060933e+19
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7721612837162844e+19
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.641990358189254e+20
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.47873174518534e+19
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.296946409132576e+19
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0535810688419365e+20
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4763930756083142e+20
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.063414661036114e+20
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.31376244796959e+19
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.595150027151324e+19
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.905866403702165e+19
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.0904701159610515e+19
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.983345130074066e+19
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.30486312445102e+19
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.145916504330155e+19
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3971919706668925e+19
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1337104856026238e+20
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.802687784949593e+19
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.347597087422462e+20
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.530913295504811e+20
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.449869672958027e+19
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.109842079527993e+18
  4%|▎         | 5150/139870 [00:20<07:59, 281.00it/s, epoch=13, test_loss=-8.58e+19, train_loss=6.11e+18] Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3806640517555316e+20
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.584798533243455e+20
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.904831103553451e+19
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.277774112978528e+19
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.349922430371234e+18
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.612262046721114e+19
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.767004010780715e+19
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3802792427266874e+20
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.98918442422428e+19
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.922322782538957e+16
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.019785036440011e+20
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.94417733925832e+19
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.544391061158914e+19
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.794142380578518e+19
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.292930992667938e+19
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1385853003964088e+20
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.402546692498548e+19
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.018721272585624e+20
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.14882977033911e+19
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.006002473919232e+20
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.55108627528282e+20
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.038762499133617e+19
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.112555670326634e+19
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.331995329186287e+19
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.965927986280792e+19
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.379741949526593e+19
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2505171473598723e+20
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.41917779737848e+19
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.785458525703273e+20
Batch 58/394
Batch data shape: (1, 28, 28, 1)
  4%|▎         | 5179/139870 [00:21<14:07, 159.02it/s, epoch=13, test_loss=-8.58e+19, train_loss=-7.79e+20]Train Loss: -7.503004719761106e+20
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.319024722316257e+19
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1732246665615848e+20
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.082115518888506e+20
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.723379239630615e+19
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.733186891147798e+19
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.916140240352104e+19
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8397021958878265e+19
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.084158487210387e+20
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1097344431876997e+19
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.810026697160865e+20
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.927020567615924e+19
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.779965385651782e+19
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.792238358242053e+19
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3993385970463303e+20
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2147934428273836e+19
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1359030877102696e+20
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.937120125824795e+19
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.212038075032923e+20
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.15456284776737e+19
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.210053676447113e+20
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.618434089272937e+20
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4011994027277576e+20
  4%|▎         | 5202/139870 [00:21<18:02, 124.36it/s, epoch=13, test_loss=-8.58e+19, train_loss=-5.4e+20] Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.02925193955697e+19
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.256470207277197e+19
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.4986452810019635e+17
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.08120428373316e+20
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.737498004421545e+20
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3532570971560057e+20
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.705291069738975e+19
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.331073159037295e+19
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.896898563064988e+19
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1144507362430576e+20
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.398651270337575e+20
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.398268280649967e+19
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.2296616909677265e+19
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.281374469651746e+20
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7439458122870345e+20
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0901992040933884e+20
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4779059156472037e+19
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9459093016705237e+19
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.89323586993054e+19
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0231684892567221e+20
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.085576517609954e+19
Running validation...
Epoch 14, Step 100: Train Loss = -5.982884770403582e+19, Test Loss = -1.1690333081956254e+20
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.457948004789623e+19
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.745268540917416e+19
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1455410308560454e+19
  4%|▎         | 5226/139870 [00:21<15:41, 143.09it/s, epoch=13, test_loss=-1.17e+20, train_loss=-2.15e+19]Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.283291138321285e+19
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.137445426945117e+19
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.700578555104906e+19
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.958816149109599e+20
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.787237339554277e+19
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.217448024085302e+20
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.634345186059826e+19
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4217885935867e+19
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.519769585377608e+19
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8377014126294073e+19
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.725114052975631e+19
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0035239877932483e+18
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.554457641816372e+19
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8119997805236847e+19
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.5131916607587615e+18
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.457616176079071e+20
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0836258837778924e+20
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.096633314540677e+19
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.002600285424383e+20
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.851400764110196e+19
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.970292135549427e+20
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.264223183871358e+19
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3097304224839814e+20
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.151466958725107e+19
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.758920077287883e+19
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.2153711223379067e+18
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.08189174628202e+20
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2651130322772034e+20
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2213222109317222e+20
  4%|▍         | 5255/139870 [00:21<13:08, 170.73it/s, epoch=13, test_loss=-1.17e+20, train_loss=-1.22e+20]Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.220343110019999e+19
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.333332890929542e+20
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.651125588476643e+20
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4610934152882763e+20
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.457831999917502e+20
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.819564848631303e+19
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.330445485434072e+20
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.79453611374307e+19
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.546469789497641e+20
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4479481820321677e+20
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.714729709559035e+19
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.257219694525321e+20
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.4705164181036335e+19
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.740971093872188e+19
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.075378123599847e+20
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8956606287713403e+18
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.05968778196104e+18
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.81578219682891e+19
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.076996180506124e+20
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4523721970585646e+20
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.083898290981369e+20
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.435144805032657e+18
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.299597883372313e+19
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4667344377043485e+19
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.326893919238829e+19
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.869988667884726e+19
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0878460732880873e+20
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0753274736988809e+20
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.711177854994088e+19
  4%|▍         | 5284/139870 [00:21<11:27, 195.86it/s, epoch=13, test_loss=-1.17e+20, train_loss=-5.71e+19]Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1199710762628651e+20
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.565364473158487e+19
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4925017126036046e+20
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.888177892641735e+19
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.97411028668357e+18
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.339326216912031e+19
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7252758008828527e+19
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4111893972532907e+20
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0653921107084365e+20
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.312591879505036e+20
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2447028419112626e+20
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.502393241263145e+18
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.891068288808832e+19
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.115237106959243e+20
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4553883495175815e+19
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.39341667974434e+18
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.052502104436113e+19
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.481584649955285e+19
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1526988754924549e+20
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1070153509322097e+19
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.511471906824336e+19
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.158265395000629e+20
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.610051588544633e+19
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5989931960737097e+20
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.807961258720022e+19
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.33172529107986e+18
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3640783146747167e+20
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9634772544786635e+20
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.226367993935561e+19
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.811719769145779e+20
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.485748056786312e+19
  4%|▍         | 5315/139870 [00:21<10:05, 222.34it/s, epoch=13, test_loss=-1.17e+20, train_loss=-9.49e+19]Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.463545622385341e+19
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.791604200016907e+20
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2373012814767954e+20
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1484634687413315e+20
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1781400792233778e+20
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.522145857704978e+19
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4686771428092333e+20
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.635086696701598e+19
Running validation...
Epoch 14, Step 200: Train Loss = -8.098464329305057e+19, Test Loss = -4.993560480502828e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.602760230912278e+20
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.037225050075143e+19
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2497109634441544e+18
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.732240423741188e+20
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.33667763758557e+20
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.037721537201035e+20
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2849490136906544e+20
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3721086198384112e+20
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2480727131090007e+20
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.695213114984235e+18
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.528912576267734e+19
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.529098505633346e+19
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.720555693770487e+19
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.451111473086569e+19
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2937634905472788e+20
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.902950930872533e+19
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.379529739883722e+19
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.637043495596196e+19
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7061861880032133e+19
  4%|▍         | 5342/139870 [00:22<09:38, 232.41it/s, epoch=13, test_loss=-4.99e+19, train_loss=-2.71e+19]Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3329421684774955e+20
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2000098053044935e+20
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1259460184109377e+20
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0702746460232037e+20
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2509651763579585e+19
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.4636155060960625e+18
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.885996129769384e+19
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.430773918408114e+19
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.951125820738634e+19
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.320696423693838e+20
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2883493193702493e+20
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.071172691485052e+19
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7133221303679975e+19
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.86718975108506e+19
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.479095023827157e+19
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.104453312917631e+20
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3680062979747139e+20
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.462334948182642e+19
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.593124934939155e+19
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.917208633851458e+19
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2000516947481566e+20
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0879885900359244e+20
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2670264463923243e+20
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.272094139465209e+20
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3666833655841738e+20
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.470156981904448e+19
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2723079724865788e+20
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.152802038068764e+20
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.078279090673413e+20
  4%|▍         | 5371/139870 [00:22<09:05, 246.41it/s, epoch=13, test_loss=-4.99e+19, train_loss=-1.08e+20]Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.650765064461961e+20
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0058374592141355e+21
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.480925944235216e+19
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2748749363132498e+19
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0454351830159301e+20
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.901453835840153e+19
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0847524873721768e+20
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3719987566365639e+20
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4013335509437671e+20
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.578104938052012e+20
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.65450813829179e+19
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5299129436836934e+20
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3402051904468628e+20
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.669860839052752e+19
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.445596206962416e+19
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0726533734591994e+20
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2857844666059037e+20
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.768963428462342e+20
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.952965028112981e+20
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.598329779143133e+20
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.387232814244823e+17
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0807044374024264e+20
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5513598894524072e+20
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2706005829109154e+20
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1006092042633111e+20
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0417059914182348e+20
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4556273471624158e+20
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.976236351494554e+19
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8455839233894515e+20
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.101042343924602e+20
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2529177770674933e+20
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.982481861463989e+20
  4%|▍         | 5403/139870 [00:22<08:29, 264.14it/s, epoch=13, test_loss=-4.99e+19, train_loss=-7.98e+20]Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1583126300201583e+20
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6690255776542045e+20
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0858392622572566e+21
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.855238499495785e+19
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2564146638484721e+20
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7415372260139388e+20
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3965154230707998e+20
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0714535184509174e+20
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2279400746622827e+21
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0314837199142833e+20
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.407905172163291e+19
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0689756590465614e+20
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.641980054516961e+20
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2580505612287423e+20
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.066214193562292e+20
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.465119243427014e+19
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.360064421931516e+18
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5999555086912166e+20
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1428284276685144e+20
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6634190359227793e+19
Running validation...
Epoch 14, Step 300: Train Loss = -1.7871301469173449e+19, Test Loss = -1.2363221883624935e+20
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1863553662233294e+20
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1630310302391311e+21
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1120554721521802e+20
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4551872786285147e+20
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.723228942239164e+19
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.290008944211447e+19
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7134096075131034e+20
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3329031138244769e+20
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.815575348356747e+20
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.205621624691732e+20
  4%|▍         | 5433/139870 [00:22<08:11, 273.51it/s, epoch=13, test_loss=-1.24e+20, train_loss=-1.21e+20]Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1289042300065452e+21
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.1414329119157e+19
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3462152089652167e+20
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.437945805056351e+19
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.355841529572072e+19
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.209331832870666e+17
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.191057837716511e+19
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2682025719903237e+20
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0470782052315483e+20
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2701576795863699e+20
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.08636827788247e+18
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.8572595655460415e+20
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.70286467527622e+20
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.484102323376698e+20
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.168031081356675e+20
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.0179971225314394e+19
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5128512978886223e+20
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2847661429167227e+20
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2225402938934375e+20
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8728044887778368e+20
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7282823934387736e+20
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0564671549234532e+20
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0182709126228869e+21
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2980757751514163e+20
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3565318825881738e+20
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.2541116743216005e+18
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.94575997239663e+19
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.0277735401209725e+19
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.0133864382689444e+18
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.325831743127222e+20
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4991443461321156e+20
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4726101398822532e+20
  4%|▍         | 5465/139870 [00:22<07:51, 285.15it/s, epoch=13, test_loss=-1.24e+20, train_loss=-1.47e+20]Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.156797927206892e+19
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4606229122725183e+20
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8823237965683307e+20
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.712854614125157e+19
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.252274686909219e+18
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3799235966449222e+20
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0365567585630342e+20
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.506531305052166e+20
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0942958084966213e+20
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.600888034837712e+19
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.046931650625667e+18
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1849490468709387e+20
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0488755988797063e+20
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.575315269393475e+19
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.578724087242424e+17
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.432714220729585e+19
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.63795144391279e+20
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.048788393962137e+19
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1586136323233782e+20
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5311425974064434e+20
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2838421133447397e+20
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.514380762543324e+20
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.508966279604328e+20
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.536956111206681e+20
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7482431914122097e+20
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.281863784063115e+21
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5285896594494325e+20
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1274504342414033e+19
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1584076630091702e+21
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6292032016366227e+20
  4%|▍         | 5495/139870 [00:22<07:51, 285.17it/s, epoch=13, test_loss=-1.24e+20, train_loss=-1.63e+20]Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0446039522253314e+20
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.110699926297405e+20
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3702530840153765e+20
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.084576073929196e+18
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.940959100460828e+20
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.725535717634238e+19
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0964616704814796e+20
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.289227541338464e+20
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9278840600270366e+20
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1189604931355437e+20
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.15216618409903e+20
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.078579881419053e+19
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.496647195445743e+19
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.2543063722924442e+19
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.409936737848577e+19
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.434261805335912e+20
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.165817016782055e+19
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.908836952317572e+19
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1694184011481376e+20
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.320688947014769e+20
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.583701892042043e+20
Starting epoch 15/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4579792465146937e+20
Running validation...
Epoch 15, Step 0: Train Loss = -1.4781672475708935e+20, Test Loss = -6.202153337456532e+19
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3340896364044285e+21
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2796690708931438e+20
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4125233247601086e+20
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3630361936552e+19
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.394305972546306e+19
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3355970932343885e+20
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.223301236103368e+19
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1793405348146766e+21
  4%|▍         | 5525/139870 [00:22<07:57, 281.51it/s, epoch=14, test_loss=-6.2e+19, train_loss=-1.18e+21] Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.484945868697528e+20
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3878234557490974e+20
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8951812216607526e+20
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.662512970421439e+20
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9014880203576757e+20
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7970847293710428e+20
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.641634516144751e+19
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.669451496721796e+19
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.748558935515988e+19
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6709199042474672e+20
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.048042932875926e+19
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2728569366520948e+20
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.99571892593125e+19
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0498510176758163e+20
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3754000298863913e+20
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0947643411875423e+21
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1179497868629199e+21
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7175573172167954e+20
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.093246830423258e+19
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5108232907222994e+20
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3994702217823146e+21
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.2723075649662616e+19
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5608610772912754e+20
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.654858342492013e+18
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3326276201910213e+20
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6079076845079967e+20
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.518061107986448e+20
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.152077943691176e+19
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.415623188186399e+17
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.279424381177452e+21
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4242399845484803e+20
  4%|▍         | 5556/139870 [00:22<07:48, 286.83it/s, epoch=14, test_loss=-6.2e+19, train_loss=-1.42e+20]Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4675385765284387e+20
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3597006750994242e+20
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4580964104737495e+20
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.851211703786815e+20
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5221730454298375e+20
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9405504139381388e+20
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2399387899695045e+20
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.841390474002044e+20
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3837684216814874e+21
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2924156652134859e+20
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3501315814224246e+20
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.125267783962329e+19
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5796020330843918e+20
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.818943180858248e+19
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2115439662035318e+20
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6024342276639975e+20
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3190139773892942e+21
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2641891258505227e+21
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.979077381538919e+19
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8919516882284734e+20
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0546224207026496e+21
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4203189501619706e+20
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.818789912836047e+19
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.899701546108322e+19
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3703563022682817e+19
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0403190435113704e+20
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.992919245321509e+19
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2599796675738149e+21
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.795492033050968e+19
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5857814643543533e+20
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2032264485617848e+20
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4791629853418848e+20
  4%|▍         | 5588/139870 [00:22<07:36, 293.97it/s, epoch=14, test_loss=-6.2e+19, train_loss=-2.48e+20]Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.195340991110578e+19
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0375326171637953e+20
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1768247114728368e+20
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4534378412795138e+21
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.965311380159935e+19
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5182974164755085e+21
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2849621198692575e+21
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.806143789938896e+20
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5261054267763458e+20
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.872472692351618e+19
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.0222248938753229e+18
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5091342800836698e+21
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4794468328150202e+21
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.357950184667952e+20
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.940418105104243e+19
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6009378363190595e+20
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.059490372095186e+20
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8586865835553325e+20
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4223242155291207e+20
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.75843133852127e+19
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.930035528703672e+19
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1560917572695391e+21
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.41944498260274e+20
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8951989897686575e+20
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.464308583250631e+19
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0637799149001507e+19
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1697416575667038e+20
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8195232108269745e+20
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0553566481793994e+20
Running validation...
Epoch 15, Step 100: Train Loss = -1.0366348678690714e+20, Test Loss = -1.467691593362653e+21
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6656105824992625e+20
  4%|▍         | 5618/139870 [00:23<07:40, 291.64it/s, epoch=14, test_loss=-1.47e+21, train_loss=-1.67e+20]Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.293596452740787e+20
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.374403932474258e+19
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0943729502324261e+20
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2714989078503961e+20
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7387910857724055e+20
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3261946855201597e+21
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.375928323233305e+20
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3538040030357545e+21
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4094491782097771e+20
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.029574244000145e+20
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6639980827264313e+20
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.061625088113325e+19
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2513150409579168e+20
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.8062678065057628e+18
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.16480898452171e+20
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.41465298692347e+19
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.834066093435781e+18
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3717262184903638e+21
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9913955259655704e+20
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3842615658406845e+20
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4105718763387596e+21
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7143384749362486e+20
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.1614777203749185e+20
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.393051941402428e+20
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2479786149545404e+20
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4608505072807576e+19
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1027868530128191e+20
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.749404903661109e+18
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0308981615531037e+21
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1923610946969797e+20
  4%|▍         | 5648/139870 [00:23<07:54, 283.11it/s, epoch=14, test_loss=-1.47e+21, train_loss=-2.19e+20]Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1600208513259484e+20
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.715578864338521e+19
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2792473182736063e+20
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.964089582798273e+20
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4894084985722923e+20
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5693860947674163e+20
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3607095869690715e+20
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1941169707860728e+20
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.530093575352615e+20
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6444776372105642e+20
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3637656336086545e+20
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.729916003834858e+20
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1262960828723442e+21
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.228476073386684e+20
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.653324903453284e+20
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5678834557477412e+21
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.245097846610592e+18
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3.7035416406867313e+18
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6480838393869317e+20
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9892971300141924e+20
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3303630382041557e+20
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.064489967813237e+20
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.896137701018567e+18
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2041611214063246e+20
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8362755659505074e+19
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2548629450784244e+20
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6452419976533167e+20
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8874846402663206e+20
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.849510851158286e+20
  4%|▍         | 5677/139870 [00:23<07:54, 282.65it/s, epoch=14, test_loss=-1.47e+21, train_loss=-1.85e+20]Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0365223658393174e+20
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.799814157035834e+20
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.532638053465483e+19
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.52743101803567e+20
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3305631771587091e+20
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.274816430598521e+18
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.934096584647639e+19
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4898472677331173e+19
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3056169496230232e+20
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.668212642737092e+20
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.111391826974532e+20
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0380681233069874e+20
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.194298750393123e+18
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4057627356241697e+20
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9521489421972223e+20
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.625496864285943e+19
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.5250437383485653e+19
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.463796542634931e+20
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2593022452658025e+20
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.045319798316356e+20
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.602368864895422e+19
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1875808379031834e+20
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.059337779922128e+20
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3594862263515428e+20
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8288075070527924e+20
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8709004864822496e+20
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5.828562266131268e+18
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2931430341863696e+20
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0867091303857914e+20
  4%|▍         | 5706/139870 [00:23<07:59, 279.70it/s, epoch=14, test_loss=-1.47e+21, train_loss=-3.09e+20]Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.141893827190063e+19
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.210641467018576e+21
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7547723553407343e+20
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2744549228714394e+20
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5349510242100828e+21
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.175577797366886e+20
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1736296386843273e+20
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.049459591536328e+20
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.565570841744245e+19
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.687891985774695e+20
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4949216937158744e+20
Running validation...
Epoch 15, Step 200: Train Loss = -1.5274171001678175e+20, Test Loss = -6.065647209649537e+19
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.482552335986957e+20
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3652651715062032e+20
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.148456166034768e+18
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.44109614871317e+21
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.247292182998582e+21
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.883800484664899e+20
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2923666910162295e+20
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4351605299982893e+20
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9894091922392954e+20
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.411343561026961e+18
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4670609486773328e+20
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3878954957509493e+20
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5900927813882583e+20
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.7464163230550065e+19
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3413999838029467e+20
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3040692570148884e+20
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2870277943545928e+20
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6215002870552147e+20
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.432867164746364e+19
  4%|▍         | 5736/139870 [00:23<07:52, 283.61it/s, epoch=14, test_loss=-6.07e+19, train_loss=-4.43e+19]Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1940217970595725e+20
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.021582837608486e+20
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.083797995480039e+20
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.85978662294869e+20
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3719795132337226e+19
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.4143022407648543e+18
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5545257792529603e+20
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.4223210890173874e+19
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.169290362318068e+19
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2273026946183987e+20
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0397705191505055e+20
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4122276880736322e+20
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8252399876448453e+19
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6711609171962757e+20
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.524832456194172e+20
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.4187567562438056e+20
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3371732851839153e+20
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4606187781087979e+20
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3754904537226596e+20
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.517798712734174e+19
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.893631741995715e+20
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.548520550706593e+20
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2685282233948832e+20
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3510257243190093e+20
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4875008019176358e+20
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.704734548730881e+20
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3469446890004257e+20
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5225955393698802e+21
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.835125720629767e+20
  4%|▍         | 5765/139870 [00:23<07:58, 280.33it/s, epoch=14, test_loss=-6.07e+19, train_loss=-1.84e+20]Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6917362302691207e+20
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5751956126927306e+21
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6255770323290948e+19
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.510240021793997e+19
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9411693070431814e+20
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.510470371429371e+20
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9600721109479064e+20
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4198910402773176e+20
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5551376555244624e+20
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.822376683444256e+20
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7900985204490843e+20
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.6097164173187324e+20
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2993395298767942e+20
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7951981433396396e+20
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1070939479220735e+20
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8139855425039132e+20
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.123973230667917e+20
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6696636814264076e+21
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7706674583818176e+21
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5759736095283589e+21
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6.491296748063949e+17
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9509806451220126e+20
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5704988005347256e+20
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.87674887289261e+20
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8854126325940093e+20
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8613335038675753e+20
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.487135764057214e+20
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3410426664635073e+20
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1869881092761715e+20
  4%|▍         | 5794/139870 [00:23<08:12, 272.04it/s, epoch=14, test_loss=-6.07e+19, train_loss=-3.19e+20]Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.535194469777948e+20
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.059284299676553e+20
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.467315261318791e+21
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.018540972719546e+20
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8724410542550365e+20
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8105461293947415e+21
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.723700332661645e+20
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1520807940766617e+20
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8086950644356533e+20
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9992464309133666e+20
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.547333429992316e+20
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9707298794660786e+21
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.643621093712939e+20
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.87811956027084e+19
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.739832055502408e+20
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6489177290463144e+20
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3360454501366078e+20
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7846628868050806e+20
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2588685099188775e+20
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.6276137696177422e+19
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.722084022730516e+20
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.892402887718758e+20
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9690059708277195e+19
Running validation...
Epoch 15, Step 300: Train Loss = -2.922808450470432e+19, Test Loss = -6.034095184174226e+19
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.159566445160421e+20
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0315684438822732e+21
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.053387926680046e+20
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3204339683189326e+20
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0235755724417899e+20
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8473171935194776e+19
  4%|▍         | 5823/139870 [00:23<08:04, 276.91it/s, epoch=14, test_loss=-6.03e+19, train_loss=-1.85e+19]Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.178458361950676e+20
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2338702095125e+20
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.229464090636693e+20
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1399699813599648e+20
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.937651643890484e+21
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5332355397801476e+20
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3918434662229842e+20
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1243070223572324e+20
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4635039845810124e+20
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.241884089921831e+18
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.774162271282187e+19
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0533643531507466e+20
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7430383672491088e+20
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.189337525681526e+20
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.838548908394676e+18
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.516223860239478e+20
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7130736168005324e+20
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7560770118986655e+20
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9231096966155652e+20
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1650404097291243e+20
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4768872601551792e+20
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0577738345827795e+20
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9328130186720838e+20
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.055776789663895e+20
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0711418049550877e+20
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8863140562069252e+20
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8151637263876799e+21
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.269817730631939e+20
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.353204868404331e+20
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.0205181985908326e+18
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.734150091172028e+20
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2654177289394926e+20
  4%|▍         | 5855/139870 [00:23<07:47, 286.92it/s, epoch=14, test_loss=-6.03e+19, train_loss=-1.27e+20]Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4.946988136631632e+18
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7345518263325384e+21
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2623486162030612e+20
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5972033871064282e+20
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5575938564991064e+20
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.196942275864806e+20
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4181504894309604e+20
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.1830183166960206e+19
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.24315554957335e+18
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.319935053922713e+20
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.855957859577983e+20
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4852217342155817e+20
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1145761885704317e+20
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7317496373862675e+20
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.182502545538207e+19
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8376832726369042e+20
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9060232859199262e+20
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1848171054756056e+20
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0886146959372452e+18
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.054156317730747e+19
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6582002460126505e+20
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3521681408398564e+20
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0144041701712016e+20
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.585515490542239e+20
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.21845083436643e+20
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6175492780323766e+20
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.605086505795563e+20
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.488429669340781e+20
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0076034029310286e+20
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0976870566490947e+21
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.039322878179184e+20
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.37383933130404e+19
  4%|▍         | 5887/139870 [00:23<07:35, 293.83it/s, epoch=14, test_loss=-6.03e+19, train_loss=-5.37e+19]Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0543330140987243e+21
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6357019511241677e+20
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5591841901175217e+20
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.517910850676751e+20
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1215842118030852e+20
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8.682701487547089e+18
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.0536122871747445e+20
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.409847449706981e+19
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9033142651909466e+20
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.8441403486844014e+20
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.7932961363071494e+20
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.071415695250922e+20
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0287045063629865e+20
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3210821323728617e+20
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.427520012000743e+19
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2.0863299107747267e+19
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.190327754198247e+19
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.417464514420492e+20
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.928526682542362e+19
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.27417827015105e+19
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.935537344602922e+20
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3671691938426677e+20
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8222484364079923e+20


SOM initialization...

 10%|▉         | 13661/139870 [00:26<00:38, 3260.31it/s, epoch=4, test_loss=1.47e+20, train_loss=1.19e+20] 

Training...

 10%|█         | 13988/139870 [00:27<02:59, 699.63it/s, cah=[nan], cr_ratio=nan, cs_ratio=nan, epoch=0, ssom=[nan], test_loss=nan, train_loss=nan, vae=[nan], vc_ratio=nan]]] INFO:tensorflow:Restoring parameters from ../models/hyperopt_10_8-8_2024-06-18_34329/hyperopt_10_8-8_2024-06-18_34329.ckpt
INFO - tensorflow - Restoring parameters from ../models/hyperopt_10_8-8_2024-06-18_34329/hyperopt_10_8-8_2024-06-18_34329.ckpt
Evaluation...
/Users/paulgramlich/PycharmProjects/pythonProject/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and binary values for target
  warnings.warn(msg, UserWarning)
/Users/paulgramlich/PycharmProjects/pythonProject/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received binary values for label, and continuous values for target
  warnings.warn(msg, UserWarning)
 94%|█████████▍| 131990/139870 [16:41<00:59, 131.84it/s, cah=[nan], cr_ratio=nan, cs_ratio=nan, epoch=299, ssom=[nan], test_loss=nan, train_loss=nan, vae=[nan], vc_ratio=nan]

 NMI: 0.0, AMI: 0.0, PUR: 0.7133620689655172.  Name: %r.


 Time: 1001.853848695755
INFO - hyperopt - Result: {'NMI': 0.0, 'Purity': 0.7133620689655172, 'AMI': 0.0}
INFO - hyperopt - Completed after 0:16:42
