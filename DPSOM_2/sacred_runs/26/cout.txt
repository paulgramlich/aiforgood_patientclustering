INFO - hyperopt - Running command 'main'
INFO - hyperopt - Started run with ID "26"
[ 1.          1.         -1.          1.          1.          0.66666667
  0.5         1.          1.          0.16666667  0.5         1.
  1.          0.83333333  1.          1.         -1.          1.
  0.83333333  0.66666667  0.5        -1.          0.83333333 -1.
  0.5         1.         -1.         -1.          0.83333333  1.
 -1.          0.5        -1.          1.          0.83333333 -1.
  0.83333333  0.5         0.83333333  1.          0.66666667  0.33333333
 -1.          0.5         1.         -1.         -1.         -1.
  1.          0.83333333  1.         -1.          1.         -1.
  1.          0.5         0.66666667  0.83333333  1.          0.33333333
  0.83333333  0.83333333 -1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         1.          1.          0.5
 -1.          0.5        -1.          1.          0.5         0.83333333
  1.          0.83333333  0.5         0.5         0.83333333  1.
  1.         -1.         -1.         -1.         -1.          0.66666667
  1.          1.         -1.          0.5        -1.          0.83333333
 -1.         -1.          1.          0.83333333  1.          0.83333333
  1.          1.         -1.          0.5         0.5         1.
  0.83333333  1.          0.83333333 -1.         -1.          0.83333333
  0.83333333 -1.          1.          1.         -1.         -1.
  1.          1.          1.          0.5         1.          0.83333333
 -1.         -1.          1.          0.5         1.         -1.
  1.         -1.          1.          1.         -1.          1.
  0.83333333 -1.          0.5        -1.          1.          0.83333333
 -1.          0.83333333  0.66666667 -1.          1.          1.
  0.83333333 -1.         -1.          0.5         1.          0.83333333
  1.          1.         -1.         -1.          0.5         1.
 -1.          0.5         0.83333333  1.          1.         -1.
  1.          0.83333333  0.33333333  0.83333333  0.83333333 -1.
  0.66666667  1.          1.          1.          1.          0.83333333
  1.          0.5         1.          1.          1.          1.
  0.5         1.          0.66666667  0.5        -1.          1.
  1.          0.66666667  1.          1.         -1.         -1.
  1.          1.         -1.          1.         -1.          1.
 -1.          0.5        -1.          1.          0.83333333 -1.
 -1.          0.33333333  1.          1.          0.33333333  1.
  0.83333333  0.33333333 -1.          0.5         1.          0.83333333
  0.33333333  1.         -1.          0.66666667  0.83333333  1.
  1.          1.          1.          0.83333333  0.5         0.66666667
  1.         -1.          1.          1.          0.83333333  0.83333333
  1.          1.          1.         -1.          1.          0.5
  1.         -1.          0.66666667  0.66666667 -1.          0.66666667
  0.33333333 -1.          0.83333333  1.          1.         -1.
  1.          1.          0.83333333  1.          1.          0.66666667
 -1.          1.          1.         -1.         -1.         -1.
 -1.          1.          1.         -1.          1.          0.83333333
  1.          1.         -1.          0.66666667 -1.          0.83333333
  1.         -1.          0.83333333  1.          0.5         0.66666667
 -1.          1.          0.83333333 -1.          1.          0.83333333
  1.          0.83333333  0.66666667  1.          1.          1.
  0.83333333 -1.         -1.          0.16666667  1.         -1.
 -1.          0.83333333  0.66666667 -1.          0.66666667  1.
  1.         -1.          1.         -1.          1.          1.
  1.          1.          1.         -1.          1.         -1.
 -1.          0.66666667 -1.         -1.         -1.         -1.
 -1.          0.66666667  1.          1.         -1.         -1.
  0.5         0.83333333 -1.          1.          1.          1.
  0.83333333  1.          1.          0.5        -1.          1.
 -1.          1.         -1.          0.83333333 -1.         -1.
  0.83333333  0.5         1.          1.          0.5         0.83333333
  0.5         0.83333333  0.66666667 -1.          1.         -1.
  0.83333333  0.66666667  0.66666667  0.83333333 -1.          0.66666667
  0.83333333  1.         -1.          1.         -1.         -1.
  1.          0.83333333  1.          1.          0.66666667  0.66666667
  1.          0.83333333 -1.          0.83333333  1.          1.
  0.5         1.          0.66666667  1.          0.83333333  0.5
  0.83333333  0.5         0.83333333  1.         -1.          1.
  0.83333333  0.83333333  0.83333333  0.83333333  1.          0.66666667
 -1.          0.5         0.66666667  1.          1.          0.66666667
 -1.         -1.          1.          0.5        -1.          1.
  1.          0.83333333  1.          1.          0.5         0.66666667
  0.5         1.          1.          0.83333333  1.          1.
  0.83333333  1.          0.83333333  0.83333333  1.         -1.
  0.66666667  0.83333333 -1.          0.66666667  1.          1.
  0.5         1.         -1.          0.83333333  0.83333333 -1.
  0.66666667  1.          0.83333333  1.          1.          1.
  1.         -1.          1.          0.83333333  0.66666667  1.
  1.          0.5         1.         -1.          1.          0.83333333
  1.          0.83333333 -1.         -1.          0.83333333  0.66666667
  1.         -1.         -1.          1.         -1.          0.83333333
 -1.          0.83333333  1.         -1.          0.83333333  0.5
 -1.         -1.          0.83333333 -1.          0.83333333 -1.
 -1.          1.          1.          0.83333333  1.          1.
  1.          0.83333333  1.          0.16666667  1.          1.
  0.16666667  0.5         1.          0.83333333  0.66666667 -1.
  0.83333333  1.          0.5         0.83333333 -1.         -1.
  1.         -1.          1.          0.83333333  1.          0.83333333
  1.          0.83333333  0.5         0.83333333 -1.          1.
  1.          0.83333333  0.83333333  1.         -1.          0.5
  1.          0.83333333  1.          0.5         0.83333333  1.
  0.5         1.          1.          0.66666667 -1.         -1.
 -1.          1.          0.5         0.83333333  1.          1.
  1.          0.5        -1.         -1.          1.          1.
 -1.          0.83333333  1.          0.66666667 -1.         -1.
  0.83333333  1.         -1.          0.5        -1.          0.83333333
  1.         -1.         -1.         -1.          0.5        -1.
 -1.          0.66666667 -1.          1.          0.5         0.66666667
  1.          0.5         1.         -1.          1.         -1.
  0.83333333  0.66666667 -1.         -1.          1.          0.5
 -1.          0.5         1.         -1.          1.         -1.
 -1.          1.          1.          1.          0.83333333 -1.
 -1.          1.          1.          0.66666667  1.         -1.
 -1.         -1.          1.         -1.          0.83333333  1.
 -1.          0.83333333  0.66666667  1.          0.66666667 -1.
  0.83333333  0.83333333  0.5         1.          0.66666667  1.
 -1.          0.83333333 -1.          1.         -1.          0.5
  0.5         0.83333333  0.83333333 -1.          0.66666667  0.83333333
  0.33333333  0.83333333  1.          0.33333333  1.          0.83333333
  1.          0.66666667 -1.          0.83333333  1.          1.
  0.66666667 -1.         -1.          0.66666667  0.83333333  0.33333333
 -1.          0.         -1.         -1.          1.         -1.
  1.          0.83333333  1.          1.         -1.          0.5
  1.         -1.          0.83333333 -1.          0.83333333  0.5
 -1.          0.83333333  1.          1.          0.5         0.5
 -1.          1.          1.          0.83333333  0.66666667  0.83333333
  1.          1.          1.         -1.         -1.         -1.
  0.83333333 -1.          1.          1.         -1.          1.
  0.66666667  1.          1.          1.         -1.          1.
  0.83333333  0.83333333 -1.         -1.         -1.          0.66666667
  0.5        -1.          1.          1.          0.66666667  0.83333333
  1.          1.         -1.          0.83333333 -1.          1.
 -1.          0.83333333 -1.          1.         -1.         -1.
  1.          0.83333333 -1.          1.         -1.          0.83333333
  1.          0.5         1.          0.16666667  0.83333333  0.5
 -1.         -1.          1.          0.5        -1.          0.16666667
  1.         -1.          1.          0.83333333 -1.          1.
  0.83333333  1.          0.5         0.83333333 -1.          1.
  0.83333333  1.          1.         -1.         -1.         -1.
  1.          0.5         0.83333333 -1.          1.          0.5
  0.83333333  1.          0.5         0.83333333  0.5        -1.
 -1.         -1.          1.          0.83333333 -1.          0.83333333
  1.          0.83333333  0.5         0.83333333  0.5         1.
 -1.         -1.          0.5         0.66666667  0.83333333  1.
  0.5         0.5        -1.          1.         -1.          1.
  0.5         0.5         0.83333333  0.83333333  1.          1.
 -1.          1.          0.5        -1.          0.66666667  1.
  0.83333333  0.83333333  1.          1.          0.83333333  0.66666667
  0.83333333  0.83333333  0.66666667  1.          0.16666667  0.83333333
  1.          0.83333333  0.5        -1.          0.5         1.
  0.83333333 -1.          0.5         0.66666667  1.         -1.
  0.66666667  0.5        -1.         -1.          0.33333333  0.83333333
  0.83333333  1.          1.          0.66666667  1.          0.83333333
  0.5         1.          0.66666667 -1.         -1.         -1.
  0.          1.          1.          1.          1.          1.
  1.         -1.          1.         -1.         -1.          1.
  0.83333333  1.          0.83333333  1.          0.66666667  0.5
  1.          1.          1.          1.         -1.          0.83333333
  1.          0.83333333 -1.         -1.          1.         -1.
  0.5         1.         -1.          1.         -1.          1.
  1.          1.          1.          1.          1.         -1.
  1.         -1.         -1.          0.83333333  1.         -1.
  1.          0.83333333 -1.         -1.          0.66666667  0.83333333
 -1.          1.          0.83333333  1.         -1.          1.
  1.          0.5         0.83333333  1.          0.83333333  0.66666667
 -1.          0.83333333 -1.          0.66666667]
num_features: 359
data_train_padded.shape: (928, 784)
data_train.shape: (394, 28, 28, 1), data_val.shape: (70, 28, 28, 1), data_test.shape: (464, 28, 28, 1)
Initializing global variables...
2024-06-19 14:18:58.620503: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
Variables initialized.

**********Starting job hyperopt_10_8-8_2024-06-19_69329********* 

  0%|          | 0/139870 [00:00<?, ?it/s]Number of batches: 394


Autoencoder Pretraining...

Starting epoch 1/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 548.42529296875
Running validation...
Epoch 1, Step 0: Train Loss = 545.937255859375, Test Loss = 546.6760864257812
  0%|          | 1/139870 [00:00<8:38:30,  4.50it/s, epoch=0, test_loss=547, train_loss=546]Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 549.3080444335938
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.3309326171875
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 547.06396484375
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.9373779296875
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.501708984375
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.6618041992188
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.5235595703125
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.8580322265625
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.0128784179688
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.7739868164062
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.2412109375
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 546.88525390625
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.8223876953125
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.05859375
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.2640380859375
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.2258911132812
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 545.2261352539062
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.9644165039062
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.2387084960938
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 544.011962890625
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.5164184570312
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.7937622070312
  0%|          | 23/139870 [00:00<26:18, 88.60it/s, epoch=0, test_loss=547, train_loss=544] Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.58447265625
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.4203491210938
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.3834838867188
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.2010498046875
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 543.1856689453125
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.6705932617188
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.7661743164062
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.8069458007812
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.4160766601562
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.2664794921875
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.6124877929688
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 542.1203002929688
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.8302001953125
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 540.8114624023438
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.6802978515625
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.6552124023438
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 541.1240234375
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 540.0005493164062
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 540.3840942382812
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.7271118164062
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.912841796875
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.3599243164062
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 539.5894775390625
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 536.3738403320312
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.121826171875
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 531.1617431640625
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 537.5767211914062
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 538.9203491210938
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 537.3834838867188
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 535.31103515625
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 529.4696044921875
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 535.5945434570312
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 531.3890991210938
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 533.904296875
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 518.208984375
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 524.2421875
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 523.114990234375
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 519.4248657226562
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 525.808349609375
  0%|          | 62/139870 [00:00<11:54, 195.74it/s, epoch=0, test_loss=547, train_loss=526]Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 517.20751953125
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 524.0985107421875
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 524.4138793945312
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 524.6661376953125
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 514.3889770507812
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 507.43328857421875
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 516.328125
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 508.1816101074219
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 472.85162353515625
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 463.20147705078125
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 482.9903564453125
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 432.911865234375
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 434.6319580078125
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 218.4859619140625
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 386.5906066894531
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 312.0287780761719
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 189.775634765625
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 65.12734985351562
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 274.31463623046875
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 412.5968017578125
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 365.0170593261719
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 203.44175720214844
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -94.6280517578125
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 177.33969116210938
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 277.532958984375
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 117.18531799316406
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 127.65727996826172
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 69.43959045410156
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 144.2193145751953
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 111.95553588867188
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 131.13494873046875
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -623.2190551757812
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -99.83515930175781
  0%|          | 95/139870 [00:00<09:41, 240.48it/s, epoch=0, test_loss=547, train_loss=-99.8]Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9.289886474609375
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 204.28492736816406
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 275.0240173339844
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 119.6146240234375
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 61.462745666503906
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 90.27606201171875
Running validation...
Epoch 1, Step 100: Train Loss = 87.23858642578125, Test Loss = -92.09696960449219
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 57.42502975463867
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 55.29927062988281
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 180.5440216064453
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7.814945220947266
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 80.99424743652344
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47.57640838623047
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1075.667724609375
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22.38579559326172
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2406.964111328125
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -55.669822692871094
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 36.062442779541016
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -93.58766174316406
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 165.0031280517578
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -58.25634765625
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 289.473876953125
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59.31828308105469
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 273.7359313964844
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 315.5811462402344
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5066.6953125
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -435.7359619140625
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -101.16327667236328
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9094.078125
  0%|          | 123/139870 [00:00<09:12, 252.96it/s, epoch=0, test_loss=-92.1, train_loss=-9.09e+3]Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -326.43585205078125
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1905.552001953125
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -265.50982666015625
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -652.6115112304688
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 153.3493194580078
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -79.97540283203125
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 410.83831787109375
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2679.591796875
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 88.25064086914062
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -406.8754577636719
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -368.9805908203125
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1771.703857421875
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12286.0546875
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1559.775390625
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2181.122802734375
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1903.0135498046875
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4465.06005859375
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -878.169189453125
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4386.888671875
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6529.18017578125
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2874.677978515625
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -36852.33203125
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -862.0797119140625
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1708.8154296875
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105868.109375
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 930.3617553710938
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3157.4365234375
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -616.6588134765625
  0%|          | 151/139870 [00:00<09:22, 248.18it/s, epoch=0, test_loss=-92.1, train_loss=-617]    Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9513.8662109375
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14313.255859375
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59389.6171875
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2666.53515625
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -733.017822265625
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3350.61962890625
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1750.3603515625
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17046.107421875
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14775.0341796875
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13364.716796875
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4318.578125
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9026.931640625
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1567.6512451171875
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22508.8359375
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4520.65673828125
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5117.66357421875
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2013.27001953125
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1491.452880859375
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13197.462890625
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35956.703125
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -45339.9296875
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1104.1396484375
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3267.0712890625
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21395.150390625
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -31304.181640625
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12718.85546875
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3073.117919921875
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37273.17578125
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8589.638671875
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18509.2265625
  0%|          | 181/139870 [00:00<08:52, 262.19it/s, epoch=0, test_loss=-92.1, train_loss=-1.85e+4]Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10373.9609375
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19665.2265625
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50264.828125
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24069.05859375
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17042.18359375
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52221.25390625
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2100.69580078125
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105111.5078125
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -155528.8125
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30820.634765625
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -600104.4375
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87703.7734375
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57184.875
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1171438.5
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98920.890625
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -107846.640625
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148767.625
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57602.82421875
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -302019.46875
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -126466.875
Running validation...
Epoch 1, Step 200: Train Loss = -147310.484375, Test Loss = -61187.66796875
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -579109.0625
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148260.578125
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 32999.265625
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2971253.5
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2223540.5
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190891.90625
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -332794.375
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -187122.046875
  0%|          | 209/139870 [00:00<08:55, 260.62it/s, epoch=0, test_loss=-6.12e+4, train_loss=-1.87e+5]Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -352353.34375
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 15054.1298828125
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -369949.8125
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -209390.1875
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -274847.875
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -124497.6015625
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1185211.25
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -296337.78125
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -368705.9375
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -847228.375
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 17107.296875
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -334555.09375
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -957850.5
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2582489.5
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 145266.015625
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -218911.703125
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 69840.515625
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1633189.125
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -260957.921875
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -325654.1875
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1127796.75
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1013343.1875
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -776899.4375
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -173167.96875
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1246552.75
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1537676.375
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6117731.5
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1692130.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1634892.375
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2704996.5
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -820229.25
  0%|          | 240/139870 [00:01<08:31, 273.12it/s, epoch=0, test_loss=-6.12e+4, train_loss=-8.2e+5] Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4826087.5
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8664514.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3399850.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4341397.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4720713.5
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4514986.5
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3513606.5
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24769876.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1717607.125
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5729593.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17360846.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1441386.375
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1404943.875
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 228422.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4512175.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5373224.5
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7347954.5
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10584116.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17262138.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5981787.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8827909.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13756377.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6027064.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2947325.5
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10784450.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14965641.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67078432.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -117050576.0
  0%|          | 268/139870 [00:01<08:41, 267.65it/s, epoch=0, test_loss=-6.12e+4, train_loss=-1.17e+8]Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -137113664.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1083751.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10983529.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24512186.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44871228.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16574880.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15934476.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35394280.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8714212.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40479024.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46921516.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37839272.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -213655392.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41650296.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24240628.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -270834560.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24629074.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39996848.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -73581616.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -93188048.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87849024.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -392537440.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112626688.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17171128.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136016448.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -71647696.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19420228.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67371096.0
  0%|          | 296/139870 [00:01<09:01, 257.56it/s, epoch=0, test_loss=-6.12e+4, train_loss=-6.74e+7]Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43646168.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8435827.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83192336.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84802672.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6197303.0
Running validation...
Epoch 1, Step 300: Train Loss = -7974469.0, Test Loss = -14742108.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65408140.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1093269888.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98642848.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44669248.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -56786544.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14611756.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160243840.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -181662640.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -134544480.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103784496.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -916417152.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -139787232.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -126912736.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -128951088.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -117927936.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1131495.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59103948.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -226512112.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112322256.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160026224.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3291022.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -737645632.0
  0%|          | 323/139870 [00:01<09:45, 238.29it/s, epoch=0, test_loss=-1.47e+7, train_loss=-7.38e+8]Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393269728.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -310988384.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -191661456.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -132124512.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -258129328.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -263429040.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -252801952.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -310480608.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -464986240.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -329839936.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1489910400.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -261886704.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -415397184.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16359774.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -348319776.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -168841056.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 7159954.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2825915392.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -485656480.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -779788672.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -512445696.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -665728384.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1055798528.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -160102656.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3407145.25
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -625770240.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -174920160.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -568178176.0
  0%|          | 351/139870 [00:01<09:32, 243.56it/s, epoch=0, test_loss=-1.47e+7, train_loss=-5.68e+8]Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -732169280.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -614365696.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -87275624.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -776245120.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -749948416.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -606051520.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -86370800.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -219128032.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -821088640.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -931054080.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -615192576.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1100973312.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1327787008.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -930247680.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3124417792.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1400270848.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1034028224.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6585954304.0
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2282360832.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -341303264.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11381456896.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1935268864.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1049269760.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2462026752.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1878409472.0
  0%|          | 376/139870 [00:01<10:07, 229.60it/s, epoch=0, test_loss=-1.47e+7, train_loss=-1.88e+9]Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 11955104.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3113165312.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -773044032.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1161889664.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1328383744.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3047697664.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1983507712.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1182294016.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1615460096.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -478151616.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 486985536.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1225632000.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11450015744.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 101867968.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1532387584.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3128530944.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2682701568.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3730645760.0
Starting epoch 2/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3362101760.0
Running validation...
Epoch 2, Step 0: Train Loss = -2495728640.0, Test Loss = -315388544.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25571491840.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3209919488.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2997765888.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1562020352.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1690087424.0
  0%|          | 400/139870 [00:01<10:02, 231.40it/s, epoch=1, test_loss=-3.15e+8, train_loss=-1.69e+9]Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3522740736.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1425162624.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27354226688.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3653915648.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3383614208.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5899639296.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6775406080.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3451104768.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3549682688.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1958566272.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3018016256.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1516684800.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6188477440.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1987460992.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2797382912.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1753537280.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9327770624.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5108072448.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25389670400.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40661786624.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6824975872.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 383244128.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7940619776.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50476851200.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4058888192.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10622615552.0
  0%|          | 426/139870 [00:01<09:43, 238.92it/s, epoch=1, test_loss=-3.15e+8, train_loss=-1.06e+10]Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -498450880.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5819740160.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13828832256.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25749327872.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4181012224.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -246036992.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -56947765248.0
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7517380096.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12275312640.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8255409152.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7856595456.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17206145024.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14212604928.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13366716416.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11061745664.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11859582976.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98551316480.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4960820224.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7162713600.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8228197376.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13276263424.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7272290304.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16384997376.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15988475904.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104222359552.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -128145276928.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5441837568.0
  0%|          | 453/139870 [00:01<09:25, 246.58it/s, epoch=1, test_loss=-3.15e+8, train_loss=-5.44e+9] Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37056593920.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103586660352.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19234752512.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11871135744.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12666170368.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4148122624.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15926048768.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12234737664.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -133956935680.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13101089792.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23478190080.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16875755520.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39509835776.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5944664064.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33331021824.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16757303296.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -185258409984.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19794675712.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -166676692992.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -192840564736.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -122017595392.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28490004480.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11831711744.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 156543872.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -198116081664.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228358881280.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39839100928.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19253409792.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35076644864.0
  0%|          | 482/139870 [00:02<08:58, 258.82it/s, epoch=1, test_loss=-3.15e+8, train_loss=-3.51e+10]Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -29687046144.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41627361280.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59499364352.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30584064000.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23216519168.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -252781920256.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -193958936576.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46567858176.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10725614592.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9790466048.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27105161216.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51626450944.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27842207744.0
Running validation...
Epoch 2, Step 100: Train Loss = -25219633152.0, Test Loss = -411642691584.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46055673856.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39768473600.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13440053248.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27498360832.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38063456256.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53376024576.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -443553120256.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42891337728.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393048555520.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53369339904.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43675787264.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -56876183552.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16716569600.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -55035998208.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 32994238.0
  0%|          | 510/139870 [00:02<08:46, 264.82it/s, epoch=1, test_loss=-4.12e+11, train_loss=3.3e+7]  Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47332958208.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14567579648.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 16113420288.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -665897205760.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -100376944640.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -54072225792.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -607614205952.0
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -80234913792.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -185833947136.0
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82161778688.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85922127872.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16440721408.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46974451712.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2693277696.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -475567947776.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -107165483008.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -118756016128.0
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46900719616.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -100055973888.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -408679350272.0
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148120567808.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -121511936000.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -78053425152.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -130625462272.0
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104703287296.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -161856815104.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169056403456.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -137455599616.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -619736793088.0
  0%|          | 539/139870 [00:02<08:33, 271.56it/s, epoch=1, test_loss=-4.12e+11, train_loss=-6.2e+11]Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -66869059584.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -105079095296.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1222979354624.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5430157312.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3909217792.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -126824415232.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -142644215808.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -371976634368.0
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -562806325248.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4102098944.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -102812147712.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19580678144.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -99725574144.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -129196040192.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -164635164672.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -173466599424.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88177975296.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -212438056960.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65185554432.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -190716657664.0
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104411365376.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6170465280.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -56320512000.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23106715648.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -285862658048.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -178447646720.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -267943411712.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -282660995072.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20286672896.0
  0%|          | 568/139870 [00:02<08:23, 276.42it/s, epoch=1, test_loss=-4.12e+11, train_loss=-2.03e+10]Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -150056435712.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -220829646848.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -117047705600.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 15776480256.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -178631360512.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -172826624000.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -266094985216.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82576154624.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103793082368.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -226375499776.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -183056531456.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -272497983488.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -181073002496.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 11157286912.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -239885811712.0
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -447693094912.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136359960576.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1668356374528.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -178324930560.0
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182006628352.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2172494282752.0
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -294354583552.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -346328301568.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -203919572992.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96375595008.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -272451862528.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -268034916352.0
Running validation...
Epoch 2, Step 200: Train Loss = -226279456768.0, Test Loss = -257918271488.0
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -633524584448.0
  0%|          | 596/139870 [00:02<08:23, 276.48it/s, epoch=1, test_loss=-2.58e+11, train_loss=-6.34e+11]Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -300919291904.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 491438240.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2489454166016.0
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2230757359616.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -213616214016.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -406863478784.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -459044749312.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -403212009472.0
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 4735105024.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -321365770240.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -252914352128.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -391753269248.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -108896223232.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -350013292544.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -266546872320.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -177261756416.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -324404707328.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -135198638080.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -408804196352.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -486687014912.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -859723661312.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -414139580416.0
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84871028736.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9855395840.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -273234296832.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -164611719168.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -189427269632.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -524150571008.0
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -512905805824.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -516243193856.0
  0%|          | 626/139870 [00:02<08:12, 283.01it/s, epoch=1, test_loss=-2.58e+11, train_loss=-5.16e+11]Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88784052224.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -487400275968.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -564726071296.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -881646895104.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -692986970112.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -347169226752.0
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -554571661312.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -260962680832.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -999742504960.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1111972642816.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -752230465536.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -535654629376.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -815896526848.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -542476337152.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -695051616256.0
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5668227514368.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -525357613056.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -804854562816.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4793193463808.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -149373927424.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148251639808.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -618390749184.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -319590596608.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -784357064704.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -958261231616.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -771037462528.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1251224584192.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -783751184384.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2450277269504.0
  0%|          | 655/139870 [00:02<08:09, 284.42it/s, epoch=1, test_loss=-2.58e+11, train_loss=-2.45e+12]Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -750130036736.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -703865290752.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393768501248.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -685682458624.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -995750641664.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6291300810752.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6474501718016.0
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3774288232448.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -50267734016.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1077371994112.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1171230687232.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1907895042048.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -940455559168.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -851560103936.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -936381513728.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -646195970048.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1662218928128.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1646978793472.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1226509516800.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5012470104064.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1095237042176.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1238790307840.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10755492544512.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -950035283968.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1155599695872.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1890569551872.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2006854139904.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1799763787776.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10902613000192.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2320261709824.0
  0%|          | 685/139870 [00:02<08:03, 287.65it/s, epoch=1, test_loss=-2.58e+11, train_loss=-2.32e+12]Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -607864356864.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2611509985280.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1743739420672.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1810277597184.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1553234395136.0
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -392349024256.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 89398501376.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3151128690688.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1682324586496.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -325712183296.0
Running validation...
Epoch 2, Step 300: Train Loss = -317747036160.0, Test Loss = -275447021568.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1317296799744.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13013853667328.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1109878243328.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1789556162560.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -707651895296.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -266143596544.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2684302393344.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2264878546944.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2872901107712.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1979381186560.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17956266311680.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1373500473344.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2164320370688.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1325418020864.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1220708401152.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -73809756160.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -878567030784.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2012669280256.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1895747420160.0
  1%|          | 714/139870 [00:02<08:07, 285.49it/s, epoch=1, test_loss=-2.75e+11, train_loss=-1.9e+12] Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1671682064384.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -133061509120.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8046137835520.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3887358541824.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3191184293888.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2358559899648.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1311101419520.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2919937081344.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1930909188096.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2018757705728.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3486185422848.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2351755689984.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2028865454080.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17546240589824.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3007501041664.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2651886452736.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 49985687552.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2409653862400.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1969754341376.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 353661812736.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20156292005888.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3043115401216.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3853227655168.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2337183629312.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2759810351104.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5487616589824.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -879291531264.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 151567548416.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3818454515712.0
  1%|          | 743/139870 [00:02<08:13, 281.86it/s, epoch=1, test_loss=-2.75e+11, train_loss=-3.82e+12]Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2269549428736.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2984746418176.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2432577044480.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2485637873664.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -335353282560.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2803835338752.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2691438739456.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1953239793664.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -167287193600.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1410799894528.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3343304884224.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2412101238784.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3558377259008.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4973897187328.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3968563150848.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4204084592640.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6975671238656.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4005287165952.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5551698739200.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -29456257253376.0
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10199532306432.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -797069017088.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27332985225216.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4032146178048.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3322329694208.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5639452491776.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3691920490496.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 48279584768.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9783968006144.0
  1%|          | 772/139870 [00:03<08:16, 280.44it/s, epoch=1, test_loss=-2.75e+11, train_loss=-9.78e+12]Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2542032912384.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3678331207680.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8695603789824.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7246051803136.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4890945912832.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4365802536960.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2875875655680.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1931493376000.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 158984175616.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2278019039232.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20462721564672.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 295958216704.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2096200155136.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5412913414144.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5363021643776.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6754049982464.0
Starting epoch 3/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6662419120128.0
Running validation...
Epoch 3, Step 0: Train Loss = -6104041914368.0, Test Loss = -7662081146880.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -49902033829888.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5590211362816.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5423942860800.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2012016803840.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2250067935232.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5032891121664.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2628479877120.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44164234346496.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6185299738624.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5744014393344.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8544926040064.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10261454913536.0
  1%|          | 801/139870 [00:03<08:25, 275.28it/s, epoch=2, test_loss=-7.66e+12, train_loss=-1.03e+13]Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9774317961216.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9691159592960.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2478391689216.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4453258231808.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5487829975040.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7734134571008.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2040860901376.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7144610988032.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4865641152512.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7999864700928.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6200942919680.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -52349913530368.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -57380981178368.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9157736398848.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 513711308800.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -11425668923392.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67732607336448.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3910487506944.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9795966861312.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -635871625216.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7805807886336.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9754970685440.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25144055562240.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6267258535936.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -324474634240.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -89054687264768.0
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9336952717312.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9992597929984.0
  1%|          | 829/139870 [00:03<08:26, 274.52it/s, epoch=2, test_loss=-7.66e+12, train_loss=-9.99e+12]Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9335600054272.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9625374031872.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28030464425984.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12869019107328.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14058633822208.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9060426448896.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12359309459456.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88789800189952.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10407765868544.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9165287194624.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6995783974912.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12910088683520.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7088002039808.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14410151100416.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10047060967424.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82727512571904.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85202571362304.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5842157436928.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34107337736192.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82530321563648.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9775870902272.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7603510837248.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5609259270144.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3402647470080.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15712905068544.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5143407362048.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -113184954384384.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7553328611328.0
  1%|          | 857/139870 [00:03<08:29, 272.97it/s, epoch=2, test_loss=-7.66e+12, train_loss=-7.55e+12]Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17933738704896.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8314479968256.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26938986987520.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3017853632512.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20026673332224.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9670443925504.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -143073799569408.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10600354676736.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -133193327968256.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -131840664928256.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -98056812888064.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18878153359360.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7055839068160.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 170778689536.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -154538292019200.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -189730046083072.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24239444852736.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14424959090688.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20914418745344.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5922518728704.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28280916803584.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22463486361600.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -13423761948672.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9653442314240.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112694782853120.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82510146961408.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24403662340096.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4003347300352.0
  1%|          | 885/139870 [00:03<09:08, 253.60it/s, epoch=2, test_loss=-7.66e+12, train_loss=-4e+12]   Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4889278152704.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12883346849792.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25323575967744.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10724959059968.0
Running validation...
Epoch 3, Step 100: Train Loss = -13041133420544.0, Test Loss = -25269972762624.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27831614570496.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16879475228672.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5301618081792.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10958230519808.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15561448751104.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22487597318144.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -198393666207744.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16906081796096.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -245200186245120.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22228823441408.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15456032260096.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10182803324928.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8529874780160.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22677802713088.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 138719592448.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23266129346560.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6639007563776.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2325480996864.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -205089050460160.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -24719535374336.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28099756425216.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169420336922624.0
  1%|          | 911/139870 [00:03<09:10, 252.49it/s, epoch=2, test_loss=-2.53e+13, train_loss=-1.69e+14]Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30831294808064.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -81760507396096.0
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26928851451904.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39822710276096.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8113490493440.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17739125096448.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 91639775232.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -217372824698880.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37513519431680.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -31892587937792.0
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -17060472029184.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38719130173440.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112901176164352.0
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47782282919936.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40999824916480.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -20026228736000.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41199876440064.0
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27548708765696.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51802296811520.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39621459181568.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33803670126592.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -205886219878400.0
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26592940130304.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -38809433538560.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -317846336831488.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -751571042304.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1010367987712.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22114558017536.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37554514558976.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -120048362455040.0
  1%|          | 941/139870 [00:03<08:45, 264.50it/s, epoch=2, test_loss=-2.53e+13, train_loss=-1.2e+14] Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -143488616235008.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 547661807616.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28555442388992.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7258242023424.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -29964994871296.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53019680964608.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37951429935104.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44389044846592.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27532464226304.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44272313171968.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16656816406528.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -62274366603264.0
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34774745874432.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1353355362304.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10287514124288.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9137845960704.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -53212686057472.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -46773573779456.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -55262048157696.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61727395807232.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3677439918080.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35023323398144.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44503553540096.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -27816846426112.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3416583307264.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25832449900544.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -33877234024448.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34477589921792.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23353427492864.0
  1%|          | 970/139870 [00:03<08:34, 269.81it/s, epoch=2, test_loss=-2.53e+13, train_loss=-2.34e+13]Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -26608404529152.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -70578878808064.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41269711601664.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -61856983023616.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -40872783642624.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2667410882560.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -70137688358912.0
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85123617783808.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23852946030592.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -274319225651200.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39760668131328.0
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -39358384046080.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -432898545025024.0
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -44643127394304.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -41939281903616.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -43235976151040.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -25237827616768.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -79956453359616.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -48423113850880.0
Running validation...
Epoch 3, Step 200: Train Loss = -46795879088128.0, Test Loss = -37877031370752.0
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112278464626688.0
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32559612297216.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 335371993088.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -411990241574912.0
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -277155531456512.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47807226445824.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -83800096768000.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -84594531500032.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -68035662577664.0
  1%|          | 998/139870 [00:03<08:29, 272.58it/s, epoch=2, test_loss=-3.79e+13, train_loss=-6.8e+13] Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1355081842688.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34909376741376.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32798180114432.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51258765344768.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -15487423479808.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -69610216882176.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -28828632088576.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -37975559766016.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65903119040512.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18820085317632.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -75548374073344.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88689346609152.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -125279288688640.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -55060067254272.0
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -12052430061568.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -334871396352.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -64048846602240.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16596860928000.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -35690058350592.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -65492295352320.0
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67899469332480.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51803165032448.0
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21046744842240.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -66885152407552.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -42729484582912.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -206166080618496.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -64432306651136.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -59270317998080.0
  1%|          | 1026/139870 [00:04<08:32, 270.69it/s, epoch=2, test_loss=-3.79e+13, train_loss=-5.93e+13]Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -60134235570176.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -32663672979456.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -134705206788096.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -121580868861952.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85745263968256.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -88523914870784.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -109007880585216.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -70897729798144.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82947914858496.0
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -488801403142144.0
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -75165476061184.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97334310469632.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -513094845464576.0
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -16906423631872.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -18228688453632.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85061189763072.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51891341885440.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -74928095232000.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103776568475648.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -125878931554304.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -115269372477440.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -76747802411008.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -222033820516352.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -122698038509568.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -68363514544128.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -63515549237248.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -93375717965824.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -100508224192512.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -737002362241024.0
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -704868624891904.0
  1%|          | 1056/139870 [00:04<08:17, 278.94it/s, epoch=2, test_loss=-3.79e+13, train_loss=-7.05e+14]Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -667446373515264.0
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2130179522560.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97756240674816.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148802228977664.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -241019606007808.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -85312520847360.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -112143785525248.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -115380764803072.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -69977180733440.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -167439333588992.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -162451685376000.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96356425269248.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -516400762322944.0
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -96009791209472.0
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -153250875572224.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -941857068548096.0
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -72512092241920.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -126244054106112.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -163110878969856.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -239547740848128.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -201625847201792.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1016823440998400.0
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -228602100580352.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -60658485821440.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -255946093756416.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -154668080562176.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -133710603091968.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -123814881001472.0
  1%|          | 1084/139870 [00:04<08:29, 272.66it/s, epoch=2, test_loss=-3.79e+13, train_loss=-1.24e+14]Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -73593744523264.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 8306736234496.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -261567551635456.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97296872112128.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23740899393536.0
Running validation...
Epoch 3, Step 300: Train Loss = -20129050001408.0, Test Loss = -140950928097280.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92466300583936.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1475716134731776.0
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -147137325170688.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -183715917463552.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -47828965523456.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -21833317351424.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -220076808601600.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -189362222399488.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -205249054769152.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -162574041612288.0
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1270565948096512.0
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104156018769920.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -179887272886272.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -89561837338624.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -97514103504896.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4648931950592.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -82782164353024.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -172090296631296.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -123256199708672.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -148693546172416.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7998420287488.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -663405480378368.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -242158829305856.0
  1%|          | 1112/139870 [00:04<08:25, 274.33it/s, epoch=2, test_loss=-1.41e+14, train_loss=-2.42e+14]Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -230848720797696.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -158218911219712.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -104914852249600.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -183651425845248.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -158008491376640.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182391238819840.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -231579670544384.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -244259118645248.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -99445412724736.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1528936148238336.0
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -201202339938304.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -249926596427776.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3053642317824.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -136679591510016.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -107534807465984.0
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 13617464344576.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1287859633913856.0
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -186899041878016.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -252598384852992.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -146493080076288.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -252021097627648.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -342923476467712.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -51579394719744.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6042179600384.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -176097786331136.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -155563228921856.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -224372463763456.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -185782534930432.0
  1%|          | 1140/139870 [00:04<08:36, 268.67it/s, epoch=2, test_loss=-1.41e+14, train_loss=-1.86e+14]Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169563513683968.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -23157295546368.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -173296964337664.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -183092895547392.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -116478699372544.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -14604960792576.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -79627796086784.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -237246124916736.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -170544796270592.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -193172730806272.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -175847050838016.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -171676889251840.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -235325553115136.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -552802354987008.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -231786634280960.0
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -298906772570112.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1689110913744896.0
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -670259241549824.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -67946281959424.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1853101489258496.0
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -246733623787520.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -197511218200576.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -340487357595648.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -264527169978368.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3690603741184.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -545252037361664.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -91647773769728.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -219486904909824.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -368034774515712.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -543585254506496.0
  1%|          | 1170/139870 [00:04<08:21, 276.49it/s, epoch=2, test_loss=-1.41e+14, train_loss=-5.44e+14]Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -182526983274496.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -180314001375232.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -154079267389440.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95954426396672.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 19000903860224.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -95647436898304.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -976908162433024.0
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 18341169201152.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -143915596382208.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -270373274779648.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -278809026756608.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -349711739387904.0
Starting epoch 4/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -302320063610880.0
Running validation...
Epoch 4, Step 0: Train Loss = -293681072439296.0, Test Loss = -284083431145472.0
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2375609762185216.0
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -239778259795968.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -197400689901568.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -103834256932864.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -115225508446208.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -262284895059968.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -122303102844928.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2140854902849536.0
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -393351257915392.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -354214979043328.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -367901529866240.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -572522965762048.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -365280593182720.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -352668354609152.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -106211177398272.0
  1%|          | 1198/139870 [00:04<08:38, 267.26it/s, epoch=3, test_loss=-2.84e+14, train_loss=-1.06e+14]Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -203507378421760.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -207200765083648.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -413203200737280.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -126598623789056.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -349772942671872.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -199959651549184.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -444007276609536.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -299913506193408.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2250216178712576.0
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2128009997844480.0
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -403098853965824.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 22366740545536.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -592493288620032.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3186640516284416.0
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -184470002991104.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -397831948992512.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -22198693658624.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -399287473143808.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -482390526918656.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1074463378505728.0
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -247685714018304.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -10307221061632.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2990447752380416.0
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -428576801292288.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -429238024929280.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -378459465449472.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -438353354817536.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1097468766846976.0
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -537005364609024.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -445910517547008.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -408573058220032.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -582719687884800.0
  1%|          | 1230/139870 [00:04<08:14, 280.24it/s, epoch=3, test_loss=-2.84e+14, train_loss=-5.83e+14]Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3270797447659520.0
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -238154023960576.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -363794366726144.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -233474724200448.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -445885553049600.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -292306984894464.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -732858121453568.0
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -425645821657088.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4371598639366144.0
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3855045607030784.0
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -184938305421312.0
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1143284860715008.0
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2899545843302400.0
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -525039787048960.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -341259042422784.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -296217451954176.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -110318424424448.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -539350584524800.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -195812340531200.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3354550383673344.0
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -239428521951232.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -613574732939264.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -380861459464192.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -883840046333952.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -90979218489344.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -665799521992704.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -360305980866560.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4268548146855936.0
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -382216521646080.0
  1%|          | 1259/139870 [00:04<08:22, 275.85it/s, epoch=3, test_loss=-2.84e+14, train_loss=-3.82e+14]Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4606674547507200.0
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4521520378413056.0
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3773033743384576.0
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -507564672417792.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -222424880644096.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3025887297536.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4327074592456704.0
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4259522809954304.0
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -909283566813184.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -400020234829824.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -760402350702592.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -408732575989760.0
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -820820460961792.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -854299261272064.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -368188286042112.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -426893945864192.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4448223003410432.0
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2146919530889216.0
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -797133984759808.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92633133219840.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -125523724337152.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -447874391343104.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -757334938746880.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -413637529305088.0
Running validation...
Epoch 4, Step 100: Train Loss = -444062842748928.0, Test Loss = -612558033649664.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -675319988092928.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -486302470373376.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -169035501142016.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -445734759432192.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -442098297864192.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -646515789922304.0
  1%|          | 1289/139870 [00:04<08:13, 281.01it/s, epoch=3, test_loss=-6.13e+14, train_loss=-6.47e+14]Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5159373487734784.0
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -563507628081152.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5442345797419008.0
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -516993803354112.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -374910648057856.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -747687905329152.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -283405765836800.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -539161874399232.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6892280610816.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -549663102992384.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -195399050592256.0
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 30339644784640.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6504201836298240.0
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -955348970110976.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -647635031556096.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6610034125438976.0
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -812857860030464.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2020087267590144.0
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -883252374011904.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1176631792107520.0
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -195711811452928.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -646469350588416.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9278840635392.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4340601591955456.0
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1039523282681856.0
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1056731069153280.0
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -511968658063360.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1165474809249792.0
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3583610116374528.0
  1%|          | 1318/139870 [00:05<08:17, 278.31it/s, epoch=3, test_loss=-6.13e+14, train_loss=-3.58e+15]Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1261112456642560.0
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1309978178617344.0
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -778192843440128.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -949108449738752.0
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -790985067986944.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1431235037495296.0
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1341508607279104.0
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -927890304663552.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6006309899993088.0
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -503777585004544.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -906582300819456.0
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7299095056089088.0
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -30984389001216.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 21932846088192.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -945182983847936.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -893464061411328.0
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2558157683425280.0
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4356191786369024.0
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 13796827463680.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -799007865569280.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -162746846937088.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -734433300709376.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -987028044906496.0
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1353024756776960.0
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1016972624003072.0
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -522007540137984.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1023531441717248.0
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -418316057313280.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1529100967608320.0
  1%|          | 1347/139870 [00:05<08:11, 281.59it/s, epoch=3, test_loss=-6.13e+14, train_loss=-1.53e+15]Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -762672375136256.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 34442483597312.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -291958421454848.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -192732966420480.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1421640348991488.0
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1067125158445056.0
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1182373223858176.0
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1113287466942464.0
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -89132927811584.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -852470074966016.0
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1059385895813120.0
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -665999372189696.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 89102712045568.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -836685197737984.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -703483095285760.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1326940615081984.0
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -406403764191232.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -732879126528000.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1202148729683968.0
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -821827496574976.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1724349912449024.0
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1158593734770688.0
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 59452409511936.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1507990934913024.0
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2103294742757376.0
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -691751861878784.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7402849654800384.0
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -960703854804992.0
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -788989854351360.0
  1%|          | 1376/139870 [00:05<08:23, 274.93it/s, epoch=3, test_loss=-6.13e+14, train_loss=-7.89e+14]Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0231907031711744e+16
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1541509664997376.0
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1206686899503104.0
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1204104953069568.0
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -696264530329600.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1749349038030848.0
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -915629917863936.0
Running validation...
Epoch 4, Step 200: Train Loss = -1087189500821504.0, Test Loss = -1.0403482385252352e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3019316643495936.0
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -989757697949696.0
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 13008467132416.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0505354714546176e+16
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9281589682896896.0
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1146242751004672.0
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1431316239220736.0
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1750666787684352.0
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1352858729447424.0
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 35622647496704.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -968871339098112.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -988349351329792.0
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1090233760219136.0
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -517097788538880.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1807356866330624.0
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -945520138780672.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1127740166111232.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1301646478934016.0
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -335266824847360.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1435657511632896.0
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1698797809827840.0
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2646301418192896.0
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1383707466268672.0
  1%|          | 1406/139870 [00:05<08:15, 279.20it/s, epoch=3, test_loss=-1.04e+16, train_loss=-1.38e+15]Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -204111106539520.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9615190261760.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1131226203160576.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -401079313367040.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -577379969794048.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1355590597083136.0
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1823070272618496.0
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1210275076243456.0
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -237099810816000.0
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1420714246668288.0
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1172538151403520.0
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3853916567502848.0
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1780102480265216.0
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1076895202410496.0
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1172146504073216.0
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -662880756170752.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2912276931674112.0
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2710929938579456.0
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1844462162542592.0
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1820132246552576.0
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2085003282350080.0
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1255018703355904.0
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2048925288628224.0
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1861613044826112e+16
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1640785954996224.0
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2322388674936832.0
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2198648792219648e+16
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -320766209949696.0
  1%|          | 1434/139870 [00:05<08:20, 276.86it/s, epoch=3, test_loss=-1.04e+16, train_loss=-3.21e+14]Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -245547675942912.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1834921966436352.0
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1409473646166016.0
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1612596876673024.0
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2241501018980352.0
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2088055930355712.0
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2613226143481856.0
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1466982855606272.0
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5415872692748288.0
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2132544006914048.0
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1691157599879168.0
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1161009653874688.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1564331510595584.0
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1938871315070976.0
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.42073760055296e+16
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2949171778617344e+16
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2845459055837184e+16
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -34189080526848.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1533723291942912.0
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2412362871078912.0
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3293985170784256.0
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1788325463588864.0
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1812624912154624.0
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2285456788029440.0
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1235801543278592.0
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2768470420750336.0
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3442385149231104.0
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2081160360361984.0
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5054027876204544e+16
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2290402979741696.0
  1%|          | 1464/139870 [00:05<08:11, 281.71it/s, epoch=3, test_loss=-1.04e+16, train_loss=-2.29e+15]Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2785068858736640.0
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7157953039630336e+16
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1902826607345664.0
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2109147575222272.0
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2818020384702464.0
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4722018578595840.0
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3735160453332992.0
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7783487109005312e+16
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3938167988158464.0
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -959847411482624.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3943086799454208.0
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2765905788403712.0
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2352828181905408.0
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2050849165541376.0
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1402518483501056.0
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 154320322428928.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5163569670782976.0
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2240636925247488.0
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -428967374880768.0
Running validation...
Epoch 4, Step 300: Train Loss = -453789769269248.0, Test Loss = -3532035880648704.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2622981624823808.0
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3644980375453696e+16
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2108430181466112.0
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2790023371948032.0
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1200180426702848.0
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -354610921340928.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3678406755483648.0
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3157482520182784.0
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3780290627502080.0
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2383887506341888.0
  1%|          | 1493/139870 [00:05<08:32, 270.07it/s, epoch=3, test_loss=-3.53e+15, train_loss=-2.38e+15]Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.201900024646861e+16
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1695616715456512.0
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3381881944932352.0
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1406186788225024.0
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1957934527414272.0
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -66666348150784.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1396413858578432.0
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2440731666939904.0
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1800963673292800.0
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2829290915758080.0
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -146789416042496.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9662235454472192.0
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3093765606604800.0
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3123883796332544.0
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2255369468379136.0
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1584508461645824.0
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3273151089737728.0
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2582627454287872.0
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2175615281135616.0
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4305466075119616.0
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4277167609348096.0
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2782173513908224.0
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.279690257563648e+16
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3156200204009472.0
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3524561832247296.0
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 59338303471616.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2211483492548608.0
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1704422874808320.0
  1%|          | 1521/139870 [00:05<08:27, 272.52it/s, epoch=3, test_loss=-3.53e+15, train_loss=-1.7e+15] Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 99978257104896.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4378917271896064e+16
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2964037193170944.0
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3380172011077632.0
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2428974193967104.0
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3472080553115648.0
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4917388587827200.0
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1022032900784128.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 25482747707392.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3150877128916992.0
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2830972663889920.0
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3940042472947712.0
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2930978829893632.0
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2426350774255616.0
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -318827871076352.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3153307811971072.0
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2918788907401216.0
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1684779707662336.0
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -19370705158144.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1509648792289280.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4101878719709184.0
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2081821516890112.0
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2627846749028352.0
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3490334164123648.0
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3055574757408768.0
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3739156383531008.0
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8036073863118848.0
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3921880935301120.0
  1%|          | 1549/139870 [00:05<08:27, 272.50it/s, epoch=3, test_loss=-3.53e+15, train_loss=-3.92e+15]Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4505553837490176.0
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0162090735960064e+16
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9446002104729600.0
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -801691347714048.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1381431951294464e+16
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3906950555238400.0
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2466240283017216.0
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5289491132579840.0
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3372474624376832.0
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 57019281178624.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8341591529881600.0
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1608688993304576.0
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3231344951820288.0
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6329281240104960.0
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8309693881516032.0
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3358042997391360.0
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3407299561390080.0
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2319948596641792.0
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1331789096288256.0
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 244722287247360.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1858915935453184.0
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4266860228837376e+16
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 305303522377728.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1295967324209152.0
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3370819451355136.0
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4353397373272064.0
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4691696646356992.0
Starting epoch 5/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4302397857857536.0
Running validation...
Epoch 5, Step 0: Train Loss = -4618006281846784.0, Test Loss = -2.725399414964224e+16
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.500291186805965e+16
  1%|          | 1578/139870 [00:06<08:19, 277.12it/s, epoch=4, test_loss=-2.73e+16, train_loss=-3.5e+16] Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3461887790415872.0
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4333194383982592.0
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1303742422974464.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1198318289944576.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3471651324821504.0
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1436045937737728.0
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.316760362300211e+16
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3935694623866880.0
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4627756394479616.0
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5901280232865792.0
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8354638566785024.0
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6412809227206656.0
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5446774445572096.0
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1705325623246848.0
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2541831975862272.0
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2435133445505024.0
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4636638386847744.0
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1332507026915328.0
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3850880562495488.0
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3066974137483264.0
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5747383501586432.0
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4432819103203328.0
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.632256419207578e+16
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.144197595778253e+16
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5396105642639360.0
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 308729496993792.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8410694869319680.0
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.895132040042906e+16
  1%|          | 1606/139870 [00:06<08:19, 276.58it/s, epoch=4, test_loss=-2.73e+16, train_loss=-3.9e+16]Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2475409769758720.0
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5147196718579712.0
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -204247303979008.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4305285954928640.0
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4644632394727424.0
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1902501133484032e+16
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3639789093912576.0
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -92265762521088.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.305981981635379e+16
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5041633401765888.0
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4253663132385280.0
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4278776611471360.0
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4228786413371392.0
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2757506715549696e+16
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5348098645688320.0
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6811409035821056.0
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3680690604343296.0
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6521135818604544.0
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.559786389536768e+16
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4549556851179520.0
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5810135121264640.0
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2875131370143744.0
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6383653915459584.0
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3373563667021824.0
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0002480616177664e+16
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5108149560279040.0
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.327215628445286e+16
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.588993455639757e+16
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2527462089031680.0
  1%|          | 1635/139870 [00:06<08:12, 280.46it/s, epoch=4, test_loss=-2.73e+16, train_loss=-2.53e+15]Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3334408870232064e+16
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.582796220071936e+16
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5266556544090112.0
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2780527467692032.0
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2618201863094272.0
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1501832354463744.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8334162847072256.0
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2275169267613696.0
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.337298735674163e+16
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2786492640395264.0
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6671172984897536.0
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5101730731655168.0
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9975834639073280.0
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1172839872856064.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7973615911829504.0
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4677460440383488.0
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.739427074670592e+16
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4329185032011776.0
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.64239517351936e+16
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.641565385837773e+16
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.626980481381171e+16
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6347622897942528.0
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2832366112342016.0
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 25450925522944.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.276666924361318e+16
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.539913817862963e+16
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9575979626266624.0
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4258466784870400.0
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7532011132551168.0
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4739530770874368.0
  1%|          | 1665/139870 [00:06<08:07, 283.74it/s, epoch=4, test_loss=-2.73e+16, train_loss=-4.74e+15]Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8490337521631232.0
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9681734605996032.0
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3959844956536832.0
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4609678877130752.0
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.468619508233011e+16
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.198420484148429e+16
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9274421382479872.0
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1074277084299264.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1676083069976576.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5983492886233088.0
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8109046968090624.0
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4811151498018816.0
Running validation...
Epoch 5, Step 100: Train Loss = -5400004399202304.0, Test Loss = -8206534135775232.0
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6704436969734144.0
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7394215696793600.0
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1811469834387456.0
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4942173770350592.0
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5955088120020992.0
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7430300938272768.0
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.658393309708288e+16
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6259673242009600.0
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.650277539505766e+16
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7813509731581952.0
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4536219903983616.0
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8678114028683264.0
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2774821603639296.0
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7124621136494592.0
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 64696807849984.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5166999202168832.0
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1851135199543296.0
  1%|          | 1694/139870 [00:06<08:12, 280.46it/s, epoch=4, test_loss=-8.21e+15, train_loss=-1.85e+15]Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 379456669614080.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.399140251782349e+16
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9339441818632192.0
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5493417790406656.0
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.057869935896166e+16
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7876968410251264.0
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7003086455177216e+16
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7069816816926720.0
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.157867885297664e+16
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2105854945918976.0
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6383365615779840.0
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 195603682295808.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.72127458288599e+16
Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2876908953862144e+16
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2749913213370368e+16
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5391879931691008.0
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.381317316968448e+16
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.37148170888151e+16
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4954993446551552e+16
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4021922538913792e+16
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8251285547515904.0
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2110987435966464e+16
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8519345193877504.0
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2955947089526784e+16
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.355477719973888e+16
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9871662354792448.0
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.900698801668096e+16
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6223229370761216.0
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9695191812276224.0
  1%|          | 1723/139870 [00:06<08:25, 273.33it/s, epoch=4, test_loss=-8.21e+15, train_loss=-9.7e+15] Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.561160029103718e+16
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -365730558115840.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 208315107770368.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9177915614822400.0
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0974082749169664e+16
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.079117432324096e+16
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.899531617048986e+16
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 300504903057408.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7407931137982464.0
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1809679638331392.0
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7291337271410688.0
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.100661605269504e+16
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2284883145588736e+16
Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1807586651209728e+16
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7289329911070720.0
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1509387071848448e+16
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4879467247828992.0
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4653779873890304e+16
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8457094474760192.0
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 340992821559296.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2312621382434816.0
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1910452724432896.0
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5987504658251776e+16
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0652122336985088e+16
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3133434465550336e+16
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3099927278190592e+16
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -760787891126272.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9065181313236992.0
  1%|▏         | 1751/139870 [00:06<08:26, 272.75it/s, epoch=4, test_loss=-8.21e+15, train_loss=-9.07e+15]Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.158847674712064e+16
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6142160252436480.0
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 910758888079360.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9731561595338752.0
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8053073343676416.0
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3248550326501376e+16
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3817068164022272.0
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7509401887834112.0
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0703335527022592e+16
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8123756157337600.0
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7836535323820032e+16
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1590626378252288e+16
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 477339376943104.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6183298258632704e+16
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.097579563994317e+16
Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6419105649262592.0
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.073669832087962e+16
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0376814933311488e+16
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8323694401159168.0
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.094562325515469e+16
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4030387919454208e+16
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2121689420726272e+16
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2344561716166656e+16
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6015029220474880.0
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5711120291528704e+16
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9743190219292672.0
Running validation...
Epoch 5, Step 200: Train Loss = -8794593004879872.0, Test Loss = -1.5297634126266368e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5464137395994624e+16
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0613506286026752e+16
  1%|▏         | 1779/139870 [00:06<08:35, 267.86it/s, epoch=4, test_loss=-1.53e+16, train_loss=-1.06e+16]Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 124296705867776.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.612070665951642e+16
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.58497304577966e+16
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0582862768111616e+16
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4856744995913728e+16
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5911112222441472e+16
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4137690161152e+16
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 352541585965056.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9389077681930240.0
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0910073777815552e+16
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0788777458925568e+16
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4305760280379392.0
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7227743036964864e+16
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9640604925427712.0
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6795431757479936.0
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1113111914283008e+16
Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2838213441880064.0
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5096312902975488e+16
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5718005124104192e+16
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.448923780186112e+16
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3475060459241472e+16
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1980407541137408.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 88414921687040.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9582443552047104.0
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3285650249875456.0
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5517230162837504.0
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5962293200224256e+16
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4506908098494464e+16
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.228605459791872e+16
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3313045161902080.0
  1%|▏         | 1809/139870 [00:06<08:20, 275.67it/s, epoch=4, test_loss=-1.53e+16, train_loss=-3.31e+15]Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3562539279384576e+16
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0492329152479232e+16
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.706510390801203e+16
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7592791634804736e+16
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0931570089132032e+16
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1072180875952128e+16
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5788582438502400.0
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8269491922141184e+16
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.595361763885056e+16
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4824717424787456e+16
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7022826926047232e+16
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9752539561918464e+16
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3851891226116096e+16
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.820029757895475e+16
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1372314049590067e+17
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3563669929525248e+16
Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1363248934682624e+16
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1297863368494285e+17
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2839949682409472.0
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2019458189099008.0
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.560022316220416e+16
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1355666971099136e+16
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.297635247915008e+16
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.024764406444851e+16
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8645488119054336e+16
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.206424772693197e+16
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2838630057836544e+16
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.520460809981133e+16
  1%|▏         | 1837/139870 [00:06<08:24, 273.37it/s, epoch=4, test_loss=-1.53e+16, train_loss=-4.52e+16]Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9774862654439424e+16
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4170174072553472e+16
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9894187310776320.0
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3681509873483776e+16
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.589903477440512e+16
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1839321305566413e+17
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3541361869389824e+17
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3922880378319667e+17
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -217255820394496.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.641202566823936e+16
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2175319876173824e+16
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1617298670288896e+16
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.707482502135808e+16
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5081190323126272e+16
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9621371260698624e+16
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.27752728477696e+16
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.295507548372992e+16
Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.637647005404365e+16
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.856659386479411e+16
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.232667468061737e+17
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6097652315783168e+16
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.363138465647821e+16
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4824107109934694e+17
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.462829353795584e+16
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9751635471302656e+16
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.626817030619136e+16
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.666199545747866e+16
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2224351495389184e+16
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5735993950378394e+17
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.205139960483021e+16
  1%|▏         | 1867/139870 [00:07<08:13, 279.62it/s, epoch=4, test_loss=-1.53e+16, train_loss=-3.21e+16]Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7700422168936448.0
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1583346953814016e+16
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.583938009871155e+16
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1452981538914304e+16
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9456508941041664e+16
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0452287172378624e+16
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1336881585324032.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.321738686149427e+16
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7193346791374848e+16
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3359110833635328.0
Running validation...
Epoch 5, Step 300: Train Loss = -3209916017803264.0, Test Loss = -920581142740992.0
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7493982690934784e+16
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9362920315171635e+17
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.656567275454464e+16
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.333278779264205e+16
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0355378751537152e+16
Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2491295511609344.0
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.84209260265472e+16
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.385129342448435e+16
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.633296847778611e+16
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2616282020970496e+16
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.852532094706647e+17
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5075527408746496e+16
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.968557457932288e+16
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1685824328368128e+16
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.484671409979392e+16
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -302529711702016.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0478586330873856e+16
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9220715236491264e+16
  1%|▏         | 1895/139870 [00:07<08:21, 275.02it/s, epoch=4, test_loss=-9.21e+14, train_loss=-1.92e+16]Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6007685635833856e+16
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7827788622921728e+16
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1247653639749632.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.945319083927142e+16
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.097281520520397e+16
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.977562098873139e+16
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2166394934132736e+16
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4326959706210304e+16
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.369289932557517e+16
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1727097089163264e+16
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.102036451557376e+16
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9162888069382144e+16
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.61853142171648e+16
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.890669085012787e+16
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9589627586910618e+17
Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.241681085484237e+16
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.154286876196864e+16
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 434415037579264.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.69959372095488e+16
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1975269690638336e+16
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 579052121358336.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9491515071987712e+17
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5506752061505536e+16
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3902830884552704e+16
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7907191830806528e+16
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.424445902572749e+16
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.707209611476992e+16
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6717665469005824.0
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 410735842689024.0
  1%|▏         | 1924/139870 [00:07<08:13, 279.26it/s, epoch=4, test_loss=-9.21e+14, train_loss=4.11e+14] Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.559775812354048e+16
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.075492265177907e+16
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.9803662830206976e+16
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.962954887843021e+16
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7708228544561152e+16
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2613603563732992.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2358347760009216e+16
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.865765146643661e+16
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3555590022299648e+16
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -807216017833984.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9806569977937920.0
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.914249341717709e+16
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.959028213992653e+16
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4620208534585344e+16
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.769208081383424e+16
Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.487915855531213e+16
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.711785040137421e+16
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.54301760323584e+16
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.740745118364467e+16
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.423129522352947e+16
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.944594890495099e+17
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.99055362497577e+16
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7169744599777280.0
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3727553698699674e+17
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7388948464533504e+16
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.959897515373363e+16
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.266809490407424e+16
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4194190728495104e+16
  1%|▏         | 1952/139870 [00:07<08:14, 278.82it/s, epoch=4, test_loss=-9.21e+14, train_loss=-2.42e+16]Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 670586397261824.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.445355388377498e+16
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1948506675675136e+16
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.154476069506253e+16
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.275109944203674e+16
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.821679563859558e+16
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.503328130924544e+16
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8215246485192704e+16
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.004159300842291e+16
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.191012147920896e+16
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1993226273685504.0
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.209543858061312e+16
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0833224357484954e+17
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2228982799925248.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1660450735325184e+16
Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.342399142317261e+16
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2851060238319616e+16
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.2297977972260864e+16
Starting epoch 6/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3165647265398784e+16
Running validation...
Epoch 6, Step 0: Train Loss = -2.6635383567548416e+16, Test Loss = -1.070948033673298e+17
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.116225203923845e+17
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.786215937127219e+16
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.014685004451021e+16
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9478363609563136.0
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9511315672399872.0
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.556870481726669e+16
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2568200663269376e+16
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.838456955788329e+17
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.6016614344228864e+16
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.870795654345523e+16
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.879791705351782e+16
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.712936575382323e+16
  1%|▏         | 1983/139870 [00:07<07:59, 287.43it/s, epoch=5, test_loss=-1.07e+17, train_loss=-5.71e+16]Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.173813999520973e+16
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.480164755308544e+16
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1424734340186112e+16
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5719158322823168e+16
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5148933768544256e+16
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.403431541099725e+16
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.067467950522368e+16
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.086145314566963e+16
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2545872202039296e+16
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.599689642193715e+16
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0956294613499904e+16
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6527240156505702e+17
Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4816532348678963e+17
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.091841971696435e+16
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2070488843026432.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.322775733259469e+16
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.892986547773112e+17
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6455414501605376e+16
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.770429812087194e+16
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1849022075633664.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.634619510213837e+16
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.977265988134502e+16
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0293299723737498e+17
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2696909294534656e+16
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -499831114039296.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.297214094985134e+17
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1387487855181824e+16
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.866479212213043e+16
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.588610746797261e+16
  1%|▏         | 2012/139870 [00:07<08:11, 280.38it/s, epoch=5, test_loss=-1.07e+17, train_loss=-3.59e+16]Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0003649392410624e+16
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.927933243804877e+16
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.896352240251699e+16
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.668873834391142e+16
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1553874888228864e+16
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.151682892038144e+16
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.8629999451058995e+17
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.973300417573683e+16
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.391101951226675e+16
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7013769913761792e+16
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.995018805955789e+16
Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.271740058350387e+16
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.333992040352973e+16
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.947196063101747e+16
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.3032909583128986e+17
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.378684470629499e+17
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5275852199624704e+16
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0041341480271872e+17
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5120885757181952e+17
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.150711370435789e+16
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1761304356192256e+16
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.721121170784256e+16
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6778211086106624.0
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.824680887006003e+16
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6788327080394752e+16
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.915994172181709e+17
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9351464778399744e+16
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.181185451891098e+16
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6567235321462784e+16
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.027237182655693e+16
  1%|▏         | 2042/139870 [00:07<08:07, 282.94it/s, epoch=5, test_loss=-1.07e+17, train_loss=-8.03e+16]Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5790755155083264.0
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.699137704453734e+16
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.259159240146944e+16
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.876666343549829e+17
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.860894036741325e+16
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.205555010429256e+17
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.213143717938463e+17
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3888559996718285e+17
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.945642573430784e+16
Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6874522779058176e+16
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 89092838653952.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.7072178578142e+17
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5259173239259136e+17
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.573502422109389e+16
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.166111135911117e+16
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.604297041104077e+16
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.822075048578253e+16
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.917173624714035e+16
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.784893840470835e+16
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.479242625830093e+16
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5342727260471296e+16
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.243050090616586e+17
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.840007970071511e+17
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.437792809962701e+16
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5871826957762560.0
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7837796697899008.0
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.0973961961472e+16
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.395645436893594e+16
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.077162819964109e+16
Running validation...
Epoch 6, Step 100: Train Loss = -3.2430134115958784e+16, Test Loss = -4.431004028549202e+17
  1%|▏         | 2071/139870 [00:07<08:19, 275.93it/s, epoch=5, test_loss=-4.43e+17, train_loss=-3.24e+16]Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.999912975702426e+16
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.962715497929114e+16
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1642889687793664e+16
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.956788630552576e+16
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.81030171148288e+16
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.72257147551744e+16
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.074271260488499e+17
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.390663462322176e+16
Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.9802520211973734e+17
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.5873815271571456e+16
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6772511135891456e+16
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.749532461216563e+16
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5731427970646016e+16
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.735240286037606e+16
Batch 116/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 400340746764288.0
Batch 117/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.826884580212736e+16
Batch 118/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1370368644153344e+16
Batch 119/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2655874900295680.0
Batch 120/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.4872626182619136e+17
Batch 121/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.996062725013504e+16
Batch 122/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.259310477508608e+16
Batch 123/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3156910285992755e+17
Batch 124/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.691720295933542e+16
Batch 125/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.731392542122967e+17
Batch 126/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.856393395024691e+16
Batch 127/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.968392694726656e+16
Batch 128/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0894649476513792e+16
Batch 129/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.770130452866662e+16
Batch 130/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 523788978487296.0
Batch 131/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.564810829771571e+17
  2%|▏         | 2101/139870 [00:07<08:09, 281.19it/s, epoch=5, test_loss=-4.43e+17, train_loss=-3.56e+17]Batch 132/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.741964353857126e+16
Batch 133/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.244089222437274e+16
Batch 134/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.893543304383693e+16
Batch 135/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.931388974491238e+16
Batch 136/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.589838478892073e+17
Batch 137/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.809082721298022e+16
Batch 138/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.023742797263667e+16
Batch 139/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.428610357375795e+16
Batch 140/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.00305995275305e+16
Batch 141/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.780480035559834e+16
Batch 142/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.136454765019136e+16
Batch 143/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.812674172950938e+16
Batch 144/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.659005100543181e+16
Batch 145/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.491327375310848e+17
Batch 146/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.664474686881792e+16
Batch 147/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.643127894940058e+16
Batch 148/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.120796048532439e+17
Batch 149/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2536447160614912.0
Batch 150/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1180523166695424.0
Batch 151/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.733457687117824e+16
Batch 152/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.479778075272806e+16
Batch 153/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.474193923160146e+17
Batch 154/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5664133836623053e+17
Batch 155/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2055239024771072.0
Batch 156/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.019848870887424e+16
Batch 157/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1963745219641344e+16
Batch 158/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.585821622291661e+16
Batch 159/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.022315525118362e+16
Batch 160/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.123444395573248e+16
  2%|▏         | 2130/139870 [00:08<08:11, 280.05it/s, epoch=5, test_loss=-4.43e+17, train_loss=-6.12e+16]Batch 161/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.822705631546573e+16
Batch 162/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.379691511611392e+16
Batch 163/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.90469722872873e+16
Batch 164/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.1527385677430784e+16
Batch 165/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.604374943550669e+16
Batch 166/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.803656350590566e+16
Batch 167/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1920832318210048.0
Batch 168/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3720139883085824e+16
Batch 169/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3702015121096704e+16
Batch 170/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.147772675345613e+16
Batch 171/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.850637763864166e+16
Batch 172/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.328464901452595e+16
Batch 173/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.833828979854541e+16
Batch 174/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4105991956201472.0
Batch 175/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.142129838286438e+16
Batch 176/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.658974418783437e+16
Batch 177/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.668285181866803e+16
Batch 178/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 5221805669220352.0
Batch 179/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.321763409467802e+16
Batch 180/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.383732915601408e+16
Batch 181/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.629853923527885e+16
Batch 182/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.400797812640973e+16
Batch 183/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.98321709481984e+16
Batch 184/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.993264850337792e+16
Batch 185/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.836035679538381e+16
Batch 186/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0192199629563494e+17
Batch 187/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.823228141574554e+16
Batch 188/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2529345432190976.0
Batch 189/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.923270439816397e+16
Batch 190/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5221917288811725e+17
  2%|▏         | 2160/139870 [00:08<08:03, 284.77it/s, epoch=5, test_loss=-4.43e+17, train_loss=-1.52e+17]Batch 191/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.523239197068493e+16
Batch 192/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.784627599578235e+17
Batch 193/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.662567158926541e+16
Batch 194/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.433849600488243e+16
Batch 195/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.782300039514685e+17
Batch 196/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.454413770922394e+16
Batch 197/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.449627459367731e+16
Batch 198/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.69355258349486e+16
Batch 199/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.656438373574246e+16
Batch 200/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.89447765056553e+16
Batch 201/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.81136238091305e+16
Running validation...
Epoch 6, Step 200: Train Loss = -5.856260922540032e+16, Test Loss = -7.784782842822656e+16
Batch 202/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.587429533328343e+17
Batch 203/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.032642531976806e+16
Batch 204/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 595604187119616.0
Batch 205/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.769648783847588e+17
Batch 206/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.782030331261747e+17
Batch 207/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.769143765899674e+16
Batch 208/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.848117102070989e+16
Batch 209/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.143701905342464e+16
Batch 210/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.371928252501197e+16
Batch 211/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2092198292094976.0
Batch 212/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.871047823438643e+16
Batch 213/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.600845659897856e+16
Batch 214/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.746500456310374e+16
Batch 215/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.2578821316214784e+16
Batch 216/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.141270282351411e+16
Batch 217/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.399362731088282e+16
Batch 218/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.538571211086234e+16
Batch 219/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.500247459908813e+16
  2%|▏         | 2189/139870 [00:08<08:11, 280.03it/s, epoch=5, test_loss=-7.78e+16, train_loss=-6.5e+16] Batch 220/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7386472143323136e+16
Batch 221/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.188645302121267e+16
Batch 222/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.67046608779346e+16
Batch 223/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6561895545556173e+17
Batch 224/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.595276672322765e+16
Batch 225/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7403500342345728.0
Batch 226/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1030413388611584.0
Batch 227/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.667296964404838e+16
Batch 228/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7325810025234432e+16
Batch 229/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.486553921401651e+16
Batch 230/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.386451738925466e+16
Batch 231/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.126478360471142e+16
Batch 232/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.670557516084019e+16
Batch 233/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6010976654524416e+16
Batch 234/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.770143634286182e+16
Batch 235/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.605244565402419e+16
Batch 236/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3307419997804954e+17
Batch 237/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.237759971137946e+16
Batch 238/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.715509502797414e+16
Batch 239/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.25099437433815e+16
Batch 240/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.789853372186624e+16
Batch 241/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7972325009850368e+17
Batch 242/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5461662363274445e+17
Batch 243/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.200372468318208e+16
Batch 244/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.682098577217946e+16
Batch 245/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.352793798213632e+16
Batch 246/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.962913120937574e+16
Batch 247/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.297849999582822e+16
Batch 248/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.541813809802117e+17
Batch 249/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.963775604883456e+16
  2%|▏         | 2219/139870 [00:08<08:06, 283.05it/s, epoch=5, test_loss=-7.78e+16, train_loss=-7.96e+16]Batch 250/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0994846553813811e+17
Batch 251/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.859022227220726e+17
Batch 252/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3271106420998144e+16
Batch 253/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4819871627935744e+16
Batch 254/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.0227841605632e+16
Batch 255/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.31275366907904e+16
Batch 256/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.998220383603917e+16
Batch 257/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0459846529567949e+17
Batch 258/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0122986231588454e+17
Batch 259/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1851385009707418e+17
Batch 260/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.999954878889984e+16
Batch 261/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5513978343981056e+17
Batch 262/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.461279893133722e+16
Batch 263/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.692381775409971e+16
Batch 264/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.875971144450048e+16
Batch 265/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.885612354057011e+16
Batch 266/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.13921710049198e+16
Batch 267/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.952747347151421e+17
Batch 268/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.958894991540224e+17
Batch 269/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.101640135101645e+17
Batch 270/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -873708554027008.0
Batch 271/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.839755605226291e+16
Batch 272/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1481827125703475e+17
Batch 273/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.624837667684352e+17
Batch 274/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.012057503694848e+17
Batch 275/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.97934896880681e+16
Batch 276/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1193741335330816e+17
Batch 277/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.612407711858688e+16
Batch 278/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4634180220131738e+17
  2%|▏         | 2248/139870 [00:08<08:14, 278.52it/s, epoch=5, test_loss=-7.78e+16, train_loss=-1.46e+17]Batch 279/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4381449741546291e+17
Batch 280/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0831555333193728e+17
Batch 281/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.774813739719066e+17
Batch 282/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.922081780360806e+16
Batch 283/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1319185304138547e+17
Batch 284/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.284085863376486e+17
Batch 285/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.654978623216026e+16
Batch 286/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.169629030467502e+17
Batch 287/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2311605358362624e+17
Batch 288/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9307128689996595e+17
Batch 289/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5475574621339648e+17
Batch 290/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.462056907230413e+17
Batch 291/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.517211284804731e+17
Batch 292/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.966385547483546e+16
Batch 293/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5888733295345664e+17
Batch 294/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2610006801186816e+17
Batch 295/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0274620051973734e+17
Batch 296/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.406635508236288e+16
Batch 297/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.674234195581338e+16
Batch 298/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 6888676571217920.0
Batch 299/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.25192419593814e+17
Batch 300/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.31182238719017e+16
Batch 301/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.656666596573184e+16
Running validation...
Epoch 6, Step 300: Train Loss = -1.7126408652324864e+16, Test Loss = -9.127666215446118e+16
Batch 302/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.055096340175913e+17
Batch 303/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.818044412565914e+17
Batch 304/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.278541544606925e+16
Batch 305/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1133664191787827e+17
Batch 306/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.39226744511529e+16
  2%|▏         | 2276/139870 [00:08<08:13, 278.55it/s, epoch=5, test_loss=-9.13e+16, train_loss=-5.39e+16]Batch 307/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1547299821912064e+16
Batch 308/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7774096807257702e+17
Batch 309/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2356174233993216e+17
Batch 310/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.454344645902336e+17
Batch 311/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0189365810141594e+17
Batch 312/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.332215579106345e+17
Batch 313/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.131074646979379e+16
Batch 314/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.244215432529838e+17
Batch 315/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.652285436716646e+16
Batch 316/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.520365758224794e+16
Batch 317/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1836008861597696.0
Batch 318/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.207534459263386e+16
Batch 319/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1195318447321907e+17
Batch 320/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.692530865287987e+16
Batch 321/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.064243589724242e+17
Batch 322/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5796977488953344.0
Batch 323/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.3516677362548736e+17
Batch 324/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.452319482922926e+17
Batch 325/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2690496206300774e+17
Batch 326/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.92282357572567e+16
Batch 327/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.427111038805606e+16
Batch 328/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.336426404690002e+17
Batch 329/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0717121083552563e+17
Batch 330/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.499596639384371e+16
Batch 331/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6678694604190515e+17
Batch 332/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.541986545953669e+17
Batch 333/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.181636774487654e+16
Batch 334/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.936758683016888e+17
  2%|▏         | 2304/139870 [00:08<08:21, 274.33it/s, epoch=5, test_loss=-9.13e+16, train_loss=-9.94e+17]Batch 335/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1481608941364838e+17
Batch 336/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.262361755254784e+17
Batch 337/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2153341580738560.0
Batch 338/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.546557811798835e+16
Batch 339/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.329789469366682e+16
Batch 340/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3063658154295296.0
Batch 341/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.356154696022098e+17
Batch 342/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3822982874988544e+17
Batch 343/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3223051535161754e+17
Batch 344/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.943782532120576e+16
Batch 345/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.227504831472599e+17
Batch 346/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9606134005222605e+17
Batch 347/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.503076500353843e+16
Batch 348/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 2687210981687296.0
Batch 349/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.317024834823127e+17
Batch 350/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.869647773125837e+16
Batch 351/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5197573414177997e+17
Batch 352/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.127899214118912e+17
Batch 353/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.581877045860762e+16
Batch 354/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7678444821282816.0
Batch 355/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2235691811405824e+17
Batch 356/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1841661203749274e+17
Batch 357/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.295245664393626e+16
Batch 358/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 497941328429056.0
Batch 359/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.793136070203802e+16
Batch 360/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.577115255266345e+17
Batch 361/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.998992618723738e+16
Batch 362/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.924800307167232e+16
Batch 363/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.559070036071547e+17
  2%|▏         | 2333/139870 [00:08<08:15, 277.62it/s, epoch=5, test_loss=-9.13e+16, train_loss=-1.56e+17]Batch 364/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1585959466788454e+17
Batch 365/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4760596569535283e+17
Batch 366/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.453931954058035e+17
Batch 367/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5598056780700058e+17
Batch 368/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6965558905864192e+17
Batch 369/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0856496723677676e+18
Batch 370/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.682192285971251e+17
Batch 371/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.181000955789312e+16
Batch 372/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.166498549064532e+18
Batch 373/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6025388846782874e+17
Batch 374/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.82692057947177e+16
Batch 375/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.1397527068672e+17
Batch 376/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.24780611108864e+17
Batch 377/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 3365677033324544.0
Batch 378/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.59645805428736e+17
Batch 379/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.572862659474227e+16
Batch 380/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1488655264710656e+17
Batch 381/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3094226411166106e+17
Batch 382/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.667604359344292e+17
Batch 383/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.063722696090583e+17
Batch 384/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2248273488602726e+17
Batch 385/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.576234021309645e+16
Batch 386/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.707684876864717e+16
Batch 387/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 1.3181019481964544e+16
Batch 388/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.508335070819123e+16
Batch 389/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.349789601569178e+17
Batch 390/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9992632256167936.0
Batch 391/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.439184808863334e+16
  2%|▏         | 2361/139870 [00:08<08:15, 277.70it/s, epoch=5, test_loss=-9.13e+16, train_loss=-5.44e+16]Batch 392/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.174503818348462e+17
Batch 393/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.378083292493906e+17
Batch 394/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4708167044759552e+17
Starting epoch 7/15
Batch 1/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.606845877882716e+17
Running validation...
Epoch 7, Step 0: Train Loss = -1.529194243560571e+17, Test Loss = -1.1485631607734272e+16
Batch 2/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.287037733939708e+18
Batch 3/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.350501012518994e+17
Batch 4/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.431220456081326e+17
Batch 5/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.849781480364442e+16
Batch 6/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.389279194362675e+16
Batch 7/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1292387285191885e+17
Batch 8/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.72611801750569e+16
Batch 9/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1681901477038653e+18
Batch 10/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.464183213386629e+17
Batch 11/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6495340732337357e+17
Batch 12/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0439185861954765e+17
Batch 13/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6728143264730317e+17
Batch 14/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.029040132287365e+17
Batch 15/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.86641445979947e+17
Batch 16/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.450980507045069e+16
Batch 17/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.959668085653504e+16
Batch 18/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.809439875085107e+16
Batch 19/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5348380023862067e+17
Batch 20/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.40627094397911e+16
Batch 21/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3206363010236416e+17
Batch 22/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.697785515769856e+16
Batch 23/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.137716720570204e+17
Batch 24/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.514030775622697e+17
Batch 25/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.204514438472794e+18
  2%|▏         | 2389/139870 [00:08<08:16, 276.94it/s, epoch=6, test_loss=-1.15e+16, train_loss=-1.2e+18] Batch 26/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.078591220034306e+18
Batch 27/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6660578432135987e+17
Batch 28/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 9639332541366272.0
Batch 29/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7235093716559462e+17
Batch 30/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.46373475008145e+18
Batch 31/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.960087086969651e+16
Batch 32/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.772527505501061e+17
Batch 33/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8132270393131008.0
Batch 34/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5477680873301606e+17
Batch 35/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7713061886007706e+17
Batch 36/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.60057758373249e+17
Batch 37/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1661773370503987e+17
Batch 38/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -711931631501312.0
Batch 39/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3666448496918528e+18
Batch 40/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8872254661342003e+17
Batch 41/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7338796717847347e+17
Batch 42/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.413657304015831e+17
Batch 43/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7509897906998477e+17
Batch 44/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.0328718989236634e+17
Batch 45/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.915637190193316e+17
Batch 46/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8135674642025677e+17
Batch 47/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6543616164744397e+17
Batch 48/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.917567692093522e+17
Batch 49/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.590222567740801e+18
Batch 50/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7406768870273843e+17
Batch 51/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.72886813214507e+17
Batch 52/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.941734316729958e+16
Batch 53/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8478232643620045e+17
  2%|▏         | 2417/139870 [00:09<08:23, 272.96it/s, epoch=6, test_loss=-1.15e+16, train_loss=-1.85e+17]Batch 54/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.161704815806382e+17
Batch 55/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4889512432972595e+17
Batch 56/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.803738173847634e+17
Batch 57/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5761386484227113e+18
Batch 58/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6096667436832522e+18
Batch 59/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -6.04153679225815e+16
Batch 60/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.789064800497828e+17
Batch 61/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3458929421071155e+18
Batch 62/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.823058998329344e+17
Batch 63/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.595840843533517e+16
Batch 64/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.686633391181005e+16
Batch 65/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.282573308873933e+16
Batch 66/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.449313677317243e+17
Batch 67/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.485351841824768e+16
Batch 68/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5003267719317422e+18
Batch 69/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -8.169873046555853e+16
Batch 70/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.8263193939030835e+17
Batch 71/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3510893371392e+17
Batch 72/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.129994181880054e+17
Batch 73/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.718895545987891e+16
Batch 74/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.7123548261921587e+17
Batch 75/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3230147680128205e+17
Batch 76/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5396835157700772e+18
Batch 77/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.1362020730968474e+17
Batch 78/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5974489704754053e+18
Batch 79/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7444815752166113e+18
Batch 80/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -9.761561813052293e+17
Batch 81/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.772976587281531e+17
  2%|▏         | 2445/139870 [00:09<08:27, 271.04it/s, epoch=6, test_loss=-1.15e+16, train_loss=-1.77e+17]Batch 82/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.574333740285952e+16
Batch 83/394
Batch data shape: (1, 28, 28, 1)
Train Loss: 71491618078720.0
Batch 84/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6046887047884964e+18
Batch 85/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5550625223467336e+18
Batch 86/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.6120785067455283e+17
Batch 87/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0549002319691776e+17
Batch 88/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9381634346673766e+17
Batch 89/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.489950955578327e+17
Batch 90/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.273324128188498e+17
Batch 91/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -3.322036944371712e+17
Batch 92/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.0813732936902246e+17
Batch 93/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2849525960369766e+17
Batch 94/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4371873167067054e+18
Batch 95/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.599227886129644e+17
Batch 96/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4565403892894925e+17
Batch 97/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.5954487369728e+16
Batch 98/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -4.387920266710221e+16
Batch 99/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.6929572233884467e+17
Batch 100/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.3121014981184717e+17
Batch 101/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2660818841278874e+17
Running validation...
Epoch 7, Step 100: Train Loss = -1.4534837626575258e+17, Test Loss = -1.700030862726267e+17
Batch 102/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.0504448749010944e+17
Batch 103/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5701945167642624e+17
Batch 104/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -5.686302624186368e+16
Batch 105/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.3518223162579354e+17
Batch 106/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7420026575323136e+17
Batch 107/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -2.4177430907112653e+17
Batch 108/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.5889922142293197e+18
Batch 109/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.867799500853084e+17
  2%|▏         | 2473/139870 [00:09<08:29, 269.72it/s, epoch=6, test_loss=-1.7e+17, train_loss=-1.87e+17] Batch 110/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.7901341226355917e+18
Batch 111/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.996097733127045e+17
Batch 112/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.2550512055209165e+17
Batch 113/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.9631938168736973e+17
Batch 114/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -7.146125500874752e+16
Batch 115/394
Batch data shape: (1, 28, 28, 1)
Train Loss: -1.4905680564794163e+17
Batch 116/394
Batch data shape: (1, 28, 28, 1)
